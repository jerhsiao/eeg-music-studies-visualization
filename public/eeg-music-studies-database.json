{
  "metadata": {
    "columns": [
      "Study Name",
      "Authors",
      "Year",
      "DOI/URL",
      "Dataset",
      "Study Description",
      "Paradigm Type",
      "Stimulus Type",
      "Stimulus Description",
      "Composer",
      "Passage Name",
      "Passage Length",
      "Musical Features Analyzed",
      "Task Description",
      "Number of Participants",
      "Demographics",
      "Musical Training",
      "EEG System Used",
      "Channel Count",
      "Findings",
      "Sampling Rate",
      "Recording Environment",
      "Data Format",
      "Preprocessing",
      "License",
      "EEG Analysis Techniques",
      "Statistical Tests",
      "Event Markers",
      "Publication"
    ],
    "count": 194,
    "yearRange": {
      "min": 1975,
      "max": 2025
    },
    "featureCategories": [
      {
        "name": "coherence",
        "count": 82,
        "percentage": 42
      },
      {
        "name": "tempo",
        "count": 70,
        "percentage": 36
      },
      {
        "name": "synchronization",
        "count": 66,
        "percentage": 34
      },
      {
        "name": "harmony",
        "count": 46,
        "percentage": 24
      },
      {
        "name": "arousal",
        "count": 43,
        "percentage": 22
      },
      {
        "name": "spectral",
        "count": 39,
        "percentage": 20
      },
      {
        "name": "emotion",
        "count": 26,
        "percentage": 13
      },
      {
        "name": "power",
        "count": 25,
        "percentage": 13
      },
      {
        "name": "valence",
        "count": 25,
        "percentage": 13
      },
      {
        "name": "alpha",
        "count": 24,
        "percentage": 12
      },
      {
        "name": "gamma",
        "count": 24,
        "percentage": 12
      },
      {
        "name": "localization",
        "count": 22,
        "percentage": 11
      },
      {
        "name": "theta",
        "count": 21,
        "percentage": 11
      },
      {
        "name": "beta",
        "count": 21,
        "percentage": 11
      },
      {
        "name": "erp",
        "count": 19,
        "percentage": 10
      },
      {
        "name": "expectancy",
        "count": 16,
        "percentage": 8
      },
      {
        "name": "imagery",
        "count": 15,
        "percentage": 8
      },
      {
        "name": "envelope",
        "count": 15,
        "percentage": 8
      },
      {
        "name": "delta",
        "count": 14,
        "percentage": 7
      },
      {
        "name": "melody",
        "count": 11,
        "percentage": 6
      },
      {
        "name": "attention",
        "count": 10,
        "percentage": 5
      }
    ]
  },
  "studies": [
    {
      "id": "study-0",
      "Study Name": "Effect of Music and Biofeedback on Alpha Brainwave Rhythms and Attentiveness",
      "Authors": "Michael Wagner",
      "Year": 1975,
      "DOI/URL": "https://doi.org/10.2307/3345198",
      "Study Description": "This study investigates alpha rhythm production in musicians vs. nonmusicians during passive listening to two musical excerpts (slow and brisk tempo) and silence.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Slow tempo, orchestral",
      "Composer": "Camille Saint-Saëns",
      "Passage Name": "Saint-Saëns: Symphony No. 3 in C Minor, Op. 78 (2nd Movement)",
      "Passage Length": "2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Tempo",
        "Pulse",
        "Cortical Activation (Alpha Rhythms)",
        "Attentiveness"
      ],
      "normalizedFeatures": [
        "alpha",
        "attention",
        "arousal",
        "tempo",
        "power"
      ],
      "Task Description": "EEG-monitored passive listening with and without alpha biofeedback; subjects rated attentiveness after each excerpt",
      "Number of Participants": "60",
      "participantsValue": 60,
      "Demographics": "Undergraduate students from Florida State University; musicians and nonmusicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Aquarius Electronics \"Alphaphone Brainwave Analyzer\"",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Musicians produced significantly more alpha than nonmusicians; no significant difference between musical stimuli or feedback condition; attentiveness ratings were higher for music than silence, but alpha rhythm did not correlate with attentiveness reports",
      "Sampling Rate": "Not Reported",
      "Recording Environment": "Sound-treated room at FSU School of Music",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Manual alpha duration timing using EEG-linked timer; no digital preprocessing reported"
      ],
      "License": "Copyright by the Journal of Research in Music Education (MENC), 1975",
      "EEG Analysis Techniques": [
        "Manual timing of alpha rhythm duration using EEG-linked digital timer"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Math problems used to block alpha before each aural condition"
      ],
      "Publication": "Journal of Research in Music Education 23(1) 3-13",
      "year": 1975
    },
    {
      "id": "study-1",
      "Study Name": "Effect of Music and Biofeedback on Alpha Brainwave Rhythms and Attentiveness",
      "Authors": "Michael Wagner",
      "Year": 1975,
      "DOI/URL": "https://doi.org/10.2307/3345198",
      "Study Description": "This study investigates alpha rhythm production in musicians vs. nonmusicians during passive listening to two musical excerpts (slow and brisk tempo) and silence.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Brisk tempo, rhythmic contrast",
      "Composer": "Jules Massenet",
      "Passage Name": "Massenet: Le Cid (“Castillane” movement)",
      "Passage Length": "3 minutes",
      "passageLengthSeconds": 180,
      "Musical Features Analyzed": [
        "Tempo",
        "Pulse",
        "Cortical Activation (Alpha Rhythms)",
        "Attentiveness"
      ],
      "normalizedFeatures": [
        "alpha",
        "attention",
        "arousal",
        "tempo",
        "power"
      ],
      "Task Description": "EEG-monitored passive listening with and without alpha biofeedback; subjects rated attentiveness after each excerpt",
      "Number of Participants": "60",
      "participantsValue": 60,
      "Demographics": "Undergraduate students from Florida State University; musicians and nonmusicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Aquarius Electronics \"Alphaphone Brainwave Analyzer\"",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Musicians produced significantly more alpha than nonmusicians; no significant difference between musical stimuli or feedback condition; attentiveness ratings were higher for music than silence, but alpha rhythm did not correlate with attentiveness reports",
      "Sampling Rate": "Not Reported",
      "Recording Environment": "Sound-treated room at FSU School of Music",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Manual alpha duration timing using EEG-linked timer; no digital preprocessing reported"
      ],
      "License": "Copyright by the Journal of Research in Music Education (MENC), 1976",
      "EEG Analysis Techniques": [
        "Manual timing of alpha rhythm duration using EEG-linked digital timer"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Math problems used to block alpha before each aural condition"
      ],
      "Publication": "Journal of Research in Music Education 23(1) 3-14",
      "year": 1975
    },
    {
      "id": "study-2",
      "Study Name": "Subjective Reactions to Music and Brainwave Rhythms",
      "Authors": "James Walker",
      "Year": 1977,
      "DOI/URL": "https://doi.org/10.3758/BF03337859",
      "Study Description": "This study investigates EEG during passive listening to orchestral and rock music; subjects rated emotion, attention, familiarity.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Harp and strings, orchestral",
      "Composer": "Gustav Mahler",
      "Passage Name": "Symphony No. 5, 4th Movement (Adagietto)",
      "Passage Length": "1.63 minutes",
      "passageLengthSeconds": 97.8,
      "Musical Features Analyzed": [
        "EEG bandwidths (delta",
        "theta",
        "alpha",
        "beta); attentiveness",
        "emotion",
        "familiarity)"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "attention",
        "emotion"
      ],
      "Task Description": "Passive listening with eyes closed; EEG recorded at occipital sites; subjects completed visual analog scales for attentiveness, emotion, and familiarity after each excerpt",
      "Number of Participants": "24",
      "participantsValue": 24,
      "Demographics": "Undergraduate students (Brandon University)",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass Model 79C polygraph",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Alpha and beta power linked to comfort and attention; nonmusicians showed stronger EEG-emotion correlations.",
      "Sampling Rate": "Not Reported",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Impedance check (<5 kΩ); bipolar montage with Cz reference; manual scoring of 15-second segments"
      ],
      "License": "Copyright by Physiological Psychology (1977)",
      "EEG Analysis Techniques": [
        "Band power (alpha theta delta)"
      ],
      "Statistical Tests": [
        "Pearson correlation",
        "t-tests",
        "sign test"
      ],
      "Event Markers": [
        "Stimulus onset; music vs. silence"
      ],
      "Publication": "Psychobiology 5(4) 345-349",
      "year": 1977
    },
    {
      "id": "study-3",
      "Study Name": "Subjective Reactions to Music and Brainwave Rhythms",
      "Authors": "James Walker",
      "Year": 1977,
      "DOI/URL": "https://doi.org/10.3758/BF03337859",
      "Study Description": "This study investigates EEG during passive listening to orchestral and rock music; subjects rated emotion, attention, familiarity.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Electric guitars, drums",
      "Composer": "Uriah Heap",
      "Passage Name": "Gypsy (Instrumental Interlude)",
      "Passage Length": "1.75 minutes",
      "passageLengthSeconds": 105,
      "Musical Features Analyzed": [
        "EEG bandwidths (delta",
        "theta",
        "alpha",
        "beta); attentiveness",
        "emotion",
        "familiarity)"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "attention",
        "emotion"
      ],
      "Task Description": "Passive listening with eyes closed; EEG recorded at occipital sites; subjects completed visual analog scales for attentiveness, emotion, and familiarity after each excerpt",
      "Number of Participants": "24",
      "participantsValue": 24,
      "Demographics": "Undergraduate students (Brandon University)",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass Model 79C polygraph",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "EEG-emotion links were weaker; delta power associated with attention; theta linked to unpleasantness.",
      "Sampling Rate": "Not Reported",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Impedance check (<5 kΩ); bipolar montage with Cz reference; manual scoring of 15-second segments"
      ],
      "License": "Copyright by Physiological Psychology (1977)",
      "EEG Analysis Techniques": [
        "Band power (alpha theta delta)"
      ],
      "Statistical Tests": [
        "Pearson correlation",
        "t-tests",
        "sign test"
      ],
      "Event Markers": [
        "Stimulus onset; music vs. silence"
      ],
      "Publication": "Psychobiology 5(4) 345-350",
      "year": 1977
    },
    {
      "id": "study-4",
      "Study Name": "The EEG: An Adequate Method to Concretize Brain Processes Elicited by Music",
      "Authors": "H. Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1988,
      "DOI/URL": "https://doi.org/10.2307/40285422",
      "Study Description": "This study investigates whether music processing by the brain can be reflected in ongoing EEG, examining parameters like location, power, frequency, and coherence to identify significant changes in brain electrical activity when listening to various musical stimuli.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "First movement of Mozart string quartet KV 458",
      "Composer": "Mozart",
      "Passage Name": "String quartet KV 458 (first movement)",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Brain electrical activity parameters (power",
        "coherence) in different frequency bands (theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "theta",
        "alpha",
        "beta",
        "coherence",
        "power",
        "spectral"
      ],
      "Task Description": "Binaural listening to music with eyes closed",
      "Number of Participants": "75",
      "participantsValue": 75,
      "Demographics": "Healthy students 52 with musical training, 23 without (40 males, mean age 24.6±6.3 years; 35 females, mean age 22.7±3.7 years)",
      "Musical Training": "Mixed Groups",
      "Channel Count": "19 electrodes (10/20-system)",
      "channelCountValue": 19,
      "Findings": "Listening to Mozart caused strong alpha and theta power reductions, especially in the left hemisphere. Musically trained subjects showed broader alpha blocking and stronger interhemispheric beta 2 coherence, while untrained listeners had more limited effects. In groups split by sex, females showed more extensive interhemispheric coherence (especially beta 1–2) and alpha blocking than males, and trained females drove most of the coherence increases between posterior regions.",
      "Sampling Rate": "256 Hz",
      "Data Format": "Digitized analog recordings",
      "Preprocessing": [
        "Artifact removal by visual inspection; Fourier transformation of 2-sec epochs; averaging of power and cross-power spectra"
      ],
      "License": "Copyright by 1996 Elsevier Science B.V.",
      "EEG Analysis Techniques": [
        "Spectral analysis; broad band parameter extraction for five frequency bands; Fisher permutation test"
      ],
      "Statistical Tests": [
        "Fisher permutation test"
      ],
      "Publication": "Music Perception: An Interdisciplinary Journal, Winter 1988, Vol. 6, No. 2 (Winter, 1988), pp. 133-159",
      "year": 1988
    },
    {
      "id": "study-5",
      "Study Name": "The EEG: An Adequate Method to Concretize Brain Processes Elicited by Music",
      "Authors": "H. Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1988,
      "DOI/URL": "https://doi.org/10.2307/40285422",
      "Study Description": "This study investigates whether music processing by the brain can be reflected in ongoing EEG, examining parameters like location, power, frequency, and coherence to identify significant changes in brain electrical activity when listening to various musical stimuli.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Spoken Text",
      "Stimulus Description": "Recital of the fate of the sons of J.S. Bach taken from Geiringer's biography",
      "Passage Name": "Bach biography excerpt",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Brain electrical activity parameters (power",
        "coherence) in different frequency bands (theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "theta",
        "alpha",
        "beta",
        "coherence",
        "power",
        "spectral"
      ],
      "Task Description": "Binaural listening with eyes closed",
      "Number of Participants": "39",
      "participantsValue": 39,
      "Demographics": "17 females, 22 males",
      "Musical Training": "Mixed Groups",
      "Channel Count": "19 electrodes (10/20-system)",
      "channelCountValue": 19,
      "Findings": "Listening to spoken text caused localized alpha and theta power decreases in the left midtemporal area and beta 1 power increases in the right frontal region. Unlike music, it reduced interhemispheric coherence in posterior areas. Females showed stronger alpha reduction than males.",
      "Sampling Rate": "256 Hz",
      "Data Format": "Digitized analog recordings",
      "Preprocessing": [
        "Artifact removal by visual inspection; Fourier transformation of 2-sec epochs; averaging of power and cross-power spectra"
      ],
      "License": "Copyright by 1996 Elsevier Science B.V.",
      "EEG Analysis Techniques": [
        "Spectral analysis; broad band parameter extraction for five frequency bands; Fisher permutation test"
      ],
      "Statistical Tests": [
        "Fisher permutation test"
      ],
      "Publication": "Music Perception: An Interdisciplinary Journal, Winter 1988, Vol. 6, No. 2 (Winter, 1988), pp. 133-159",
      "year": 1988
    },
    {
      "id": "study-6",
      "Study Name": "The EEG: An Adequate Method to Concretize Brain Processes Elicited by Music",
      "Authors": "H. Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1988,
      "DOI/URL": "https://doi.org/10.2307/40285422",
      "Study Description": "This study investigates whether music processing by the brain can be reflected in ongoing EEG, examining parameters like location, power, frequency, and coherence to identify significant changes in brain electrical activity when listening to various musical stimuli.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Percussion",
      "Stimulus Description": "African Jungle Drums, from a record (Primitive Percussion—African Jungle Drums, POP Series R 6001)",
      "Passage Name": "African Jungle Drums",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Brain electrical activity parameters (power",
        "coherence) in different frequency bands (theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "theta",
        "alpha",
        "beta",
        "coherence",
        "power",
        "spectral"
      ],
      "Task Description": "Binaural listening with eyes closed",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "6 males, 2 females",
      "Musical Training": "Mixed Groups",
      "Channel Count": "19 electrodes (10/20-system)",
      "channelCountValue": 19,
      "Findings": "Drumming caused alpha power decrease in left midtemporal and central regions and increased theta coherence locally and interhemispherically. Strong motor responses (tapping) likely confounded some EEG changes. No group comparisons were made due to the small sample size.",
      "Sampling Rate": "256 Hz",
      "Data Format": "Digitized analog recordings",
      "Preprocessing": [
        "Artifact removal by visual inspection; Fourier transformation of 2-sec epochs; averaging of power and cross-power spectra"
      ],
      "License": "Copyright by 1996 Elsevier Science B.V.",
      "EEG Analysis Techniques": [
        "Spectral analysis; broad band parameter extraction for five frequency bands; Fisher permutation test"
      ],
      "Statistical Tests": [
        "Fisher permutation test"
      ],
      "Publication": "Music Perception: An Interdisciplinary Journal, Winter 1988, Vol. 6, No. 2 (Winter, 1988), pp. 133-159",
      "year": 1988
    },
    {
      "id": "study-7",
      "Study Name": "The EEG: An Adequate Method to Concretize Brain Processes Elicited by Music",
      "Authors": "H. Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1988,
      "DOI/URL": "https://doi.org/10.2307/40285422",
      "Study Description": "This study investigates whether music processing by the brain can be reflected in ongoing EEG, examining parameters like location, power, frequency, and coherence to identify significant changes in brain electrical activity when listening to various musical stimuli.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Humming",
      "Stimulus Description": "Voiceless humming of a children's song of participant's choice",
      "Composer": "Participant's choice",
      "Passage Name": "Children's song (participant's choice)",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Brain electrical activity parameters (power",
        "coherence) in different frequency bands (theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "theta",
        "alpha",
        "beta",
        "coherence",
        "power",
        "spectral"
      ],
      "Task Description": "Humming as voicelessly as possible",
      "Number of Participants": "11",
      "participantsValue": 11,
      "Demographics": "8 males, 3 females",
      "Musical Training": "Mixed Groups",
      "Channel Count": "19 electrodes (10/20-system)",
      "channelCountValue": 19,
      "Findings": "Humming reduced alpha power in bilateral temporal regions and theta power along the midline, with a large beta 3 power increase linked to motor control. Local beta 3 coherence rose in left hemisphere areas. Interhemispheric coupling increased between frontobasal and parietal regions.",
      "Sampling Rate": "256 Hz",
      "Data Format": "Digitized analog recordings",
      "Preprocessing": [
        "Artifact removal by visual inspection; Fourier transformation of 2-sec epochs; averaging of power and cross-power spectra"
      ],
      "License": "Copyright by 1996 Elsevier Science B.V.",
      "EEG Analysis Techniques": [
        "Spectral analysis; broad band parameter extraction for five frequency bands; Fisher permutation test"
      ],
      "Statistical Tests": [
        "Fisher permutation test"
      ],
      "Publication": "Music Perception: An Interdisciplinary Journal, Winter 1988, Vol. 6, No. 2 (Winter, 1988), pp. 133-159",
      "year": 1988
    },
    {
      "id": "study-8",
      "Study Name": "The EEG: An Adequate Method to Concretize Brain Processes Elicited by Music",
      "Authors": "H. Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1988,
      "DOI/URL": "https://doi.org/10.2307/40285422",
      "Study Description": "This study investigates whether music processing by the brain can be reflected in ongoing EEG, examining parameters like location, power, frequency, and coherence to identify significant changes in brain electrical activity when listening to various musical stimuli.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "German children's song 'Fuchs du hast die Gans gestohlen' played as a single melody on piano",
      "Composer": "Traditional German children's song",
      "Passage Name": "Fuchs du hast die Gans gestohlen",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Brain electrical activity parameters (power",
        "coherence) in different frequency bands (theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "theta",
        "alpha",
        "beta",
        "coherence",
        "power",
        "spectral"
      ],
      "Task Description": "Binaural listening with eyes closed",
      "Number of Participants": "19",
      "participantsValue": 19,
      "Demographics": "9 males, 10 females (music students)",
      "Musical Training": "Moderate Training (5-10 years)",
      "Channel Count": "19 electrodes (10/20-system)",
      "channelCountValue": 19,
      "Findings": "Listening to the children's melody caused alpha power reduction mainly in the left temporal and frontal areas, with more extensive local and interhemispheric coherence changes than during Mozart. Repeated presentations showed decreasing EEG changes, suggesting habituation.",
      "Sampling Rate": "256 Hz",
      "Data Format": "Digitized analog recordings",
      "Preprocessing": [
        "Artifact removal by visual inspection; Fourier transformation of 2-sec epochs; averaging of power and cross-power spectra"
      ],
      "License": "Copyright by 1996 Elsevier Science B.V.",
      "EEG Analysis Techniques": [
        "Spectral analysis; broad band parameter extraction for five frequency bands; Fisher permutation test"
      ],
      "Statistical Tests": [
        "Fisher permutation test"
      ],
      "Publication": "Music Perception: An Interdisciplinary Journal, Winter 1988, Vol. 6, No. 2 (Winter, 1988), pp. 133-159",
      "year": 1988
    },
    {
      "id": "study-9",
      "Study Name": "EEG in Music Psychological Studies",
      "Authors": "Helmuth Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.1007/978-3-642-79327-1_21",
      "Study Description": "EEG coherence and power were measured while listening to short synthesized tones.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Synthesized piano timbre (Yamaha TX7)",
      "Passage Length": "~30 seconds",
      "passageLengthSeconds": 30,
      "Musical Features Analyzed": [
        "Melodic structure",
        "harmonic context"
      ],
      "normalizedFeatures": [
        "harmony",
        "melody"
      ],
      "Task Description": "Passive listening during interictal period; EEG recorded via depth and surface electrodes",
      "Number of Participants": "2",
      "participantsValue": 2,
      "Demographics": "Adult males, 31 & 35 years, European",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Depth electrodes + surface EEG (Hess system)",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Coherence increased across hemispheres in task-specific patterns, especially in beta bands; music showed unique lateralization vs. verbal tasks.",
      "Sampling Rate": "256 Hz",
      "Recording Environment": "Hospital neurology lab",
      "Data Format": "Digital EEG (.edf format equivalent)",
      "Preprocessing": [
        "Artifact rejection; visual inspection; FFT windowing (256 samples)"
      ],
      "License": "Copyright by Springer, 1988",
      "EEG Analysis Techniques": [
        "Coherence analysis",
        "Spectral participation vectors",
        "Band power (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "Statistical Tests": [
        "Coherence computation",
        "frequency quotients"
      ],
      "Event Markers": [
        "Stimulus onset; synthesized tone presentation"
      ],
      "Publication": "Music Perception 6(1): 79–98",
      "year": 1993
    },
    {
      "id": "study-10",
      "Study Name": "EEG in Music Psychological Studies",
      "Authors": "Helmuth Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.1007/978-3-642-79327-1_21",
      "Study Description": "EEG coherence and power were measured while listening to short synthesized tones.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Pure sine tone",
      "Passage Length": "~30 seconds",
      "passageLengthSeconds": 30,
      "Musical Features Analyzed": [
        "Melodic structure",
        "harmonic context"
      ],
      "normalizedFeatures": [
        "harmony",
        "melody"
      ],
      "Task Description": "Passive listening during interictal period; EEG recorded via depth and surface electrodes",
      "Number of Participants": "2",
      "participantsValue": 2,
      "Demographics": "Adult males, 31 & 35 years, European",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Depth electrodes + surface EEG (Hess system)",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Sine tones showed less coherence increase than musical timbres; weaker hemispheric lateralization.",
      "Sampling Rate": "256 Hz",
      "Recording Environment": "Hospital neurology lab",
      "Data Format": "Digital EEG (.edf format equivalent)",
      "Preprocessing": [
        "Artifact rejection; visual inspection; FFT windowing (256 samples)"
      ],
      "License": "Copyright by Springer, 1989",
      "EEG Analysis Techniques": [
        "Coherence analysis",
        "Spectral participation vectors",
        "Band power (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "Statistical Tests": [
        "Coherence computation",
        "frequency quotients"
      ],
      "Event Markers": [
        "Stimulus onset; synthesized tone presentation"
      ],
      "Publication": "Music Perception 6(1): 79–99",
      "year": 1993
    },
    {
      "id": "study-11",
      "Study Name": "EEG in Music Psychological Studies",
      "Authors": "Helmuth Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.1007/978-3-642-79327-1_21",
      "Study Description": "EEG coherence and power were measured while listening to short synthesized tones.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Synthesized cello timbre",
      "Passage Length": "~30 seconds",
      "passageLengthSeconds": 30,
      "Musical Features Analyzed": [
        "Melodic structure",
        "harmonic context"
      ],
      "normalizedFeatures": [
        "harmony",
        "melody"
      ],
      "Task Description": "Passive listening during interictal period; EEG recorded via depth and surface electrodes",
      "Number of Participants": "2",
      "participantsValue": 2,
      "Demographics": "Adult males, 31 & 35 years, European",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Depth electrodes + surface EEG (Hess system)",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Coherence patterns differed from piano and sine, with midline beta-band engagement.",
      "Sampling Rate": "256 Hz",
      "Recording Environment": "Hospital neurology lab",
      "Data Format": "Digital EEG (.edf format equivalent)",
      "Preprocessing": [
        "Artifact rejection; visual inspection; FFT windowing (256 samples)"
      ],
      "License": "Copyright by Springer, 1990",
      "EEG Analysis Techniques": [
        "Coherence analysis",
        "Spectral participation vectors",
        "Band power (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "Statistical Tests": [
        "Coherence computation",
        "frequency quotients"
      ],
      "Event Markers": [
        "Stimulus onset; synthesized tone presentation"
      ],
      "Publication": "Music Perception 6(1): 79–100",
      "year": 1993
    },
    {
      "id": "study-12",
      "Study Name": "EEG in Music Psychological Studies",
      "Authors": "Helmuth Petsche, K. Lindner, P. Rappelsberger, G. Gruber",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.1007/978-3-642-79327-1_21",
      "Study Description": "EEG coherence and power were measured while listening to short synthesized tones.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Organ-like synthesized tone",
      "Passage Length": "~30 seconds",
      "passageLengthSeconds": 30,
      "Musical Features Analyzed": [
        "Melodic structure",
        "harmonic context"
      ],
      "normalizedFeatures": [
        "harmony",
        "melody"
      ],
      "Task Description": "Passive listening during interictal period; EEG recorded via depth and surface electrodes",
      "Number of Participants": "2",
      "participantsValue": 2,
      "Demographics": "Adult males, 31 & 35 years, European",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Depth electrodes + surface EEG (Hess system)",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Elicited broader coherence increases than sine or cello tones; varied across frequency bands.",
      "Sampling Rate": "256 Hz",
      "Recording Environment": "Hospital neurology lab",
      "Data Format": "Digital EEG (.edf format equivalent)",
      "Preprocessing": [
        "Artifact rejection; visual inspection; FFT windowing (256 samples)"
      ],
      "License": "Copyright by Springer, 1991",
      "EEG Analysis Techniques": [
        "Coherence analysis",
        "Spectral participation vectors",
        "Band power (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "Statistical Tests": [
        "Coherence computation",
        "frequency quotients"
      ],
      "Event Markers": [
        "Stimulus onset; synthesized tone presentation"
      ],
      "Publication": "Music Perception 6(1): 79–101",
      "year": 1993
    },
    {
      "id": "study-13",
      "Study Name": "EEG Coherence and Musical Thinking",
      "Authors": "Helmuth Petsche, Gerhard Doppelmayr, Peter Rappelsberger, Irenäus Simmerl, Barbara Maresch",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.2307/40285613",
      "Study Description": "EEG coherence was analyzed in multiple frequency bands while participants passively listened to different solo piano and jazz pieces. This study examines hemispheric and interhemispheric connectivity patterns across different musical styles.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Piano solo, Mozart – String Quartet KV 458, 1st movement",
      "Composer": "W.A. Mozart",
      "Passage Name": "Mozart: String Quartet KV 458, 1st movement",
      "Passage Length": "4 minutes",
      "passageLengthSeconds": 240,
      "Musical Features Analyzed": [
        "Coherence",
        "Hemispheric activation"
      ],
      "normalizedFeatures": [
        "arousal",
        "coherence",
        "power",
        "localization"
      ],
      "Task Description": "Passive listening with eyes closed; EEG recorded from 19 electrodes; music presented via headphones",
      "Number of Participants": "25",
      "participantsValue": 25,
      "Demographics": "Mixed gender, healthy participants (ages 13–68)",
      "Musical Training": "Not Reported",
      "EEG System Used": "Not Reported",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Broad bilateral coherence increases, especially in beta 3; high interhemispheric synchrony.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Visual artifact rejection; coherence computed using FFT; statistical threshold p ≤ 0.05"
      ],
      "License": "Copyright by University of California Press (1993)",
      "EEG Analysis Techniques": [
        "Coherence analysis (delta to beta3)"
      ],
      "Statistical Tests": [
        "Frequency-specific comparisons of coherence change (p ≤ 0.05)"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Music Perception 11(2):117–134",
      "year": 1993
    },
    {
      "id": "study-14",
      "Study Name": "EEG Coherence and Musical Thinking",
      "Authors": "Helmuth Petsche, Gerhard Doppelmayr, Peter Rappelsberger, Irenäus Simmerl, Barbara Maresch",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.2307/40285613",
      "Study Description": "EEG coherence was analyzed in multiple frequency bands while participants passively listened to different solo piano and jazz pieces. This study examines hemispheric and interhemispheric connectivity patterns across different musical styles.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Piano solo, J.S. Bach – Prelude and Fugue in A Minor",
      "Composer": "J.S. Bach",
      "Passage Name": "Prelude and Fugue in A Minor (Well-Tempered Clavichord I)",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Coherence",
        "Hemispheric activation"
      ],
      "normalizedFeatures": [
        "arousal",
        "coherence",
        "power",
        "localization"
      ],
      "Task Description": "Passive listening with eyes closed; EEG recorded from 19 electrodes; music presented via headphones",
      "Number of Participants": "15 Total",
      "participantsValue": 15,
      "Demographics": "Mixed gender, healthy participants (ages 13–68)",
      "Musical Training": "Not Reported",
      "EEG System Used": "Not Reported",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Strong left-lateralized coherence in beta bands; more asymmetric than Mozart.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Visual artifact rejection; coherence computed using FFT; statistical threshold p ≤ 0.06"
      ],
      "License": "Copyright by University of California Press (1993)",
      "EEG Analysis Techniques": [
        "Coherence analysis (delta to beta3)"
      ],
      "Statistical Tests": [
        "Frequency-specific comparisons of coherence change (p ≤ 0.05)"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Music Perception 11(2):117–135",
      "year": 1993
    },
    {
      "id": "study-15",
      "Study Name": "EEG Coherence and Musical Thinking",
      "Authors": "Helmuth Petsche, Gerhard Doppelmayr, Peter Rappelsberger, Irenäus Simmerl, Barbara Maresch",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.2307/40285613",
      "Study Description": "EEG coherence was analyzed in multiple frequency bands while participants passively listened to different solo piano and jazz pieces. This study examines hemispheric and interhemispheric connectivity patterns across different musical styles.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Piano solo, Beethoven – Sonata in A-flat Major, First Movement",
      "Composer": "Ludwig van Beethoven",
      "Passage Name": "Sonata in A-flat Major, First Movement",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Coherence",
        "Hemispheric activation"
      ],
      "normalizedFeatures": [
        "arousal",
        "coherence",
        "power",
        "localization"
      ],
      "Task Description": "Passive listening with eyes closed; EEG recorded from 19 electrodes; music presented via headphones",
      "Number of Participants": "15 Total",
      "participantsValue": 15,
      "Demographics": "Mixed gender, healthy participants (ages 13–68)",
      "Musical Training": "Not Reported",
      "EEG System Used": "Not Reported",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Modest coherence changes; less overall engagement than other excerpts.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Visual artifact rejection; coherence computed using FFT; statistical threshold p ≤ 0.07"
      ],
      "License": "Copyright by University of California Press (1993)",
      "EEG Analysis Techniques": [
        "Coherence analysis (delta to beta3)"
      ],
      "Statistical Tests": [
        "Frequency-specific comparisons of coherence change (p ≤ 0.05)"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Music Perception 11(2):117–136",
      "year": 1993
    },
    {
      "id": "study-16",
      "Study Name": "EEG Coherence and Musical Thinking",
      "Authors": "Helmuth Petsche, Gerhard Doppelmayr, Peter Rappelsberger, Irenäus Simmerl, Barbara Maresch",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.2307/40285613",
      "Study Description": "EEG coherence was analyzed in multiple frequency bands while participants passively listened to different solo piano and jazz pieces. This study examines hemispheric and interhemispheric connectivity patterns across different musical styles.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Piano solo, Schoenberg – Piano Pieces Op. 33a & b",
      "Composer": "Arnold Schoenberg",
      "Passage Name": "Piano Pieces Op. 33a & b",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Coherence",
        "Hemispheric activation"
      ],
      "normalizedFeatures": [
        "arousal",
        "coherence",
        "power",
        "localization"
      ],
      "Task Description": "Passive listening with eyes closed; EEG recorded from 19 electrodes; music presented via headphones",
      "Number of Participants": "15 Total",
      "participantsValue": 15,
      "Demographics": "Mixed gender, healthy participants (ages 13–68)",
      "Musical Training": "Not Reported",
      "EEG System Used": "Not Reported",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "High left-hemispheric coherence, especially in beta; weaker interhemispheric coupling.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Visual artifact rejection; coherence computed using FFT; statistical threshold p ≤ 0.08"
      ],
      "License": "Copyright by University of California Press (1993)",
      "EEG Analysis Techniques": [
        "Coherence analysis (delta to beta3)"
      ],
      "Statistical Tests": [
        "Frequency-specific comparisons of coherence change (p ≤ 0.05)"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Music Perception 11(2):117–137",
      "year": 1993
    },
    {
      "id": "study-17",
      "Study Name": "EEG Coherence and Musical Thinking",
      "Authors": "Helmuth Petsche, Gerhard Doppelmayr, Peter Rappelsberger, Irenäus Simmerl, Barbara Maresch",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.2307/40285613",
      "Study Description": "EEG coherence was analyzed in multiple frequency bands while participants passively listened to different solo piano and jazz pieces. This study examines hemispheric and interhemispheric connectivity patterns across different musical styles.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Jazz piano solo, Amalgame by A. Romano (performed by M. Petrucciani)",
      "Composer": "A. Romano",
      "Passage Name": "Amalgame",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Coherence",
        "Hemispheric activation"
      ],
      "normalizedFeatures": [
        "arousal",
        "coherence",
        "power",
        "localization"
      ],
      "Task Description": "Passive listening with eyes closed; EEG recorded from 19 electrodes; music presented via headphones",
      "Number of Participants": "15 Total",
      "participantsValue": 15,
      "Demographics": "Mixed gender, healthy participants (ages 13–68)",
      "Musical Training": "Not Reported",
      "EEG System Used": "Not Reported",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Coherence patterns similar to Schoenberg; strong left-lateralized beta activity.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Visual artifact rejection; coherence computed using FFT; statistical threshold p ≤ 0.09"
      ],
      "License": "Copyright by University of California Press (1993)",
      "EEG Analysis Techniques": [
        "Coherence analysis (delta to beta3)"
      ],
      "Statistical Tests": [
        "Frequency-specific comparisons of coherence change (p ≤ 0.05)"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Music Perception 11(2):117–138",
      "year": 1993
    },
    {
      "id": "study-18",
      "Study Name": "Spectral Analysis of the EEG as a Tool for Evaluating Expectancy Violations of Musical Contexts",
      "Authors": "Petr Janata, Helmuth Petsche",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.2307/40285571",
      "Study Description": "Musically trained participants listened to cadences resolving to either tonic, minor, or dissonant chords. EEG spectral amplitude and coherence were analyzed for expectancy",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Cadence + Resolution to Tonic (Best-Fitting)",
      "Passage Length": "6 seconds",
      "passageLengthSeconds": 6,
      "Musical Features Analyzed": [
        "Harmonic resolution",
        "expectancy fulfillment"
      ],
      "normalizedFeatures": [
        "harmony",
        "expectancy"
      ],
      "Task Description": "Passive listening with judgment of musical resolution quality; EEG recorded from 19 sites; key press or silent response",
      "Number of Participants": "23",
      "participantsValue": 23,
      "Demographics": "Music students from Vienna Hochschule für Musik (mean age 24.1 ± 2.7, 13 women, 10 men). All participants had instrumental training (avg. 14.4 ± 4.5 years) and most had theory training (avg. 4.2 ± 3.3 years)",
      "Musical Training": "Extensive Training (>10 years)",
      "EEG System Used": "Nihon Kohden EEG system, digitized with Walter Graphtek “Papierloses EEG”",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Right frontal beta coherence increased; minimal deviation from priming cadence in spectral activity; served as baseline for fulfillment.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated recording chamber",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection via eye-blink monitoring; visual inspection; 2-sec FFT windowing"
      ],
      "License": "Copyright by University of California Press (1993)",
      "EEG Analysis Techniques": [
        "Spectral amplitude",
        "local coherence",
        "interhemispheric coherence (delta–beta3)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA (within-subject: stimulus x response)"
      ],
      "Event Markers": [
        "Serial port trigger aligned to resolution onset"
      ],
      "Publication": "Music Perception 10(3):281–304",
      "year": 1993
    },
    {
      "id": "study-19",
      "Study Name": "Spectral Analysis of the EEG as a Tool for Evaluating Expectancy Violations of Musical Contexts",
      "Authors": "Petr Janata, Helmuth Petsche",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.2307/40285571",
      "Study Description": "Musically trained participants listened to cadences resolving to either tonic, minor, or dissonant chords. EEG spectral amplitude and coherence were analyzed for expectancy",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Cadence + Resolution to Minor (Ambiguous)",
      "Passage Length": "7 seconds",
      "passageLengthSeconds": 7,
      "Musical Features Analyzed": [
        "Harmonic resolution",
        "expectancy fulfillment"
      ],
      "normalizedFeatures": [
        "harmony",
        "expectancy"
      ],
      "Task Description": "Passive listening with judgment of musical resolution quality; EEG recorded from 19 sites; key press or silent response",
      "Number of Participants": "23",
      "participantsValue": 23,
      "Demographics": "Music students from Vienna Hochschule für Musik (mean age 24.1 ± 2.7, 13 women, 10 men). All participants had instrumental training (avg. 14.4 ± 4.5 years) and most had theory training (avg. 4.2 ± 3.3 years)",
      "Musical Training": "Extensive Training (>10 years)",
      "EEG System Used": "Nihon Kohden EEG system, digitized with Walter Graphtek “Papierloses EEG”",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "Slower reaction times and lower accuracy; EEG showed distinct patterns with increased local coherence at central/parietal sites and decreased frontal coherence—interpreted as ambiguity-related processing.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated recording chamber",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection via eye-blink monitoring; visual inspection; 2-sec FFT windowing"
      ],
      "License": "Copyright by University of California Press (1993)",
      "EEG Analysis Techniques": [
        "Spectral amplitude",
        "local coherence",
        "interhemispheric coherence (delta–beta3)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA (within-subject: stimulus x response)"
      ],
      "Event Markers": [
        "Serial port trigger aligned to resolution onset"
      ],
      "Publication": "Music Perception 10(3):281–304",
      "year": 1993
    },
    {
      "id": "study-20",
      "Study Name": "Spectral Analysis of the EEG as a Tool for Evaluating Expectancy Violations of Musical Contexts",
      "Authors": "Petr Janata, Helmuth Petsche",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.2307/40285571",
      "Study Description": "Musically trained participants listened to cadences resolving to either tonic, minor, or dissonant chords. EEG spectral amplitude and coherence were analyzed for expectancy",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Cadence + Resolution to Dissonant (Contextually Distant)",
      "Passage Length": "8 seconds",
      "passageLengthSeconds": 8,
      "Musical Features Analyzed": [
        "Harmonic resolution",
        "expectancy fulfillment"
      ],
      "normalizedFeatures": [
        "harmony",
        "expectancy"
      ],
      "Task Description": "Passive listening with judgment of musical resolution quality; EEG recorded from 19 sites; key press or silent response",
      "Number of Participants": "23",
      "participantsValue": 23,
      "Demographics": "Music students from Vienna Hochschule für Musik (mean age 24.1 ± 2.7, 13 women, 10 men). All participants had instrumental training (avg. 14.4 ± 4.5 years) and most had theory training (avg. 4.2 ± 3.3 years)",
      "Musical Training": "Extensive Training (>10 years)",
      "EEG System Used": "Nihon Kohden EEG system, digitized with Walter Graphtek “Papierloses EEG”",
      "Channel Count": "19 channels, 10–20 system",
      "channelCountValue": 19,
      "Findings": "EEG parameters (especially delta and beta coherence) showed stronger deviation from cadence baseline; right temporal amplitude and coherence increased, indicating strong expectancy violation detection.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated recording chamber",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection via eye-blink monitoring; visual inspection; 2-sec FFT windowing"
      ],
      "License": "Copyright by University of California Press (1993)",
      "EEG Analysis Techniques": [
        "Spectral amplitude",
        "local coherence",
        "interhemispheric coherence (delta–beta3)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA (within-subject: stimulus x response)"
      ],
      "Event Markers": [
        "Serial port trigger aligned to resolution onset"
      ],
      "Publication": "Music Perception 10(3):281–304",
      "year": 1993
    },
    {
      "id": "study-21",
      "Study Name": "EEG Power Spectrum Changes due to Listening to Pleasant Musics and Their Relation to Relaxation Effects",
      "Authors": "Michinori Kabuto, Takayuki Kageyama, Hiroshi Nitta",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.1265/jjh.48.807",
      "Study Description": "This study examined how listening to pleasant music influenced changes in EEG power spectra and psychosomatic relaxation in healthy young adults",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Classical violin sonata",
      "Composer": "Ludwig van Beethoven",
      "Passage Name": "Beethoven: Violin Sonata No. 9 (Kreutzer)",
      "Passage Length": "2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Delta",
        "Theta",
        "Alpha",
        "Beta power",
        "Alpha peak frequency",
        "Frontal/Occipital activity"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "power",
        "spectral",
        "localization"
      ],
      "Task Description": "Passive listening in sound-insulated room; pre- and post-questionnaires and FFT analysis",
      "Number of Participants": "42",
      "participantsValue": 42,
      "Demographics": "Healthy university students (ages 18–25), mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Nihon-Koden EEG Video System + FFT Analyzer (AD-3524/25)",
      "Channel Count": "8 channels, 10-20 system (Fp1-F3, Fp2-F4, F3-P3, F4-P4, P3-O1, P4-O2)",
      "channelCountValue": 8,
      "Findings": "Beethoven increased alpha activity and showed a left-occipital alpha peak shift associated with self-reported relaxation",
      "Sampling Rate": "Not Reported",
      "Recording Environment": "Sound-insulated room",
      "Data Format": "EEG digitized to VHS, then converted to FFT",
      "Preprocessing": [
        "Hanning window",
        "FFT",
        "50% overlap",
        "baseline regression"
      ],
      "License": "© Japanese Journal of Hygiene 1993",
      "EEG Analysis Techniques": [
        "Power spectrum",
        "Alpha peak frequency"
      ],
      "Statistical Tests": [
        "Multiple regression",
        "Principal component analysis",
        "GLM",
        "Pearson correlation",
        "t-tests"
      ],
      "Event Markers": [
        "Before vs. after music period"
      ],
      "Publication": "Japanese Journal of Hygiene 48:807–818",
      "year": 1993
    },
    {
      "id": "study-22",
      "Study Name": "EEG Power Spectrum Changes due to Listening to Pleasant Musics and Their Relation to Relaxation Effects",
      "Authors": "Michinori Kabuto, Takayuki Kageyama, Hiroshi Nitta",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.1265/jjh.48.807",
      "Study Description": "This study examined how listening to pleasant music influenced changes in EEG power spectra and psychosomatic relaxation in healthy young adults",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Classical piano waltz",
      "Composer": "Frederic Chopin",
      "Passage Name": "Chopin: Waltz Op. 64 No. 1 (Minute Waltz)",
      "Passage Length": "2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Delta",
        "Theta",
        "Alpha",
        "Beta power",
        "Alpha peak frequency",
        "Frontal/Occipital activity"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "power",
        "spectral",
        "localization"
      ],
      "Task Description": "Passive listening in sound-insulated room; pre- and post-questionnaires and FFT analysis",
      "Number of Participants": "42",
      "participantsValue": 42,
      "Demographics": "Healthy university students (ages 18–25), mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Nihon-Koden EEG Video System + FFT Analyzer (AD-3524/25)",
      "Channel Count": "8 channels, 10-20 system (Fp1-F3, Fp2-F4, F3-P3, F4-P4, P3-O1, P4-O2)",
      "channelCountValue": 8,
      "Findings": "Chopin increased overall alpha power; associated with subjective calmness and pleasantness, particularly in left-parietal regions",
      "Sampling Rate": "Not Reported",
      "Recording Environment": "Sound-insulated room",
      "Data Format": "EEG digitized to VHS, then converted to FFT",
      "Preprocessing": [
        "Hanning window",
        "FFT",
        "50% overlap",
        "baseline regression"
      ],
      "License": "© Japanese Journal of Hygiene 1993",
      "EEG Analysis Techniques": [
        "Power spectrum",
        "Alpha peak frequency"
      ],
      "Statistical Tests": [
        "Multiple regression",
        "Principal component analysis",
        "GLM",
        "Pearson correlation",
        "t-tests"
      ],
      "Event Markers": [
        "Before vs. after music period"
      ],
      "Publication": "Japanese Journal of Hygiene 48:807–818",
      "year": 1993
    },
    {
      "id": "study-23",
      "Study Name": "EEG Power Spectrum Changes due to Listening to Pleasant Musics and Their Relation to Relaxation Effects",
      "Authors": "Michinori Kabuto, Takayuki Kageyama, Hiroshi Nitta",
      "Year": 1993,
      "DOI/URL": "https://doi.org/10.1265/jjh.48.807",
      "Study Description": "This study examined how listening to pleasant music influenced changes in EEG power spectra and psychosomatic relaxation in healthy young adults",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Commercially produced “α-wave” music",
      "Composer": "Sony Corp. composers",
      "Passage Name": "NA (α-wave commercial relaxation track)",
      "Passage Length": "2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Delta",
        "Theta",
        "Alpha",
        "Beta power",
        "Alpha peak frequency",
        "Frontal/Occipital activity"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "power",
        "spectral",
        "localization"
      ],
      "Task Description": "Passive listening in sound-insulated room; pre- and post-questionnaires and FFT analysis",
      "Number of Participants": "42",
      "participantsValue": 42,
      "Demographics": "Healthy university students (ages 18–25), mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Nihon-Koden EEG Video System + FFT Analyzer (AD-3524/25)",
      "Channel Count": "8 channels, 10-20 system (Fp1-F3, Fp2-F4, F3-P3, F4-P4, P3-O1, P4-O2)",
      "channelCountValue": 8,
      "Findings": "Commercial α music produced the largest frontal alpha increase; strongest correlation with high relaxation and calmness scores",
      "Sampling Rate": "Not Reported",
      "Recording Environment": "Sound-insulated room",
      "Data Format": "EEG digitized to VHS, then converted to FFT",
      "Preprocessing": [
        "Hanning window",
        "FFT",
        "50% overlap",
        "baseline regression"
      ],
      "License": "© Japanese Journal of Hygiene 1993",
      "EEG Analysis Techniques": [
        "Power spectrum",
        "Alpha peak frequency"
      ],
      "Statistical Tests": [
        "Multiple regression",
        "Principal component analysis",
        "GLM",
        "Pearson correlation",
        "t-tests"
      ],
      "Event Markers": [
        "Before vs. after music period"
      ],
      "Publication": "Japanese Journal of Hygiene 48:807–818",
      "year": 1993
    },
    {
      "id": "study-24",
      "Study Name": "An Event-Related Potential (ERP) Study of Musical Expectancy: Comparison of Musicians With Nonmusicians",
      "Authors": "Mireille Besson, Frederique Faïta",
      "Year": 1995,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "This ERP study compared musicians and nonmusicians in their neural responses to harmonic and rhythmic violations in familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Familiar classical melody fragment with congruous terminal note (37-38 phrases)",
      "Composer": "Multiple classical composers",
      "Passage Length": "7–13 seconds",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Harmony (diatonic vs nondiatonic)",
        "Rhythm (600 ms delay)"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony"
      ],
      "Task Description": "Passive listening; post-phrase familiarity & note-congruity judgment; EEG recorded",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "15 musicians (≥7 yrs training) & 15 nonmusicians, ages 16–39, right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass P5 RPS107 amplifier",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Congruous terminal notes elicited smaller LPCs than violations; ERP differences were more pronounced in musicians.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Shielded laboratory room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Artifact rejection; ocular/muscle-trial removal; ERP averaging"
      ],
      "License": "Copyright © 1995 APA",
      "EEG Analysis Techniques": [
        "Event-related potential averaging (LPC",
        "N400-like); mean amplitude & latency measurements"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Terminal-note onset"
      ],
      "Publication": "Journal of Experimental Psychology: Human Perception and Performance 21(6):1278–1296",
      "year": 1995
    },
    {
      "id": "study-25",
      "Study Name": "An Event-Related Potential (ERP) Study of Musical Expectancy: Comparison of Musicians With Nonmusicians",
      "Authors": "Mireille Besson, Frederique Faïta",
      "Year": 1995,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "This ERP study compared musicians and nonmusicians in their neural responses to harmonic and rhythmic violations in familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Familiar classical melody fragment with out-of-key (nondiatonic) terminal note (37-38 phrases)",
      "Composer": "Multiple classical composers",
      "Passage Length": "7–13 seconds",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Harmony (diatonic vs nondiatonic)",
        "Rhythm (600 ms delay)"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony"
      ],
      "Task Description": "Passive listening; post-phrase familiarity & note-congruity judgment; EEG recorded",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "15 musicians (≥7 yrs training) & 15 nonmusicians, ages 16–39, right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass P5 RPS107 amplifier",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Out-of-key terminal notes evoked larger LPCs than in-key notes; musicians showed stronger effects.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Shielded laboratory room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Artifact rejection; ocular/muscle-trial removal; ERP averaging"
      ],
      "License": "Copyright © 1995 APA",
      "EEG Analysis Techniques": [
        "Event-related potential averaging (LPC",
        "N400-like); mean amplitude & latency measurements"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Terminal-note onset"
      ],
      "Publication": "Journal of Experimental Psychology: Human Perception and Performance 21(6):1278–1297",
      "year": 1995
    },
    {
      "id": "study-26",
      "Study Name": "An Event-Related Potential (ERP) Study of Musical Expectancy: Comparison of Musicians With Nonmusicians",
      "Authors": "Mireille Besson, Frederique Faïta",
      "Year": 1995,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "This ERP study compared musicians and nonmusicians in their neural responses to harmonic and rhythmic violations in familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Familiar classical melody fragment with in-key but non-cadential (diatonic) terminal note (37-38 phrases)",
      "Composer": "Multiple classical composers",
      "Passage Length": "7–13 seconds",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Harmony (diatonic vs nondiatonic)",
        "Rhythm (600 ms delay)"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony"
      ],
      "Task Description": "Passive listening; post-phrase familiarity & note-congruity judgment; EEG recorded",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "15 musicians (≥7 yrs training) & 15 nonmusicians, ages 16–39, right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass P5 RPS107 amplifier",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Diatonic non-cadential endings were harder to distinguish; musicians had more defined ERP differences.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Shielded laboratory room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Artifact rejection; ocular/muscle-trial removal; ERP averaging"
      ],
      "License": "Copyright © 1995 APA",
      "EEG Analysis Techniques": [
        "Event-related potential averaging (LPC",
        "N400-like); mean amplitude & latency measurements"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Terminal-note onset"
      ],
      "Publication": "Journal of Experimental Psychology: Human Perception and Performance 21(6):1278–1298",
      "year": 1995
    },
    {
      "id": "study-27",
      "Study Name": "An Event-Related Potential (ERP) Study of Musical Expectancy: Comparison of Musicians With Nonmusicians",
      "Authors": "Mireille Besson, Frederique Faïta",
      "Year": 1995,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "This ERP study compared musicians and nonmusicians in their neural responses to harmonic and rhythmic violations in familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Familiar classical melody fragment with delayed terminal note (600 ms silence) (37-38 phrases)",
      "Composer": "Multiple classical composers",
      "Passage Length": "7–13 seconds",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Harmony (diatonic vs nondiatonic)",
        "Rhythm (600 ms delay)"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony"
      ],
      "Task Description": "Passive listening; post-phrase familiarity & note-congruity judgment; EEG recorded",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "15 musicians (≥7 yrs training) & 15 nonmusicians, ages 16–39, right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass P5 RPS107 amplifier",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Delayed endings elicited distinct ERP signatures; timing violation detection was not modulated by training.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Shielded laboratory room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Artifact rejection; ocular/muscle-trial removal; ERP averaging"
      ],
      "License": "Copyright © 1995 APA",
      "EEG Analysis Techniques": [
        "Event-related potential averaging (LPC",
        "N400-like); mean amplitude & latency measurements"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Expected terminal-note time"
      ],
      "Publication": "Journal of Experimental Psychology: Human Perception and Performance 21(6):1278–1299",
      "year": 1995
    },
    {
      "id": "study-28",
      "Study Name": "An Event-Related Potential (ERP) Study of Musical Expectancy: Comparison of Musicians With Nonmusicians",
      "Authors": "Mireille Besson, Frederique Faïta",
      "Year": 1995,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "This ERP study compared musicians and nonmusicians in their neural responses to harmonic and rhythmic violations in familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Unfamiliar melody fragment composed for the experiment with congruous terminal note (37-38 phrases)",
      "Passage Length": "7–13 seconds",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Harmony (diatonic vs nondiatonic)",
        "Rhythm (600 ms delay)"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony"
      ],
      "Task Description": "Passive listening; post-phrase familiarity & note-congruity judgment; EEG recorded",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "15 musicians (≥7 yrs training) & 15 nonmusicians, ages 16–39, right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass P5 RPS107 amplifier",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Unfamiliar congruous melodies evoked late negativity; musicians processed them more efficiently.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Shielded laboratory room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Artifact rejection; ocular/muscle-trial removal; ERP averaging"
      ],
      "License": "Copyright © 1995 APA",
      "EEG Analysis Techniques": [
        "Event-related potential averaging (LPC",
        "N400-like); mean amplitude & latency measurements"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Terminal-note onset"
      ],
      "Publication": "Journal of Experimental Psychology: Human Perception and Performance 21(6):1278–1300",
      "year": 1995
    },
    {
      "id": "study-29",
      "Study Name": "An Event-Related Potential (ERP) Study of Musical Expectancy: Comparison of Musicians With Nonmusicians",
      "Authors": "Mireille Besson, Frederique Faïta",
      "Year": 1995,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "This ERP study compared musicians and nonmusicians in their neural responses to harmonic and rhythmic violations in familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Unfamiliar melody fragment composed for the experiment with out-of-key (nondiatonic) terminal note (37-38 phrases)",
      "Passage Length": "7–13 seconds",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Harmony (diatonic vs nondiatonic)",
        "Rhythm (600 ms delay)"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony"
      ],
      "Task Description": "Passive listening; post-phrase familiarity & note-congruity judgment; EEG recorded",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "15 musicians (≥7 yrs training) & 15 nonmusicians, ages 16–39, right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass P5 RPS107 amplifier",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Unfamiliar out-of-key endings produced strong LPCs in musicians; expertise shaped harmonic processing.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Shielded laboratory room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Artifact rejection; ocular/muscle-trial removal; ERP averaging"
      ],
      "License": "Copyright © 1995 APA",
      "EEG Analysis Techniques": [
        "Event-related potential averaging (LPC",
        "N400-like); mean amplitude & latency measurements"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Terminal-note onset"
      ],
      "Publication": "Journal of Experimental Psychology: Human Perception and Performance 21(6):1278–1301",
      "year": 1995
    },
    {
      "id": "study-30",
      "Study Name": "An Event-Related Potential (ERP) Study of Musical Expectancy: Comparison of Musicians With Nonmusicians",
      "Authors": "Mireille Besson, Frederique Faïta",
      "Year": 1995,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "This ERP study compared musicians and nonmusicians in their neural responses to harmonic and rhythmic violations in familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Unfamiliar melody fragment composed for the experiment with in-key but non-cadential (diatonic) terminal note (37-38 phrases)",
      "Passage Length": "7–13 seconds",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Harmony (diatonic vs nondiatonic)",
        "Rhythm (600 ms delay)"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony"
      ],
      "Task Description": "Passive listening; post-phrase familiarity & note-congruity judgment; EEG recorded",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "15 musicians (≥7 yrs training) & 15 nonmusicians, ages 16–39, right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass P5 RPS107 amplifier",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Diatonic unfamiliar endings showed reduced ERP contrast; musicians responded earlier and more strongly.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Shielded laboratory room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Artifact rejection; ocular/muscle-trial removal; ERP averaging"
      ],
      "License": "Copyright © 1995 APA",
      "EEG Analysis Techniques": [
        "Event-related potential averaging (LPC",
        "N400-like); mean amplitude & latency measurements"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Terminal-note onset"
      ],
      "Publication": "Journal of Experimental Psychology: Human Perception and Performance 21(6):1278–1302",
      "year": 1995
    },
    {
      "id": "study-31",
      "Study Name": "An Event-Related Potential (ERP) Study of Musical Expectancy: Comparison of Musicians With Nonmusicians",
      "Authors": "Mireille Besson, Frederique Faïta",
      "Year": 1995,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "This ERP study compared musicians and nonmusicians in their neural responses to harmonic and rhythmic violations in familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Unfamiliar melody fragment composed for the experiment with delayed terminal note (600 ms silence) (37-38 phrases)",
      "Passage Length": "7–13 seconds",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Harmony (diatonic vs nondiatonic)",
        "Rhythm (600 ms delay)"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony"
      ],
      "Task Description": "Passive listening; post-phrase familiarity & note-congruity judgment; EEG recorded",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "15 musicians (≥7 yrs training) & 15 nonmusicians, ages 16–39, right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Grass P5 RPS107 amplifier",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Delayed endings in unfamiliar melodies triggered prolonged positivity; effects seen across groups.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Shielded laboratory room",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Artifact rejection; ocular/muscle-trial removal; ERP averaging"
      ],
      "License": "Copyright © 1995 APA",
      "EEG Analysis Techniques": [
        "Event-related potential averaging (LPC",
        "N400-like); mean amplitude & latency measurements"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Expected terminal-note time"
      ],
      "Publication": "Journal of Experimental Psychology: Human Perception and Performance 21(6):1278–1303",
      "year": 1995
    },
    {
      "id": "study-32",
      "Study Name": "Changes in Alpha Band EEG Activity in the Frontal Area After Stimulation with Music of Different Affective Content",
      "Authors": "Tatsuya Iwaki, Mitsuo Hayashi, Tadao Hori",
      "Year": 1997,
      "DOI/URL": "https://doi.org/10.1037/0096-1523.21.6.1278",
      "Study Description": "Measured EEG changes during passive listening to emotionally distinct classical music (stimulating vs. calming)",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Orchestral excerpts with stimulating or calming affect",
      "Composer": "Gustav Holst",
      "Passage Name": "The Planets – Mars, Venus",
      "Passage Length": "3 minutes 40 seconds",
      "passageLengthSeconds": 180,
      "Musical Features Analyzed": [
        "Alpha-2 amplitude",
        "Frontal coherence",
        "Mood ratings"
      ],
      "normalizedFeatures": [
        "alpha",
        "coherence",
        "power",
        "localization"
      ],
      "Task Description": "Eyes-closed passive listening followed by mood questionnaire",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "University students, 5 male and 5 female, ages 21–28",
      "Musical Training": "No Formal Training",
      "EEG System Used": "TEAC SR-50 recorder, NEC San-Ei Signal Processor",
      "Channel Count": "7 channels, 10-20 system (Fz, Cz, Pz, L/R anterior temporal, L/R posterior temporal), left-mastoid reference",
      "channelCountValue": 7,
      "Findings": "Stimulating music increased frontal alpha-2 coherence and amplitude initially; calm music showed less coherence change",
      "Sampling Rate": "200 Hz",
      "Recording Environment": "Soundproof and air-conditioned chamber (darkened)",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "High-pass filter (0.5 Hz)",
        "Low-pass filter (30 Hz)",
        "FFT",
        "Hanning window",
        "Artifact rejection"
      ],
      "License": "© Perceptual and Motor Skills 1997",
      "EEG Analysis Techniques": [
        "Power spectrum (alpha-2)",
        "Coherence analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "t-tests"
      ],
      "Event Markers": [
        "Phase onset (rest",
        "music: first",
        "middle",
        "latter)"
      ],
      "Publication": "Perceptual and Motor Skills 84(2):515–526",
      "year": 1997
    },
    {
      "id": "study-33",
      "Study Name": "Brain Indices of Music Processing: \"Nonmusicians\" are Musical",
      "Authors": "Koelsch et al.",
      "Year": 2000,
      "DOI/URL": "https://doi.org/10.1162/089892900562183",
      "Study Description": "EEG was recorded while nonmusicians listened to chord sequences with harmonic violations to assess ERPs related to expectancy.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Chord sequences consisting of 5-chord cadences; 25% included a Neapolitan chord at the 3rd or 5th position; 10% included deviant instrument sounds",
      "Passage Length": "~5 seconds",
      "passageLengthSeconds": 5,
      "Musical Features Analyzed": [
        "Harmonic integration and violation detection (ERAN",
        "N5",
        "P3a",
        "P3b)"
      ],
      "normalizedFeatures": [
        "harmony",
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening with covert counting of deviant instrument chords",
      "Number of Participants": "18",
      "participantsValue": 18,
      "Demographics": "Adult nonmusicians (ages ~21–29, mixed gender)",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "25 channels, 10-20 system",
      "channelCountValue": 25,
      "Findings": "ERAN and N5 were stronger when violations occurred at the 5th chord position, suggesting stronger expectancy buildup. Chords at the 3rd position still elicited these effects but with smaller amplitude.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Acoustically and electrically shielded room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass filtering (0.5–40 Hz); artifact rejection"
      ],
      "License": "Copyright Massachusetts Institute of Technology Journal of Cognitive Neuroscience",
      "EEG Analysis Techniques": [
        "ERP analysis (ERAN",
        "N5",
        "P3 components)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Chord position markers (e.g.",
        "3rd",
        "5th; Neapolitan occurrence)"
      ],
      "Publication": "Journal of Cognitive Neuroscience 12 (3), 2000",
      "year": 2000
    },
    {
      "id": "study-34",
      "Study Name": "Brain Indices of Music Processing: \"Nonmusicians\" are Musical",
      "Authors": "Koelsch et al.",
      "Year": 2000,
      "DOI/URL": "https://doi.org/10.1162/089892900562183",
      "Study Description": "EEG was recorded while nonmusicians listened to chord sequences with harmonic violations to assess ERPs related to expectancy.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Same 5-chord sequences as Exp. 1, but Neapolitan chords replaced by dissonant tone clusters (nonharmonic violations)",
      "Passage Length": "~5 seconds",
      "passageLengthSeconds": 5,
      "Musical Features Analyzed": [
        "Violation magnitude and integration difficulty (ERAN",
        "N5",
        "P3a",
        "P3b)"
      ],
      "normalizedFeatures": [
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening with covert counting of deviant instrument chords",
      "Number of Participants": "18",
      "participantsValue": 18,
      "Demographics": "Adult nonmusicians (ages ~21–29, mixed gender)",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "25 channels, 10-20 system",
      "channelCountValue": 25,
      "Findings": "Stronger ERAN and N5 than Neapolitans, especially at the 3rd position, indicating greater violation due to both tonal and harmonic incongruity.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Acoustically and electrically shielded room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass filtering (0.5–40 Hz); artifact rejection"
      ],
      "License": "Copyright Massachusetts Institute of Technology Journal of Cognitive Neuroscience",
      "EEG Analysis Techniques": [
        "ERP analysis (ERAN",
        "N5",
        "P3 components)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Chord position markers; cluster occurrence"
      ],
      "Publication": "Journal of Cognitive Neuroscience 12 (3), 2000",
      "year": 2000
    },
    {
      "id": "study-35",
      "Study Name": "Brain Indices of Music Processing: \"Nonmusicians\" are Musical",
      "Authors": "Koelsch et al.",
      "Year": 2000,
      "DOI/URL": "https://doi.org/10.1162/089892900562183",
      "Study Description": "EEG was recorded while nonmusicians listened to chord sequences with harmonic violations to assess ERPs related to expectancy.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Chord sequences with Neapolitan chords; participants explicitly instructed to detect harmonic violations",
      "Passage Length": "~5 seconds",
      "passageLengthSeconds": 5,
      "Musical Features Analyzed": [
        "Harmonic violation processing modulated by attention (ERAN",
        "P3a",
        "P3b",
        "N5)"
      ],
      "normalizedFeatures": [
        "attention",
        "harmony",
        "erp",
        "expectancy"
      ],
      "Task Description": "Active detection of Neapolitan chords (task-relevant)",
      "Number of Participants": "18",
      "participantsValue": 18,
      "Demographics": "Adult nonmusicians (ages ~21–29, mixed gender)",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "25 channels, 10-20 system",
      "channelCountValue": 25,
      "Findings": "ERAN remained present and amplitude unchanged, even when task relevance increased. Additional P3 components emerged, especially for detected chords, showing attentional engagement.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Acoustically and electrically shielded room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass filtering (0.5–40 Hz); artifact rejection"
      ],
      "License": "Copyright Massachusetts Institute of Technology Journal of Cognitive Neuroscience",
      "EEG Analysis Techniques": [
        "ERP analysis (ERAN",
        "N5",
        "P3 components)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Chord position markers; behavioral detection markers"
      ],
      "Publication": "Journal of Cognitive Neuroscience 12 (3), 2000",
      "year": 2000
    },
    {
      "id": "study-36",
      "Study Name": "Brain Indices of Music Processing: \"Nonmusicians\" are Musical",
      "Authors": "Koelsch et al.",
      "Year": 2000,
      "DOI/URL": "https://doi.org/10.1162/089892900562183",
      "Study Description": "EEG was recorded while nonmusicians listened to chord sequences with harmonic violations to assess ERPs related to expectancy.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Chord sequences where the probability of out-of-key chords was varied to study expectancy",
      "Passage Length": "~5 seconds",
      "passageLengthSeconds": 5,
      "Musical Features Analyzed": [
        "Effects of probability on harmonic violation processing (ERAN",
        "N5)"
      ],
      "normalizedFeatures": [
        "harmony",
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening without explicit task instructions",
      "Number of Participants": "18",
      "participantsValue": 18,
      "Demographics": "Adult nonmusicians (ages ~21–29, mixed gender)",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "25 channels, 10-20 system",
      "channelCountValue": 25,
      "Findings": "As violation frequency decreased, ERAN and N5 amplitudes increased, confirming sensitivity to violation probability. This supports ERAN as an implicit marker of musical expectation, even in nonmusicians.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Acoustically and electrically shielded room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass filtering (0.5–40 Hz); artifact rejection"
      ],
      "License": "Copyright Massachusetts Institute of Technology Journal of Cognitive Neuroscience",
      "EEG Analysis Techniques": [
        "ERP analysis (ERAN",
        "N5)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Probability markers (frequency of deviant chords)"
      ],
      "Publication": "Journal of Cognitive Neuroscience 12 (3), 2000",
      "year": 2000
    },
    {
      "id": "study-37",
      "Study Name": "Differentiating ERAN and MMN: An ERP Study",
      "Authors": "Stefan Koelsch, Thomas C. Gunter, Erich Schröger, Mari Tervaniemi, Daniela Sammler, Angela D. Friederici",
      "Year": 2001,
      "DOI/URL": "https://doi.org/10.1097/00001756-200105250-00019",
      "Study Description": "Investigated whether the ERAN reflects cognitive processing of complex musical syntax compared to MMN.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Tone pairs rising or falling in pitch (abstract feature MMN)",
      "Passage Length": "5 events per sequence (tone pairs)",
      "passageLengthSeconds": 5,
      "Musical Features Analyzed": [
        "Abstract feature (pitch direction)"
      ],
      "normalizedFeatures": [
        "melody"
      ],
      "Task Description": "Play videogame while ignoring sounds",
      "Number of Participants": "28",
      "participantsValue": 28,
      "Demographics": "Right-handed adults (ages 19–28), 15 female",
      "Musical Training": "Nonmusicians (no formal training)",
      "EEG System Used": "Extended 10–20 EEG system",
      "Channel Count": "41 channels",
      "channelCountValue": 41,
      "Findings": "Deviant tone pairs elicited abstract feature MMN at ~160 ms; no difference in amplitude between 3rd and 5th position.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "EEG booth with video game distraction",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass 0.25–25 Hz; artifact rejection for drift",
        "EOG; baseline -50 to 0 ms"
      ],
      "License": "Lippincott Williams & Wilkins (2001)",
      "EEG Analysis Techniques": [
        "ERP (MMN extraction)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Second tone onset of tone pair"
      ],
      "Publication": "NeuroReport 12(7):1385–1389",
      "year": 2001
    },
    {
      "id": "study-38",
      "Study Name": "Differentiating ERAN and MMN: An ERP Study",
      "Authors": "Stefan Koelsch, Thomas C. Gunter, Erich Schröger, Mari Tervaniemi, Daniela Sammler, Angela D. Friederici",
      "Year": 2001,
      "DOI/URL": "https://doi.org/10.1097/00001756-200105250-00019",
      "Study Description": "Investigated whether the ERAN reflects cognitive processing of complex musical syntax compared to MMN.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Five-chord harmonic sequences with occasional Neapolitan chords",
      "Passage Length": "5 chords per sequence",
      "passageLengthSeconds": 5,
      "Musical Features Analyzed": [
        "Musical syntax (harmonic appropriateness)"
      ],
      "normalizedFeatures": [
        "harmony"
      ],
      "Task Description": "Play videogame while ignoring sounds",
      "Number of Participants": "28",
      "participantsValue": 28,
      "Demographics": "Right-handed adults (ages 19–28), 15 female",
      "Musical Training": "Nonmusicians (no formal training)",
      "EEG System Used": "Extended 10–20 EEG system",
      "Channel Count": "41 channels",
      "channelCountValue": 41,
      "Findings": "Neapolitan chords elicited ERAN at ~200 ms; ERAN larger at 5th vs 3rd position, indicating context-dependent musical expectancy processing.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "EEG booth with video game distraction",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass 0.25–25 Hz; artifact rejection for drift",
        "EOG; baseline -50 to 0 ms"
      ],
      "License": "Lippincott Williams & Wilkins (2001)",
      "EEG Analysis Techniques": [
        "ERP (ERAN extraction)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Chord onset (3rd and 5th position)"
      ],
      "Publication": "NeuroReport 12(7):1385–1389",
      "year": 2001
    },
    {
      "id": "study-39",
      "Study Name": "Differentiating ERAN and MMN: An ERP Study",
      "Authors": "Stefan Koelsch, Thomas C. Gunter, Erich Schröger, Mari Tervaniemi, Daniela Sammler, Angela D. Friederici",
      "Year": 2001,
      "DOI/URL": "https://doi.org/10.1097/00001756-200105250-00019",
      "Study Description": "Investigated whether the ERAN reflects cognitive processing of complex musical syntax compared to MMN.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Single 440 Hz standard tones and 496 Hz deviant tones",
      "Passage Length": "5 tones per sequence",
      "passageLengthSeconds": 5,
      "Musical Features Analyzed": [
        "Pitch deviation"
      ],
      "normalizedFeatures": [
        "melody"
      ],
      "Task Description": "Play videogame while ignoring sounds",
      "Number of Participants": "28",
      "participantsValue": 28,
      "Demographics": "Right-handed adults (ages 19–28), 15 female",
      "Musical Training": "Nonmusicians (no formal training)",
      "EEG System Used": "Extended 10–20 EEG system",
      "Channel Count": "41 channels",
      "channelCountValue": 41,
      "Findings": "Deviant tones elicited frequency MMN at ~100 ms; no difference in MMN amplitude between 3rd and 5th position.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "EEG booth with video game distraction",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass 0.25–25 Hz; artifact rejection for drift",
        "EOG; baseline -50 to 0 ms"
      ],
      "License": "Lippincott Williams & Wilkins (2001)",
      "EEG Analysis Techniques": [
        "ERP (frequency MMN extraction)"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA"
      ],
      "Event Markers": [
        "Tone onset (3rd and 5th position)"
      ],
      "Publication": "NeuroReport 12(7):1385–1389",
      "year": 2001
    },
    {
      "id": "study-40",
      "Study Name": "Brain Electrical Activity Evoked by Mental Formation of Auditory Expectations and Images",
      "Authors": "Petr Janata",
      "Year": 2001,
      "DOI/URL": "https://doi.org/10.1023/A:1007803102254",
      "Study Description": "ERP topographies were examined in response to internally generated auditory imagery and stimulus omissions using melodic fragments with varying expectancy and imagery tasks.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "D-major scale melodic fragments with brass timbre",
      "Passage Length": "4 seconds (8 notes, 500 ms each)",
      "passageLengthSeconds": 4,
      "Musical Features Analyzed": [
        "Imagery",
        "Expectancy",
        "ERP (N100",
        "P200",
        "P300)"
      ],
      "normalizedFeatures": [
        "erp",
        "expectancy",
        "imagery"
      ],
      "Task Description": "Active listening and imagery with timed lever presses during melody completions",
      "Number of Participants": "7",
      "participantsValue": 7,
      "Demographics": "Mixed gender, ages 18–42. ≥10 years musical training (mean = 20.7)",
      "Musical Training": "Extensive Training (>10 years)",
      "EEG System Used": "Electrical Geodesics Inc. Geodesic Electrode Net",
      "Channel Count": "128 channels, 10-20 system (extended)",
      "channelCountValue": 128,
      "Findings": "Emitted potentials occurred during imagery of the final three notes, with N100-like topographies resembling auditory-evoked potentials, including P200 and a P300-like centro-parietal positivity.",
      "Sampling Rate": "125 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "Baseline correction",
        "Bandpass filtering at 50 Hz",
        "ERP averaging"
      ],
      "License": "Human Sciences Press, Inc.",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "Topographic mapping",
        "Permutation analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Scheffé comparisons"
      ],
      "Event Markers": [
        "Time-locked to imagined note onsets and omissions"
      ],
      "Publication": "Brain Topography 13(3):169–187",
      "year": 2001
    },
    {
      "id": "study-41",
      "Study Name": "Brain Electrical Activity Evoked by Mental Formation of Auditory Expectations and Images",
      "Authors": "Petr Janata",
      "Year": 2001,
      "DOI/URL": "https://doi.org/10.1023/A:1007803102254",
      "Study Description": "ERP topographies were examined in response to internally generated auditory imagery and stimulus omissions using melodic fragments with varying expectancy and imagery tasks.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "D-major scale melodic fragments with brass timbre",
      "Passage Length": "4 seconds (8 notes, 500 ms each)",
      "passageLengthSeconds": 4,
      "Musical Features Analyzed": [
        "Imagery",
        "Expectancy",
        "ERP (N100",
        "P200)",
        "Omissions",
        "Continuations"
      ],
      "normalizedFeatures": [
        "erp",
        "expectancy",
        "imagery"
      ],
      "Task Description": "Imagery task with delayed probe judgment or cue-validity task with attention control",
      "Number of Participants": "3",
      "participantsValue": 3,
      "Demographics": "Ages 32–40, Mixed gender. ≥22 years musical training (mean = 25.3)",
      "Musical Training": "Extensive Training (>10 years)",
      "EEG System Used": "Electrical Geodesics Inc. Geodesic Electrode Net",
      "Channel Count": "129 channels, 10-20 system (extended)",
      "channelCountValue": 129,
      "Findings": "ERP topographies differed significantly between imagery and non-imagery conditions; emitted potentials occurred only when subjects imagined notes, showing early activity similar to heard tones, and topographical differences during expectancy violations.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "Z-score standardization",
        "Spline interpolation"
      ],
      "License": "Human Sciences Press, Inc.",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "Correlation matrix analysis",
        "D-statistic permutation"
      ],
      "Statistical Tests": [
        "Bonferroni-corrected correlation",
        "Permutation test"
      ],
      "Event Markers": [
        "Time-locked to expected and omitted note onsets"
      ],
      "Publication": "Brain Topography 13(3):169–187",
      "year": 2001
    },
    {
      "id": "study-42",
      "Study Name": "Musicians and the gamma band: a secret affair?",
      "Authors": "Joydeep Bhattacharya, Hellmuth Petsche",
      "Year": 2001,
      "DOI/URL": "https://doi.org/10.1097/00001756-200102120-00037",
      "Study Description": "EEG phase synchrony during passive listening to music or text was compared between musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "French Suite No. 5 (Gigue) for Harpsichord",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "French Suite No. 5, Gigue",
      "Passage Length": "90 seconds",
      "passageLengthSeconds": 90,
      "Musical Features Analyzed": [
        "Gamma band (30–50 Hz)",
        "Phase synchrony",
        "Hemispheric lateralization"
      ],
      "normalizedFeatures": [
        "gamma",
        "coherence",
        "localization"
      ],
      "Task Description": "Passive listening to Bach harpsichord piece",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adult males, 10 musicians (≥5 years training) and 10 nonmusicians, mean age ~25.5",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Not Reported",
      "Channel Count": "19 channels, 10-20 system (Fz, Cz, Pz, etc.)",
      "channelCountValue": 19,
      "Findings": "Music listening induced significantly higher gamma-band phase synchrony in musicians than non-musicians, especially in midline and left-hemispheric regions.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Laboratory recording setting",
      "Data Format": "Not reported",
      "Preprocessing": [
        "Baseline drift removal",
        "Bandpass filtering",
        "Hilbert transform",
        "Windowed averaging"
      ],
      "License": "Lippincott Williams & Wilkins",
      "EEG Analysis Techniques": [
        "Phase synchrony index via Hilbert transform",
        "Topographic profiling",
        "Normalization"
      ],
      "Statistical Tests": [
        "Wilcoxon test",
        "Mann-Whitney rank sum",
        "Entropy-based statistics"
      ],
      "Event Markers": [
        "Time-locked to continuous music listening"
      ],
      "Publication": "NeuroReport 12(2):371–374",
      "year": 2001
    },
    {
      "id": "study-43",
      "Study Name": "Musicians and the gamma band: a secret affair?",
      "Authors": "Joydeep Bhattacharya, Hellmuth Petsche",
      "Year": 2001,
      "DOI/URL": "https://doi.org/10.1097/00001756-200102120-00037",
      "Study Description": "EEG phase synchrony during passive listening to music or text was compared between musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Spoken Text",
      "Stimulus Description": "Neutral German short story read aloud by female speaker",
      "Composer": "H. Weigel",
      "Passage Name": "Verständigung gegen die Nachwelt",
      "Passage Length": "90 seconds",
      "passageLengthSeconds": 90,
      "Musical Features Analyzed": [
        "Gamma band (30–50 Hz)",
        "Phase synchrony",
        "Hemispheric lateralization"
      ],
      "normalizedFeatures": [
        "gamma",
        "coherence",
        "localization"
      ],
      "Task Description": "Passive listening to neutral spoken story",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adult males, 10 musicians (≥5 years training) and 10 nonmusicians, mean age ~25.5",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Not Reported",
      "Channel Count": "19 channels, 10-20 system (Fz, Cz, Pz, etc.)",
      "channelCountValue": 19,
      "Findings": "No significant differences between groups were observed during text listening; phase synchrony was generally lower across all participants.",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Laboratory recording setting",
      "Data Format": "Not reported",
      "Preprocessing": [
        "Baseline drift removal",
        "Bandpass filtering",
        "Hilbert transform",
        "Windowed averaging"
      ],
      "License": "Lippincott Williams & Wilkins",
      "EEG Analysis Techniques": [
        "Phase synchrony index via Hilbert transform",
        "Topographic profiling",
        "Normalization"
      ],
      "Statistical Tests": [
        "Wilcoxon test",
        "Mann-Whitney rank sum",
        "Entropy-based statistics"
      ],
      "Event Markers": [
        "Time-locked to continuous text listening"
      ],
      "Publication": "NeuroReport 12(2):371–374",
      "year": 2001
    },
    {
      "id": "study-44",
      "Study Name": "Automatic Encoding of Polyphonic Melodies in Musicians and Nonmusicians",
      "Authors": "Takako Fujioka, Laurel J. Trainor, Bernhard Ross, Ryusuke Kakigi, Christo Pantev",
      "Year": 2005,
      "DOI/URL": "https://doi.org/10.1162/089892905774597263",
      "Study Description": "Study examined MMNm responses to deviant notes in polyphonic and monophonic melodic contexts in musicians and nonmusicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Two 5-note melodies (A and B) combined polyphonically using piano samples",
      "Passage Length": "1.5 seconds",
      "passageLengthSeconds": 1.5,
      "Musical Features Analyzed": [
        "MMNm",
        "Pitch",
        "Contour",
        "Interval",
        "Voice (High/Low)",
        "Tonality (In-key/Out-of-key)"
      ],
      "normalizedFeatures": [
        "harmony",
        "melody"
      ],
      "Task Description": "Passive listening to polyphonic melody pairs (oddball paradigm) with MEG recording",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adult males, 10 musicians (≥5 years training) and 10 nonmusicians, mean age ~25.5",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Whole-head MEG system (CTF OMEGA)",
      "Channel Count": "151 channels",
      "channelCountValue": 151,
      "Findings": "Polyphonic condition elicited stronger MMNm overall; musicians showed larger MMNm than nonmusicians, especially to high-voice deviants; in-key deviants evoked larger MMNm than out-of-key; MMNm was left-lateralized in musicians and right-lateralized in nonmusicians.",
      "Sampling Rate": "312.5 Hz",
      "Recording Environment": "Magnetically shielded room",
      "Data Format": "Digital MEG signals",
      "Preprocessing": [
        "Artifact rejection",
        "Signal-space projection",
        "Baseline correction",
        "Bootstrap resampling"
      ],
      "License": "Massachusetts Institute of Technology",
      "EEG Analysis Techniques": [
        "MMNm via signal space projection",
        "Auditory dipole modeling",
        "Bootstrapped confidence intervals"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA (4-way)",
        "Fisher’s PLSD",
        "t-tests"
      ],
      "Event Markers": [
        "Event-locked to 5th note onset of melodies"
      ],
      "Publication": "Journal of Cognitive Neuroscience 17(10):1578–1592",
      "year": 2005
    },
    {
      "id": "study-45",
      "Study Name": "Automatic Encoding of Polyphonic Melodies in Musicians and Nonmusicians",
      "Authors": "Takako Fujioka, Laurel J. Trainor, Bernhard Ross, Ryusuke Kakigi, Christo Pantev",
      "Year": 2005,
      "DOI/URL": "https://doi.org/10.1162/089892905774597263",
      "Study Description": "Study examined MMNm responses to deviant notes in polyphonic and monophonic melodic contexts in musicians and nonmusicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "One 5-note melody (A or B), presented without polyphonic pairing",
      "Passage Length": "1.5 seconds",
      "passageLengthSeconds": 1.5,
      "Musical Features Analyzed": [
        "MMNm",
        "Pitch",
        "Contour",
        "Interval",
        "Tonality (In-key/Out-of-key)"
      ],
      "normalizedFeatures": [
        "harmony",
        "melody"
      ],
      "Task Description": "Passive listening to single melodies with occasional deviant final notes",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adult males, 10 musicians (≥5 years training) and 10 nonmusicians, mean age ~25.5",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Whole-head MEG system (CTF OMEGA)",
      "Channel Count": "151 channels",
      "channelCountValue": 151,
      "Findings": "Monophonic condition elicited MMNm in both groups but with reduced amplitude compared to polyphonic condition; musicians still had greater MMNm overall; behavioral performance was better for out-of-key deviants and higher in musicians.",
      "Sampling Rate": "312.5 Hz",
      "Recording Environment": "Magnetically shielded room",
      "Data Format": "Digital MEG signals",
      "Preprocessing": [
        "Artifact rejection",
        "Signal-space projection",
        "Baseline correction",
        "Bootstrap resampling"
      ],
      "License": "Massachusetts Institute of Technology",
      "EEG Analysis Techniques": [
        "MMNm via signal space projection",
        "Auditory dipole modeling",
        "Bootstrapped confidence intervals"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA (4-way)",
        "Fisher’s PLSD",
        "t-tests"
      ],
      "Event Markers": [
        "Event-locked to 5th note onset of melodies"
      ],
      "Publication": "Journal of Cognitive Neuroscience 17(10):1578–1592",
      "year": 2005
    },
    {
      "id": "study-46",
      "Study Name": "Discovering EEG Signals Response to Musical Signal Stimuli by Time-frequency analysis and Independent Component Analysis",
      "Authors": "Wei-Chih Lin, Hung-Wen Chiu, Chien-Yeh Hsu",
      "Year": 2005,
      "DOI/URL": "https://doi.org/10.1109/IEMBS.2005.1617036",
      "Study Description": "Metal music produced high between-subject agreement; most consistent spectral profiles; lower variation in ICA maps across subjects",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Heavy metal music with high rhythmic intensity",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Frequency band power (delta",
        "theta",
        "alpha",
        "beta",
        "gamma)",
        "ICA spatial patterns",
        "Time-frequency plots"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "gamma",
        "power",
        "spectral"
      ],
      "Task Description": "Passive listening with eyes closed; 5 music segments presented through earphones",
      "Number of Participants": "6",
      "participantsValue": 6,
      "Demographics": "3 male, 3 female, ages 20–28, college students",
      "Musical Training": "Not reported",
      "EEG System Used": "Stellate Harmonie",
      "Channel Count": "21 channels, 10–20 system, Fp1–O2, A1/A2 linked ears",
      "channelCountValue": 21,
      "Findings": "Frontal channels showed strong consistent power response; metal induced similar EEG patterns across listeners",
      "Sampling Rate": "200 Hz",
      "Recording Environment": "Sound-controlled lab room using headphones",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "FFT",
        "Normalization",
        "Artifact rejection",
        "ICA applied for noise separation"
      ],
      "License": "© 2005 IEEE",
      "EEG Analysis Techniques": [
        "Time-frequency analysis",
        "Independent Component Analysis",
        "Spectral normalization"
      ],
      "Statistical Tests": [
        "Correlation coefficient analysis (within/between subjects)"
      ],
      "Event Markers": [
        "Segment timing by stimulus category"
      ],
      "Publication": "EMBC 2005 Proceedings, Shanghai, China",
      "year": 2005
    },
    {
      "id": "study-47",
      "Study Name": "Discovering EEG Signals Response to Musical Signal Stimuli by Time-frequency analysis and Independent Component Analysis",
      "Authors": "Wei-Chih Lin, Hung-Wen Chiu, Chien-Yeh Hsu",
      "Year": 2005,
      "DOI/URL": "https://doi.org/10.1109/IEMBS.2005.1617036",
      "Study Description": "Sonata music induced elevated alpha-band power (8–12 Hz), especially during middle segments; lower cross-subject similarity compared to metal music",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Classical sonata music (e.g., Mozart K. 448)",
      "Composer": "Wolfgang Amadeus Mozart",
      "Passage Name": "Sonata for Two Pianos in D Major, K.448",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Frequency band power (delta",
        "theta",
        "alpha",
        "beta",
        "gamma)",
        "ICA spatial patterns",
        "Time-frequency plots"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "gamma",
        "power",
        "spectral"
      ],
      "Task Description": "Passive listening with eyes closed; 5 music segments presented through earphones",
      "Number of Participants": "6",
      "participantsValue": 6,
      "Demographics": "3 male, 3 female, ages 20–28, college students",
      "Musical Training": "Not reported",
      "EEG System Used": "Stellate Harmonie",
      "Channel Count": "21 channels, 10–20 system, Fp1–O2, A1/A2 linked ears",
      "channelCountValue": 21,
      "Findings": "Frontal and parietal regions (F3, F4, Pz) showed differential power activation; more subject-specific EEG patterns in response to sonata",
      "Sampling Rate": "200 Hz",
      "Recording Environment": "Sound-controlled lab room using headphones",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "FFT",
        "Normalization",
        "Artifact rejection",
        "ICA applied for noise separation"
      ],
      "License": "© 2005 IEEE",
      "EEG Analysis Techniques": [
        "Time-frequency analysis",
        "Independent Component Analysis",
        "Spectral normalization"
      ],
      "Statistical Tests": [
        "Correlation coefficient analysis (within/between subjects)"
      ],
      "Event Markers": [
        "Segment timing by stimulus category"
      ],
      "Publication": "EMBC 2005 Proceedings, Shanghai, China",
      "year": 2005
    },
    {
      "id": "study-48",
      "Study Name": "Discovering EEG Signals Response to Musical Signal Stimuli by Time-frequency analysis and Independent Component Analysis",
      "Authors": "Wei-Chih Lin, Hung-Wen Chiu, Chien-Yeh Hsu",
      "Year": 2005,
      "DOI/URL": "https://doi.org/10.1109/IEMBS.2005.1617036",
      "Study Description": "Favorite songs produced diverse EEG responses; lowest between-subject correlation; activity varied across all frequency bands",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Favorite song chosen by each subject (varied genre)",
      "Composer": "Varied",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Frequency band power (delta",
        "theta",
        "alpha",
        "beta",
        "gamma)",
        "ICA spatial patterns",
        "Time-frequency plots"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "gamma",
        "power",
        "spectral"
      ],
      "Task Description": "Passive listening with eyes closed; 5 music segments presented through earphones",
      "Number of Participants": "6",
      "participantsValue": 6,
      "Demographics": "3 male, 3 female, ages 20–28, college students",
      "Musical Training": "Not reported",
      "EEG System Used": "Stellate Harmonie",
      "Channel Count": "21 channels, 10–20 system, Fp1–O2, A1/A2 linked ears",
      "channelCountValue": 21,
      "Findings": "Highly individual EEG patterns; responses depended on genre and personal familiarity",
      "Sampling Rate": "200 Hz",
      "Recording Environment": "Sound-controlled lab room using headphones",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "FFT",
        "Normalization",
        "Artifact rejection",
        "ICA applied for noise separation"
      ],
      "License": "© 2005 IEEE",
      "EEG Analysis Techniques": [
        "Time-frequency analysis",
        "Independent Component Analysis",
        "Spectral normalization"
      ],
      "Statistical Tests": [
        "Correlation coefficient analysis (within/between subjects)"
      ],
      "Event Markers": [
        "Segment timing by stimulus category"
      ],
      "Publication": "EMBC 2005 Proceedings, Shanghai, China",
      "year": 2005
    },
    {
      "id": "study-49",
      "Study Name": "The Perception of Musical Phrase Structure: A Cross-Cultural ERP Study",
      "Authors": "Yun Nan, Thomas R. Knösche, Angela D. Friederici",
      "Year": 2006,
      "DOI/URL": "https://doi.org/10.1016/j.brainres.2006.03.115",
      "Study Description": "ERP study comparing how German and Chinese musicians perceive phrasing in culturally familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Western biphrasal melodies with phrased and unphrased conditions",
      "Passage Length": "3–17 seconds (varied)",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Phrase boundary perception",
        "CPS",
        "N1",
        "P2",
        "Cultural familiarity"
      ],
      "normalizedFeatures": [
        "erp"
      ],
      "Task Description": "ERP while categorizing melody as Western, Chinese, or combined",
      "Number of Participants": "12",
      "participantsValue": 12,
      "Demographics": "German female musicians, ages 21–37, Formal Training Start Age Mean: 7",
      "Musical Training": "Moderate Training (5-10 years)",
      "EEG System Used": "Ag-AgCl cap",
      "Channel Count": "51 channels, 10-20 system",
      "channelCountValue": 51,
      "Findings": "The CPS was elicited across all conditions; early ERP components (N1, P2) were modulated by musical style and cultural background.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Electrically shielded room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "Bandpass filtering (0.25–100 Hz)",
        "Baseline correction",
        "Manual inspection"
      ],
      "License": "Elsevier",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "Running t-tests",
        "Time-windowed component analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA (4-way)",
        "Session × Group ANOVA"
      ],
      "Event Markers": [
        "Trigger at phrase boundary offset"
      ],
      "Publication": "Brain Research 1094:179–191",
      "year": 2006
    },
    {
      "id": "study-50",
      "Study Name": "The Perception of Musical Phrase Structure: A Cross-Cultural ERP Study",
      "Authors": "Yun Nan, Thomas R. Knösche, Angela D. Friederici",
      "Year": 2006,
      "DOI/URL": "https://doi.org/10.1016/j.brainres.2006.03.115",
      "Study Description": "ERP study comparing how German and Chinese musicians perceive phrasing in culturally familiar and unfamiliar melodies.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Chinese biphrasal melodies with phrased and unphrased conditions",
      "Passage Length": "3–17 seconds (varied)",
      "passageLengthSeconds": 10,
      "Musical Features Analyzed": [
        "Phrase boundary perception",
        "CPS",
        "N1",
        "P2",
        "Cultural familiarity"
      ],
      "normalizedFeatures": [
        "erp"
      ],
      "Task Description": "ERP while categorizing melody as Western, Chinese, or combined",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Chinese female musicians, ages 21–37, Formal Training Start Age Mean: 8",
      "Musical Training": "Moderate Training (5-10 years)",
      "EEG System Used": "Ag-AgCl cap",
      "Channel Count": "51 channels, 10-20 system",
      "channelCountValue": 51,
      "Findings": "The CPS occurred regardless of musical style, but cultural background influenced early ERP responses to phrasing.",
      "Sampling Rate": "250 Hz",
      "Recording Environment": "Electrically shielded room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "Bandpass filtering (0.25–100 Hz)",
        "Baseline correction",
        "Manual inspection"
      ],
      "License": "Elsevier",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "Running t-tests",
        "Time-windowed component analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA (4-way)",
        "Session × Group ANOVA"
      ],
      "Event Markers": [
        "Trigger at phrase boundary offset"
      ],
      "Publication": "Brain Research 1094:179–191",
      "year": 2006
    },
    {
      "id": "study-51",
      "Study Name": "Effects of Unexpected Chords and of Performer’s Expression on Brain Responses and Electrodermal Activity",
      "Authors": "Stefan Koelsch, Simone Kilches, Nikolaus Steinbeis, Stefanie Schelinski",
      "Year": 2008,
      "DOI/URL": "https://doi.org/10.1371/journal.pone.0002631",
      "Study Description": "The study examined how harmonic expectancy and expressive performance affect brain and physiological responses.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Original excerpts with unexpected chords, expressive performance",
      "Composer": "Ludwig van Beethoven, Franz Schubert, W.A. Mozart, Joseph Haydn",
      "Passage Length": "8–16 seconds",
      "passageLengthSeconds": 12,
      "Musical Features Analyzed": [
        "Music-syntactic expectancy",
        "ERAN",
        "N5",
        "SCR",
        "Emotional arousal"
      ],
      "normalizedFeatures": [
        "emotion",
        "arousal",
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening with timbre-deviant detection (button press)",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed nonmusicians, ages 19–29, 10 female",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "32 channels, 10-20 system + SCR + ECG",
      "channelCountValue": 32,
      "Findings": "ERAN present; no effect of expression. N5 larger in expressive. SCRs elevated for unexpected chords and expressive performance.",
      "Sampling Rate": "500 Hz",
      "Recording Environment": "Soundproofed room with fixation cross",
      "Data Format": "Digital EEG and physiological traces",
      "Preprocessing": [
        "Artifact rejection",
        "Bandpass filtering (0.25–25 Hz)",
        "Baseline correction",
        "Manual inspection"
      ],
      "License": "Creative Commons Attribution License",
      "EEG Analysis Techniques": [
        "ERP averaging (ERAN",
        "N5)",
        "SCR amplitude averaging",
        "ROI-based analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "Chi-square tests"
      ],
      "Event Markers": [
        "Time-locked to final chord onset in each excerpt"
      ],
      "Publication": "PLoS ONE 3(7): e2631",
      "year": 2008
    },
    {
      "id": "study-52",
      "Study Name": "Effects of Unexpected Chords and of Performer’s Expression on Brain Responses and Electrodermal Activity",
      "Authors": "Stefan Koelsch, Simone Kilches, Nikolaus Steinbeis, Stefanie Schelinski",
      "Year": 2008,
      "DOI/URL": "https://doi.org/10.1371/journal.pone.0002631",
      "Study Description": "The study examined how harmonic expectancy and expressive performance affect brain and physiological responses.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Original excerpts with unexpected chords, non-expressive performance",
      "Composer": "Ludwig van Beethoven, Franz Schubert, W.A. Mozart, Joseph Haydn",
      "Passage Length": "8–16 seconds",
      "passageLengthSeconds": 12,
      "Musical Features Analyzed": [
        "Music-syntactic expectancy",
        "ERAN",
        "N5",
        "SCR",
        "Emotional arousal"
      ],
      "normalizedFeatures": [
        "emotion",
        "arousal",
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening with timbre-deviant detection (button press)",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed nonmusicians, ages 19–29, 10 female",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "32 channels, 10-20 system + SCR + ECG",
      "channelCountValue": 32,
      "Findings": "ERAN present; no effect of expression. N5 larger in expressive. SCRs elevated for unexpected chords and expressive performance.",
      "Sampling Rate": "500 Hz",
      "Recording Environment": "Soundproofed room with fixation cross",
      "Data Format": "Digital EEG and physiological traces",
      "Preprocessing": [
        "Artifact rejection",
        "Bandpass filtering (0.25–25 Hz)",
        "Baseline correction",
        "Manual inspection"
      ],
      "License": "Creative Commons Attribution License",
      "EEG Analysis Techniques": [
        "ERP averaging (ERAN",
        "N5)",
        "SCR amplitude averaging",
        "ROI-based analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "Chi-square tests"
      ],
      "Event Markers": [
        "Time-locked to final chord onset in each excerpt"
      ],
      "Publication": "PLoS ONE 3(7): e2631",
      "year": 2008
    },
    {
      "id": "study-53",
      "Study Name": "Effects of Unexpected Chords and of Performer’s Expression on Brain Responses and Electrodermal Activity",
      "Authors": "Stefan Koelsch, Simone Kilches, Nikolaus Steinbeis, Stefanie Schelinski",
      "Year": 2008,
      "DOI/URL": "https://doi.org/10.1371/journal.pone.0002631",
      "Study Description": "The study examined how harmonic expectancy and expressive performance affect brain and physiological responses.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Excerpts with expected (tonic) chords, expressive performance",
      "Composer": "Ludwig van Beethoven, Franz Schubert, W.A. Mozart, Joseph Haydn",
      "Passage Length": "8–16 seconds",
      "passageLengthSeconds": 12,
      "Musical Features Analyzed": [
        "Expected cadence",
        "SCR",
        "Emotional valence baseline"
      ],
      "normalizedFeatures": [
        "emotion",
        "valence",
        "harmony"
      ],
      "Task Description": "Passive listening with timbre-deviant detection (button press)",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed nonmusicians, ages 19–29, 10 female",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "32 channels, 10-20 system + SCR + ECG",
      "channelCountValue": 32,
      "Findings": "No ERAN or N5 effect. Used as expected cadence baseline. Lower SCRs overall.",
      "Sampling Rate": "500 Hz",
      "Recording Environment": "Soundproofed room with fixation cross",
      "Data Format": "Digital EEG and physiological traces",
      "Preprocessing": [
        "Artifact rejection",
        "Bandpass filtering (0.25–25 Hz)",
        "Baseline correction",
        "Manual inspection"
      ],
      "License": "Creative Commons Attribution License",
      "EEG Analysis Techniques": [
        "ERP averaging (baseline)",
        "SCR control"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "Chi-square tests"
      ],
      "Event Markers": [
        "Time-locked to final chord onset in each excerpt"
      ],
      "Publication": "PLoS ONE 3(7): e2631",
      "year": 2008
    },
    {
      "id": "study-54",
      "Study Name": "Effects of Unexpected Chords and of Performer’s Expression on Brain Responses and Electrodermal Activity",
      "Authors": "Stefan Koelsch, Simone Kilches, Nikolaus Steinbeis, Stefanie Schelinski",
      "Year": 2008,
      "DOI/URL": "https://doi.org/10.1371/journal.pone.0002631",
      "Study Description": "The study examined how harmonic expectancy and expressive performance affect brain and physiological responses.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Excerpts with expected (tonic) chords, non-expressive performance",
      "Composer": "Ludwig van Beethoven, Franz Schubert, W.A. Mozart, Joseph Haydn",
      "Passage Length": "8–16 seconds",
      "passageLengthSeconds": 12,
      "Musical Features Analyzed": [
        "Expected cadence",
        "SCR",
        "Emotional valence baseline"
      ],
      "normalizedFeatures": [
        "emotion",
        "valence",
        "harmony"
      ],
      "Task Description": "Passive listening with timbre-deviant detection (button press)",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed nonmusicians, ages 19–29, 10 female",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "32 channels, 10-20 system + SCR + ECG",
      "channelCountValue": 32,
      "Findings": "No ERAN or N5 effect. Used as expected cadence baseline. Lower SCRs overall.",
      "Sampling Rate": "500 Hz",
      "Recording Environment": "Soundproofed room with fixation cross",
      "Data Format": "Digital EEG and physiological traces",
      "Preprocessing": [
        "Artifact rejection",
        "Bandpass filtering (0.25–25 Hz)",
        "Baseline correction",
        "Manual inspection"
      ],
      "License": "Creative Commons Attribution License",
      "EEG Analysis Techniques": [
        "ERP averaging (baseline)",
        "SCR control"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "Chi-square tests"
      ],
      "Event Markers": [
        "Time-locked to final chord onset in each excerpt"
      ],
      "Publication": "PLoS ONE 3(7): e2631",
      "year": 2008
    },
    {
      "id": "study-55",
      "Study Name": "Effects of Unexpected Chords and of Performer’s Expression on Brain Responses and Electrodermal Activity",
      "Authors": "Stefan Koelsch, Simone Kilches, Nikolaus Steinbeis, Stefanie Schelinski",
      "Year": 2008,
      "DOI/URL": "https://doi.org/10.1371/journal.pone.0002631",
      "Study Description": "The study examined how harmonic expectancy and expressive performance affect brain and physiological responses.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Excerpts with very unexpected (Neapolitan) chords, expressive performance",
      "Composer": "Ludwig van Beethoven, Franz Schubert, W.A. Mozart, Joseph Haydn",
      "Passage Length": "8–16 seconds",
      "passageLengthSeconds": 12,
      "Musical Features Analyzed": [
        "Harmonic violation",
        "Neapolitan",
        "N5",
        "SCR",
        "Surprise"
      ],
      "normalizedFeatures": [
        "harmony",
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening with timbre-deviant detection (button press)",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed nonmusicians, ages 19–29, 10 female",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "32 channels, 10-20 system + SCR + ECG",
      "channelCountValue": 32,
      "Findings": "ERAN increased vs. expected; N5 enhanced in expressive. SCRs highest in expressive Neapolitan condition.",
      "Sampling Rate": "500 Hz",
      "Recording Environment": "Soundproofed room with fixation cross",
      "Data Format": "Digital EEG and physiological traces",
      "Preprocessing": [
        "Artifact rejection",
        "Bandpass filtering (0.25–25 Hz)",
        "Baseline correction",
        "Manual inspection"
      ],
      "License": "Creative Commons Attribution License",
      "EEG Analysis Techniques": [
        "ERP averaging (ERAN",
        "N5)",
        "SCR amplitude averaging"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "Chi-square tests"
      ],
      "Event Markers": [
        "Time-locked to final chord onset in each excerpt"
      ],
      "Publication": "PLoS ONE 3(7): e2631",
      "year": 2008
    },
    {
      "id": "study-56",
      "Study Name": "Effects of Unexpected Chords and of Performer’s Expression on Brain Responses and Electrodermal Activity",
      "Authors": "Stefan Koelsch, Simone Kilches, Nikolaus Steinbeis, Stefanie Schelinski",
      "Year": 2008,
      "DOI/URL": "https://doi.org/10.1371/journal.pone.0002631",
      "Study Description": "The study examined how harmonic expectancy and expressive performance affect brain and physiological responses.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Chords/Melody",
      "Stimulus Description": "Excerpts with very unexpected (Neapolitan) chords, non-expressive performance",
      "Composer": "Ludwig van Beethoven, Franz Schubert, W.A. Mozart, Joseph Haydn",
      "Passage Length": "8–16 seconds",
      "passageLengthSeconds": 12,
      "Musical Features Analyzed": [
        "Harmonic violation",
        "Neapolitan",
        "N5",
        "SCR",
        "Surprise"
      ],
      "normalizedFeatures": [
        "harmony",
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening with timbre-deviant detection (button press)",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed nonmusicians, ages 19–29, 10 female",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Not Reported",
      "Channel Count": "32 channels, 10-20 system + SCR + ECG",
      "channelCountValue": 32,
      "Findings": "ERAN increased vs. expected; N5 not modulated. SCRs elevated but lower than expressive condition.",
      "Sampling Rate": "500 Hz",
      "Recording Environment": "Soundproofed room with fixation cross",
      "Data Format": "Digital EEG and physiological traces",
      "Preprocessing": [
        "Artifact rejection",
        "Bandpass filtering (0.25–25 Hz)",
        "Baseline correction",
        "Manual inspection"
      ],
      "License": "Creative Commons Attribution License",
      "EEG Analysis Techniques": [
        "ERP averaging (ERAN",
        "N5)",
        "SCR amplitude averaging"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "Chi-square tests"
      ],
      "Event Markers": [
        "Time-locked to final chord onset in each excerpt"
      ],
      "Publication": "PLoS ONE 3(7): e2631",
      "year": 2008
    },
    {
      "id": "study-57",
      "Study Name": "Single Trial Classification of EEG and Peripheral Physiological Signals for Recognition of Emotions Induced by Music Videos",
      "Authors": "Sander Koelstra, Ashkan Yazdani, Mohammad Soleymani, Christian Mühl, Jong-Seok Lee, Anton Nijholt, Thierry Pun, Touradj Ebrahimi, Ioannis Patras",
      "Year": 2010,
      "DOI/URL": "https://doi.org/10.1007/978-3-642-15314-3_9",
      "Study Description": "This study developed single-trial EEG classification for music video-induced emotion using PSD and CSP features",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Music Videos",
      "Stimulus Description": "20 two-minute music videos spanning 5 valence-arousal quadrants",
      "Composer": "Multiple commercial artists",
      "Passage Length": "2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Like/Dislike",
        "Alpha",
        "Theta",
        "Beta",
        "Gamma",
        "Power Spectrum",
        "CSP features",
        "PSD features"
      ],
      "normalizedFeatures": [
        "theta",
        "alpha",
        "beta",
        "gamma",
        "valence",
        "arousal",
        "power",
        "spectral"
      ],
      "Task Description": "Watched 20 music videos and rated valence, arousal, and like/dislike after each; EEG and peripheral signals recorded",
      "Number of Participants": "6",
      "participantsValue": 6,
      "Demographics": "Healthy adults, mixed gender, average ages not reported",
      "Musical Training": "Not Reported",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "32 channels, 10–20 system",
      "channelCountValue": 32,
      "Findings": "Average classification rates: Valence 58.8%, Arousal 55.7%, Like/Dislike 49.4%; EEG frequency power correlates with subjective valence/arousal ratings",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Laboratory with controlled light and temperature",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Common average referencing",
        "high-pass (0.5 Hz)",
        "bandpass (0.5–35 Hz)",
        "ICA for artifact removal",
        "downsampling to 100 Hz"
      ],
      "License": "Springer-Verlag Lecture Notes in Computer Science",
      "EEG Analysis Techniques": [
        "Power spectrum",
        "Common Spatial Patterns (CSP)",
        "Frequency band correlation"
      ],
      "Statistical Tests": [
        "Spearman correlation",
        "Linear SVM",
        "PCA",
        "Leave-one-trial-out CV",
        "Fisher’s method"
      ],
      "Event Markers": [
        "Synchronization marker at trial onset"
      ],
      "Publication": "Lecture Notes in Computer Science 6334: 89–100",
      "year": 2010
    },
    {
      "id": "study-58",
      "Study Name": "EEG-Based Emotion Recognition in Music Listening",
      "Authors": "Yuan-Pin Lin, Chi-Hong Wang, Tzyy-Ping Jung, Tien-Lin Wu, Shyh-Kang Jeng, Jeng-Ren Duann, Jyh-Horng Chen",
      "Year": 2010,
      "DOI/URL": "https://doi.org/10.1109/TBME.2010.2048568",
      "Study Description": "This study used EEG power and asymmetry metrics to classify music-induced emotions (joy, anger, sadness, pleasure) with high accuracy",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Film soundtrack excerpts categorized by emotion",
      "Passage Length": "30 seconds per excerpt (16 total)",
      "passageLengthSeconds": 30,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Alpha asymmetry",
        "Theta",
        "Beta",
        "Gamma",
        "PSD",
        "DASM12",
        "RASM12",
        "Classification accuracy"
      ],
      "normalizedFeatures": [
        "theta",
        "alpha",
        "beta",
        "gamma",
        "valence",
        "arousal"
      ],
      "Task Description": "Passive listening with eyes closed; subjects labeled emotions on 2D valence-arousal grid after each trial",
      "Number of Participants": "26",
      "participantsValue": 26,
      "Demographics": "Healthy adults (mean age 24.4 ± 2.5), 16 males, 10 females",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Neuroscan system",
      "Channel Count": "32 channels, 10-20 system",
      "channelCountValue": 32,
      "Findings": "DASM12 with SVM achieved 82.29% classification accuracy; frontal and parietal features were most informative; top-30 subject-independent features identified",
      "Sampling Rate": "500 Hz",
      "Recording Environment": "Sound-controlled lab room with closed eyes",
      "Data Format": "EEG traces digitized and FFT applied",
      "Preprocessing": [
        "Visual artifact rejection",
        "Bandpass filter (1–100 Hz)",
        "Notch filter (60 Hz)",
        "STFT",
        "Normalization"
      ],
      "License": "© IEEE 2010",
      "EEG Analysis Techniques": [
        "Power spectrum",
        "Differential and rational asymmetry (DASM12",
        "RASM12)",
        "SVM",
        "MLP classification"
      ],
      "Statistical Tests": [
        "10×10-fold cross-validation",
        "t-tests",
        "ANOVA",
        "F-score feature selection",
        "Confusion matrix"
      ],
      "Event Markers": [
        "Trial-onset of 30s film music clip"
      ],
      "Publication": "IEEE Transactions on Biomedical Engineering 57(7):1798–1806",
      "year": 2010
    },
    {
      "id": "study-59",
      "Study Name": "Tagging the Neuronal Entrainment to Beat and Meter",
      "Authors": "Sylvie Nozaradan, Isabelle Peretz, Marcus Missal, André Mouraux",
      "Year": 2011,
      "DOI/URL": "https://doi.org/10.1523/JNEUROSCI.0411-11.2011",
      "Study Description": "EEG was recorded during beat perception and imagined meter tasks to directly assess neural entrainment.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "333.3 Hz tone with 2.4 Hz beat embedded via amplitude modulation",
      "Passage Length": "33 seconds",
      "passageLengthSeconds": 33,
      "Musical Features Analyzed": [
        "Beat entrainment",
        "Steady-State EP",
        "Frequency tagging"
      ],
      "normalizedFeatures": [
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Detect 4 ms interruption in pseudo-periodic beat sequence (control condition)",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "Adults (3 female, 7 right-handed), mean age 30 ± 4, 3 with 15–25 years musical training, 5 amateur/dance experience",
      "Musical Training": "Mixed Groups, Extensive Training (>10 years)",
      "EEG System Used": "Waveguard64 cap, ANT Neuro amplifier",
      "Channel Count": "64 channels, 10-10 system 4 EOG",
      "channelCountValue": 64,
      "Findings": "Clear EEG response at 2.4 Hz (beat) across all tasks; no difference in beat amplitude across conditions.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Electrically shielded EEG booth",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "0.1 Hz high-pass",
        "ICA blink removal",
        "DFT",
        "noise subtraction"
      ],
      "License": "Society for Neuroscience",
      "EEG Analysis Techniques": [
        "Discrete Fourier Transform",
        "Steady-State EP amplitude extraction",
        "ICA"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "One-sample t-tests"
      ],
      "Event Markers": [
        "Time-locked to beat onset (2.4 Hz)"
      ],
      "Publication": "Journal of Neuroscience 31(28):10234–10240",
      "year": 2011
    },
    {
      "id": "study-60",
      "Study Name": "Tagging the Neuronal Entrainment to Beat and Meter",
      "Authors": "Sylvie Nozaradan, Isabelle Peretz, Marcus Missal, André Mouraux",
      "Year": 2011,
      "DOI/URL": "https://doi.org/10.1523/JNEUROSCI.0411-11.2011",
      "Study Description": "EEG was recorded during beat perception and imagined meter tasks to directly assess neural entrainment.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Same 2.4 Hz beat tone with binary (1.2 Hz) meter imagined",
      "Passage Length": "33 seconds",
      "passageLengthSeconds": 33,
      "Musical Features Analyzed": [
        "Meter entrainment (binary)",
        "Frequency-tagged EEG",
        "Beat processing"
      ],
      "normalizedFeatures": [
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Imagine binary meter while listening to 2.4 Hz beat sequence",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "Adults (3 female, 7 right-handed), mean age 30 ± 4, 3 with 15–25 years musical training, 5 amateur/dance experience",
      "Musical Training": "Mixed Groups, Extensive Training (>10 years)",
      "EEG System Used": "Waveguard64 cap, ANT Neuro amplifier",
      "Channel Count": "64 channels, 10-10 system 4 EOG",
      "channelCountValue": 64,
      "Findings": "1.2 Hz response emerged only during binary meter imagery; not present in control or ternary.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Electrically shielded EEG booth",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "0.1 Hz high-pass",
        "ICA blink removal",
        "DFT",
        "noise subtraction"
      ],
      "License": "Society for Neuroscience",
      "EEG Analysis Techniques": [
        "Discrete Fourier Transform",
        "Steady-State EP amplitude extraction",
        "ICA"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "One-sample t-tests"
      ],
      "Event Markers": [
        "Time-locked to beat and imagined meter (1.2 Hz)"
      ],
      "Publication": "Journal of Neuroscience 31(28):10234–10240",
      "year": 2011
    },
    {
      "id": "study-61",
      "Study Name": "Tagging the Neuronal Entrainment to Beat and Meter",
      "Authors": "Sylvie Nozaradan, Isabelle Peretz, Marcus Missal, André Mouraux",
      "Year": 2011,
      "DOI/URL": "https://doi.org/10.1523/JNEUROSCI.0411-11.2011",
      "Study Description": "EEG was recorded during beat perception and imagined meter tasks to directly assess neural entrainment.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Same 2.4 Hz beat tone with ternary (0.8 Hz) meter imagined",
      "Passage Length": "33 seconds",
      "passageLengthSeconds": 33,
      "Musical Features Analyzed": [
        "Meter entrainment (ternary)",
        "Frequency-tagged EEG",
        "Harmonics"
      ],
      "normalizedFeatures": [
        "synchronization",
        "spectral"
      ],
      "Task Description": "Imagine ternary meter while listening to 2.4 Hz beat sequence",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "Adults (3 female, 7 right-handed), mean age 30 ± 4, 3 with 15–25 years musical training, 5 amateur/dance experience",
      "Musical Training": "Mixed Groups, Extensive Training (>10 years)",
      "EEG System Used": "Waveguard64 cap, ANT Neuro amplifier",
      "Channel Count": "64 channels, 10-10 system 4 EOG",
      "channelCountValue": 64,
      "Findings": "0.8 Hz and 1.6 Hz responses emerged only during ternary imagery; no signal at those frequencies in other conditions.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Electrically shielded EEG booth",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "0.1 Hz high-pass",
        "ICA blink removal",
        "DFT",
        "noise subtraction"
      ],
      "License": "Society for Neuroscience",
      "EEG Analysis Techniques": [
        "Discrete Fourier Transform",
        "Steady-State EP amplitude extraction",
        "ICA"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Paired t-tests",
        "One-sample t-tests"
      ],
      "Event Markers": [
        "Time-locked to beat and imagined meter (0.8 and 1.6 Hz)"
      ],
      "Publication": "Journal of Neuroscience 31(28):10234–10240",
      "year": 2011
    },
    {
      "id": "study-62",
      "Study Name": "DEAP: A Dataset for Emotion Analysis using EEG, Physiological and Video Signals",
      "Authors": "Sander Koelstra, Christian Mühl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, Ioannis Patras",
      "Year": 2012,
      "DOI/URL": "https://doi.org/10.1109/T-AFFC.2011.15",
      "Dataset": "https://www.eecs.qmul.ac.uk/mmv/datasets/deap/download.html",
      "Study Description": "The DEAP dataset collected EEG and peripheral physiological signals to assess emotional responses to music videos.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Music Videos",
      "Stimulus Description": "Music video clips with varying \"emotion\" tags",
      "Composer": "Various",
      "Passage Length": "60 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Dominance",
        "Liking",
        "Familiarity"
      ],
      "normalizedFeatures": [
        "valence",
        "arousal"
      ],
      "Task Description": "Watched 40 one-minute music videos and rated them on five affective dimensions",
      "Number of Participants": "32",
      "participantsValue": 32,
      "Demographics": "Healthy adults (50% female), ages 19–37, mean age 26.9",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "32 channels + 13 physiological channels",
      "channelCountValue": 32,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Controlled lab setting with low illumination",
      "Data Format": "Digital EEG and physiological traces",
      "Preprocessing": [
        "Downsampling to 128Hz",
        "EOG removal",
        "Bandpass filtering (4–45Hz)",
        "Common reference reordering",
        "Segmentation",
        "Baseline removal"
      ],
      "License": "Open Data Commons PDDL",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "Band power",
        "Peripheral signal correlations",
        "Affective quadrant analysis"
      ],
      "Statistical Tests": [
        "ANOVA",
        "Regression (RVM)",
        "SVM classifiers",
        "Correlation analysis"
      ],
      "Event Markers": [
        "Trial start markers",
        "Ratings onset",
        "Fixation screens"
      ],
      "Publication": "IEEE Transactions on Affective Computing 3(1):18–31",
      "year": 2012
    },
    {
      "id": "study-63",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpt selected for high arousal/high valence, fast tempo, orchestral texture",
      "Composer": "Hans Zimmer",
      "Passage Name": "Batman Begins: Molossus (Track 18)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-64",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "High-energy film score with uplifting brass and strings",
      "Composer": "Thomas Newman",
      "Passage Name": "The Rainmaker: End Title (Track 3)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-65",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Energetic electronic piece with bright synth textures and rhythmic drive",
      "Composer": "Leftfield",
      "Passage Name": "Shallow Grave OST: Release the Dub (Track 6)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-66",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Orchestral piece with melodic sweep and layered harmonic motion",
      "Composer": "Rachel Portman",
      "Passage Name": "Oliver Twist OST: Main Theme (Track 1)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-67",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Historical drama score featuring strong choral and instrumental crescendo",
      "Composer": "David Reilly",
      "Passage Name": "Man of Galilee (CD1, Track 2)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-68",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Animated score with whimsical orchestration and dynamic pacing",
      "Composer": "Alan Menken",
      "Passage Name": "Beauty and the Beast OST: West Wing (Track 9)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-69",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Contemporary orchestral-electronic hybrid score with escalating momentum",
      "Composer": "Ramin Djawadi",
      "Passage Name": "Iron Man OST: Mark I (Track 4)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-70",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Orchestral action score with bold rhythmic sections and brassy textures",
      "Composer": "John Powell",
      "Passage Name": "The Italian Job OST: Mash-up (Track 1)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-71",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Upbeat and energetic score with a strong sense of urgency and syncopation",
      "Composer": "Steve Jablonsky",
      "Passage Name": "Transformers OST: Arrival to Earth (Track 7)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-72",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Comic and lively orchestral cue with rapid dynamic shifts and upbeat strings",
      "Composer": "John Williams",
      "Passage Name": "Harry Potter OST: Aunt Marge's Waltz (Track 3)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-73",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Epic orchestral cue with swelling brass, full choir, and rhythmic strings",
      "Composer": "Hans Zimmer",
      "Passage Name": "Gladiator OST: The Might of Rome (Track 5)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-74",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Melodramatic orchestral piece with strong dynamic contour and harmonic shifts",
      "Composer": "Dario Marianelli",
      "Passage Name": "Pride & Prejudice OST: Dawn (Track 1)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-75",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Energetic symphonic track with heroic brass and staccato rhythm",
      "Composer": "John Debney",
      "Passage Name": "Cutthroat Island OST: Main Title (Track 1)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-76",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Motivational theme with swelling dynamics, full orchestra and optimistic melody",
      "Composer": "Craig Armstrong",
      "Passage Name": "In Time OST: Escape (Track 7)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-77",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Orchestral cue with high emotional intensity, use of dissonance and large swells",
      "Composer": "James Horner",
      "Passage Name": "A Beautiful Mind OST: A Kaleidoscope of Mathematics (Track 2)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-78",
      "Study Name": "Neural correlates of emotional responses to music: An EEG study",
      "Authors": "Ian Daly, Asad Malik, Faustina Hwang, Etienne Roesch, James Weaver, Alexis Kirke, Duncan Williams, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2014,
      "DOI/URL": "https://doi.org/10.1016/j.neulet.2014.05.003",
      "Study Description": "This EEG study examined how emotional responses to musical excerpts relate to brain activity by measuring gamma coherence and frontal asymmetry during passive listening followed by emotion self-ratings.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Animated orchestral comedy track with strong syncopation and rapid phrasing",
      "Composer": "Mark Mothersbaugh",
      "Passage Name": "Cloudy With a Chance of Meatballs OST: Monkey Town (Track 11)",
      "Passage Length": "15 seconds",
      "passageLengthSeconds": 15,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Gamma band coherence",
        "Emotion tagging"
      ],
      "normalizedFeatures": [
        "gamma",
        "emotion",
        "valence",
        "arousal",
        "coherence"
      ],
      "Task Description": "Passive listening followed by SAM ratings on valence, arousal, tension, and basic emotions",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Adults, median age 35 (range 18–66), 18 female, 29 right-handed",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Brain Products BrainAmp",
      "Channel Count": "19 channels, 10/20 system (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2)",
      "channelCountValue": 19,
      "Findings": "Higher gamma coherence was associated with higher reported arousal and valence. Left-frontal asymmetry correlated with positive valence ratings. Gamma coherence and asymmetry measures significantly predicted self-reported emotional experience, indicating neural markers of affective response to music.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab, participants viewed a fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Notch filter (50Hz)",
        "bandpass 0.1–45 Hz",
        "ICA",
        "average reference",
        "downsampled to 100 Hz"
      ],
      "License": "© 2014 Elsevier Ireland Ltd.",
      "EEG Analysis Techniques": [
        "Coherence",
        "Gamma band asymmetry",
        "Laplacian differences"
      ],
      "Statistical Tests": [
        "Stepwise regression",
        "Bonferroni-corrected t-tests"
      ],
      "Event Markers": [
        "Time-locked to excerpt onset"
      ],
      "Publication": "Neuroscience Letters 573:52–57",
      "year": 2014
    },
    {
      "id": "study-79",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Chim Chim Cheree with lyrics (3/4, 212 BPM)",
      "Composer": "Robert B. Sherman, Richard M. Sherman",
      "Passage Name": "Chim Chim Cheree (lyrics)",
      "Passage Length": "13.3 seconds",
      "passageLengthSeconds": 13.3,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Lyrics",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury, 8 with 1–10 years formal training, 4 played instruments regularly",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-80",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Take Me Out to the Ballgame with lyrics (3/4, 189 BPM)",
      "Composer": "Jack Norworth, Albert Von Tilzer",
      "Passage Name": "Take Me Out to the Ballgame (lyrics)",
      "Passage Length": "7.7 seconds",
      "passageLengthSeconds": 7.7,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Lyrics",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-81",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Jingle Bells with lyrics (4/4, 200 BPM)",
      "Composer": "James Lord Pierpont",
      "Passage Name": "Jingle Bells (lyrics)",
      "Passage Length": "9.7 seconds",
      "passageLengthSeconds": 9.7,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Lyrics",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-82",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Mary Had a Little Lamb with lyrics (4/4, 160 BPM)",
      "Composer": "Traditional",
      "Passage Name": "Mary Had a Little Lamb (lyrics)",
      "Passage Length": "11.6 seconds",
      "passageLengthSeconds": 11.6,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Lyrics",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-83",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Chim Chim Cheree instrumental (3/4, 212 BPM)",
      "Composer": "Robert B. Sherman, Richard M. Sherman",
      "Passage Name": "Chim Chim Cheree",
      "Passage Length": "13.5 seconds",
      "passageLengthSeconds": 13.5,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Instrumental",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-84",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Take Me Out to the Ballgame instrumental (3/4, 189 BPM)",
      "Composer": "Jack Norworth, Albert Von Tilzer",
      "Passage Name": "Take Me Out to the Ballgame",
      "Passage Length": "7.7 seconds",
      "passageLengthSeconds": 7.7,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Instrumental",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-85",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Jingle Bells instrumental (4/4, 200 BPM)",
      "Composer": "James Lord Pierpont",
      "Passage Name": "Jingle Bells",
      "Passage Length": "9.0 seconds",
      "passageLengthSeconds": 9,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Instrumental",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-86",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Mary Had a Little Lamb instrumental (4/4, 160 BPM)",
      "Composer": "Traditional",
      "Passage Name": "Mary Had a Little Lamb",
      "Passage Length": "12.2 seconds",
      "passageLengthSeconds": 12.2,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Instrumental",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-87",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Emperor Waltz (3/4, 178 BPM)",
      "Composer": "Johann Strauss II",
      "Passage Name": "Emperor Waltz",
      "Passage Length": "8.3 seconds",
      "passageLengthSeconds": 8.3,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Orchestral",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-88",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Hedwig’s Theme (Harry Potter, 3/4, 166 BPM)",
      "Composer": "John Williams",
      "Passage Name": "Hedwig's Theme",
      "Passage Length": "16.0 seconds",
      "passageLengthSeconds": 16,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Film score",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-89",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Imperial March (Star Wars Theme, 4/4, 104 BPM)",
      "Composer": "John Williams",
      "Passage Name": "Imperial March",
      "Passage Length": "9.2 seconds",
      "passageLengthSeconds": 9.2,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Film score",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-90",
      "Study Name": "OPENMIIR: A Public Dataset of EEG Recordings During Music Perception and Imagination",
      "Authors": "Sebastian Stober, Avital Sternin, Adrian M. Owen, Jessica A. Grahn",
      "Year": 2015,
      "DOI/URL": "https://doi.org/10.6084/m9.figshare.1541151.v1",
      "Dataset": "https://github.com/sstober/openmiir",
      "Study Description": "The OPENMIIR dataset contains EEG responses to perception and imagination of well-known musical fragments.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Eine Kleine Nachtmusik (4/4, 140 BPM)",
      "Composer": "Wolfgang Amadeus Mozart",
      "Passage Name": "Eine Kleine Nachtmusik",
      "Passage Length": "6.9 seconds",
      "passageLengthSeconds": 6.9,
      "Musical Features Analyzed": [
        "Tempo",
        "Meter",
        "Classical string ensemble",
        "Imagery"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Participants either passively listened to music preceded by cue clicks, imagined music with or without cue clicks, or imagined music without cues and rated confidence",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Adults (3 male), ages 19–36, normal hearing, no brain injury",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 4 EOG",
      "channelCountValue": 64,
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Soundproof audiometric booth",
      "Data Format": "Digital EEG traces in FIF format",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "Re-referencing",
        "Bandpass 0.5–30 Hz",
        "Downsampling to 64 Hz"
      ],
      "License": "Open Data Commons Public Domain Dedication and License (PDDL)",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "PCA",
        "Beat-locked ERP"
      ],
      "Statistical Tests": [
        "None reported"
      ],
      "Event Markers": [
        "Time-locked to cue click onset and stimulus onset or imagination block onset"
      ],
      "Publication": "ISMIR 2015 Proceedings",
      "year": 2015
    },
    {
      "id": "study-91",
      "Study Name": "Event-Related Brain Responses While Listening to Entire Pieces of Music",
      "Authors": "H. Poikonen, V. Alluri, E. Brattico, O. Lartillot, M. Tervaniemi, M. Huotilainen",
      "Year": 2016,
      "DOI/URL": "https://doi.org/10.1016/j.neuroscience.2015.10.061",
      "Study Description": "This study used an ERP-based method for analyzing brain responses to continuous music",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Live-recorded modern tango with dynamic shifts in timbre and loudness",
      "Composer": "Astor Piazzolla",
      "Passage Name": "Adios Nonino",
      "Passage Length": "8 minutes (last 30s removed)",
      "passageLengthSeconds": 480,
      "Musical Features Analyzed": [
        "Brightness",
        "Spectral flux",
        "Zero-crossing rate",
        "RMS (loudness)",
        "N100",
        "P200"
      ],
      "normalizedFeatures": [
        "erp",
        "envelope",
        "spectral"
      ],
      "Task Description": "Passive listening with eyes open, EEG recorded during entire piece",
      "Number of Participants": "16",
      "participantsValue": 16,
      "Demographics": "Right-handed Finnish adults, 10 female, age 20–46, Musical laymen, some with informal training in instruments or dance",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 5 external electrodes",
      "channelCountValue": 64,
      "Findings": "The study revealed significant ERP components (N100, P200) time-locked to rapid acoustic changes, demonstrating cortical responses to structural features in natural music.",
      "Sampling Rate": "2048 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Continuous EEG traces (BioSemi BDF)",
      "Preprocessing": [
        "ICA artifact removal",
        "Downsampling to 256 Hz",
        "Bandpass filtering (1–30 Hz)",
        "Baseline correction",
        "Epoch rejection (±100 µV)",
        "Visual inspection"
      ],
      "License": "Crown Copyright © 2015 Elsevier Ltd.",
      "EEG Analysis Techniques": [
        "ERP averaging (N100",
        "P200)",
        "Feature-triggered ERP extraction"
      ],
      "Statistical Tests": [
        "Two-way ANOVA",
        "One-sample t-tests",
        "Shapiro–Wilk tests",
        "Pairwise comparisons"
      ],
      "Event Markers": [
        "Feature-triggered",
        "time-locked to rapid acoustic changes"
      ],
      "Publication": "Neuroscience 312:58–73",
      "year": 2016
    },
    {
      "id": "study-92",
      "Study Name": "Event-Related Brain Responses While Listening to Entire Pieces of Music",
      "Authors": "H. Poikonen, V. Alluri, E. Brattico, O. Lartillot, M. Tervaniemi, M. Huotilainen",
      "Year": 2016,
      "DOI/URL": "https://doi.org/10.1016/j.neuroscience.2015.10.061",
      "Study Description": "This study used an ERP-based method for analyzing brain responses to continuous music",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Looped acoustic lullaby with unclear vocals resembling humming",
      "Composer": "Kira Kira",
      "Passage Name": "Bless",
      "Passage Length": "5 minutes 43 seconds",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Brightness",
        "Spectral flux",
        "Zero-crossing rate",
        "RMS (loudness)",
        "N100",
        "P200"
      ],
      "normalizedFeatures": [
        "erp",
        "envelope",
        "spectral"
      ],
      "Task Description": "Passive listening with eyes open, EEG recorded during entire piece",
      "Number of Participants": "16",
      "participantsValue": 16,
      "Demographics": "Right-handed Finnish adults, 10 female, age 20–46, Musical laymen, some with informal training in instruments or dance",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 5 external electrodes",
      "channelCountValue": 64,
      "Findings": "The study found consistent ERP responses (N100, P200) to changes in brightness and spectral flux, indicating sensitivity to timbral variation in a minimal acoustic context.",
      "Sampling Rate": "2048 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Continuous EEG traces (BioSemi BDF)",
      "Preprocessing": [
        "ICA artifact removal",
        "Downsampling to 256 Hz",
        "Bandpass filtering (1–30 Hz)",
        "Baseline correction",
        "Epoch rejection (±100 µV)",
        "Visual inspection"
      ],
      "License": "Crown Copyright © 2015 Elsevier Ltd.",
      "EEG Analysis Techniques": [
        "ERP averaging (N100",
        "P200)",
        "Feature-triggered ERP extraction"
      ],
      "Statistical Tests": [
        "Two-way ANOVA",
        "One-sample t-tests",
        "Shapiro–Wilk tests",
        "Pairwise comparisons"
      ],
      "Event Markers": [
        "Feature-triggered",
        "time-locked to rapid acoustic changes"
      ],
      "Publication": "Neuroscience 312:58–73",
      "year": 2016
    },
    {
      "id": "study-93",
      "Study Name": "Event-Related Brain Responses While Listening to Entire Pieces of Music",
      "Authors": "H. Poikonen, V. Alluri, E. Brattico, O. Lartillot, M. Tervaniemi, M. Huotilainen",
      "Year": 2016,
      "DOI/URL": "https://doi.org/10.1016/j.neuroscience.2015.10.061",
      "Study Description": "This study used an ERP-based method for analyzing brain responses to continuous music",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Deep techno track with regular beat and low harmonic content",
      "Composer": "Len Faki (remixed by Radio Slave)",
      "Passage Name": "My Black Sheep Radio Slave",
      "Passage Length": "5 minutes 20 seconds",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Brightness",
        "Spectral flux",
        "Zero-crossing rate",
        "RMS (loudness)",
        "N100",
        "P200"
      ],
      "normalizedFeatures": [
        "erp",
        "envelope",
        "spectral"
      ],
      "Task Description": "Passive listening with eyes open, EEG recorded during entire piece",
      "Number of Participants": "16",
      "participantsValue": 16,
      "Demographics": "Right-handed Finnish adults, 10 female, age 20–46, Musical laymen, some with informal training in instruments or dance",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64 channels, 10/10 system + 5 external electrodes",
      "channelCountValue": 64,
      "Findings": "The study reported ERP components (N100, P200) in response to rapid acoustic shifts, confirming neural entrainment even in rhythmically repetitive and harmonically sparse stimuli.",
      "Sampling Rate": "2048 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Continuous EEG traces (BioSemi BDF)",
      "Preprocessing": [
        "ICA artifact removal",
        "Downsampling to 256 Hz",
        "Bandpass filtering (1–30 Hz)",
        "Baseline correction",
        "Epoch rejection (±100 µV)",
        "Visual inspection"
      ],
      "License": "Crown Copyright © 2015 Elsevier Ltd.",
      "EEG Analysis Techniques": [
        "ERP averaging (N100",
        "P200)",
        "Feature-triggered ERP extraction"
      ],
      "Statistical Tests": [
        "Two-way ANOVA",
        "One-sample t-tests",
        "Shapiro–Wilk tests",
        "Pairwise comparisons"
      ],
      "Event Markers": [
        "Feature-triggered",
        "time-locked to rapid acoustic changes"
      ],
      "Publication": "Neuroscience 312:58–73",
      "year": 2016
    },
    {
      "id": "study-94",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Bonobo – “First Fires” (ASIN B00CJE73J6, 55.97 BPM)",
      "Composer": "Bonobo",
      "Passage Name": "First Fires",
      "Passage Length": "4:38 minutes",
      "passageLengthSeconds": 278,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing, 17 reported musical training (mean 8.4 years)",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-95",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "LA Priest – “Oino” (ASIN B00T4NHS2W, 69.44 BPM)",
      "Composer": "LA Priest",
      "Passage Name": "Oino",
      "Passage Length": "4:31 minutes",
      "passageLengthSeconds": 271,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-96",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Daedelus – “Tiptoes” (ASIN B011SAZRLC, 74.26 BPM)",
      "Composer": "Daedelus",
      "Passage Name": "Tiptoes",
      "Passage Length": "4:36 minutes",
      "passageLengthSeconds": 276,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-97",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Croquet Club – “Careless Love” (ASIN B06X9736NJ, 82.42 BPM)",
      "Composer": "Croquet Club",
      "Passage Name": "Careless Love",
      "Passage Length": "4:54 minutes",
      "passageLengthSeconds": 294,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-98",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Thievery Corporation – “Lebanese Blonde” (ASIN B000SF16MI, 91.46 BPM)",
      "Composer": "Thievery Corporation",
      "Passage Name": "Lebanese Blonde",
      "Passage Length": "4:49 minutes",
      "passageLengthSeconds": 289,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-99",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Polo & Pan – “Canopee” (ASIN B01GOL4IB0, 96.15 BPM)",
      "Composer": "Polo & Pan",
      "Passage Name": "Canopee",
      "Passage Length": "4:36 minutes",
      "passageLengthSeconds": 276,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-100",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Kazy Lambist – “Doing Yoga” (ASIN B01JDDVIQ4, 108.70 BPM)",
      "Composer": "Kazy Lambist",
      "Passage Name": "Doing Yoga",
      "Passage Length": "4:52 minutes",
      "passageLengthSeconds": 292,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-101",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Rüfüs du Sol – “Until the Sun Needs to Rise” (ASIN B01APT6JKA, 120 BPM)",
      "Composer": "Rüfüs du Sol",
      "Passage Name": "Until the Sun Needs to Rise",
      "Passage Length": "4:52 minutes",
      "passageLengthSeconds": 292,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-102",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "The Knife – “Silent Shout” (ASIN B00IMN40O4, 128.21 BPM)",
      "Composer": "The Knife",
      "Passage Name": "Silent Shout",
      "Passage Length": "4:54 minutes",
      "passageLengthSeconds": 294,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-103",
      "Study Name": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "Authors": "Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, Blair Kaneshiro",
      "Year": 2017,
      "DOI/URL": "https://archives.ismir.net/ismir2017/paper/000198.pdf",
      "Dataset": "https://purl.stanford.edu/jn859kj8079",
      "Study Description": "The NMED-T dataset provides open-access EEG, tapping, and behavioral response data from 20 participants listening to 10 tempo-varied, beat-driven songs, enabling reproducible research in neuroscience and cognitive MIR.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "David Bowie – “The Last Thing You Should Do” (ASIN B018GS2A46, 150 BPM)",
      "Composer": "David Bowie",
      "Passage Name": "The Last Thing You Should Do",
      "Passage Length": "4:58 minutes",
      "passageLengthSeconds": 298,
      "Musical Features Analyzed": [
        "Tempo",
        "Beat entrainment",
        "Spectral energy"
      ],
      "normalizedFeatures": [
        "arousal",
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "Participants listened to 10 full-length songs during EEG, then tapped along to 35-second excerpts in a separate behavioral session",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "Right-handed adults, ages 18–29, 6 female, normal hearing",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system, vertex reference",
      "channelCountValue": 128,
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated lab and quiet room",
      "Data Format": "Digital EEG + tap timing + behavioral ratings",
      "Preprocessing": [
        "Downsampling to 125 Hz",
        "High-pass + notch + low-pass filtering",
        "ICA",
        "interpolation",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "SS-EPs",
        "PCA",
        "Time-frequency analysis"
      ],
      "Statistical Tests": [
        "Correlation analysis",
        "Spectral power comparison"
      ],
      "Event Markers": [
        "Audio-click sync",
        "Tap timestamps from iPad"
      ],
      "Publication": "ISMIR 2017: 339–346",
      "year": 2017
    },
    {
      "id": "study-104",
      "Study Name": "Neural Tracking of the Musical Beat is Enhanced by Low-Frequency Sounds",
      "Authors": "Lenc, Keller, Varlet, Nozaradan",
      "Year": 2018,
      "DOI/URL": "https://doi.org/10.1073/pnas.1801421115",
      "Study Description": "This study used EEG and frequency tagging to examine how low vs. high tones influence neural tracking of rhythmic beats and meter.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthetic Rhythms",
      "Stimulus Description": "Pure tones (130 Hz or 1236.8 Hz) forming isochronous, unsyncopated, or syncopated rhythms, looped for 60 seconds",
      "Composer": "N/A",
      "Passage Name": "Isochronous Rhythm",
      "Passage Length": "60 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Neural entrainment",
        "frequency tagging",
        "beat and meter tracking"
      ],
      "normalizedFeatures": [
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "EEG-monitored passive listening with deviant duration detection; tapping task after EEG",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Mean age 28.4, 10 female",
      "Musical Training": "Moderate Training (5-10 years)",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64",
      "channelCountValue": 64,
      "Findings": "Low tones enhanced EEG activity at beat frequency; strongest effects seen in syncopated rhythm.",
      "Sampling Rate": "2048 Hz",
      "Recording Environment": "Sound-attenuated lab",
      "Data Format": "Digitized EEG",
      "Preprocessing": [
        "Filtered",
        "downsampled",
        "artifact rejection",
        "common average referencing"
      ],
      "License": "CC BY-NC-ND",
      "EEG Analysis Techniques": [
        "Frequency tagging",
        "z-score standardization",
        "cochlear model comparison"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "t-tests",
        "permutation tests"
      ],
      "Event Markers": [
        "Rhythm onset",
        "tone deviants"
      ],
      "Publication": "PNAS 115(32):8221–8226",
      "year": 2018
    },
    {
      "id": "study-105",
      "Study Name": "Neural Tracking of the Musical Beat is Enhanced by Low-Frequency Sounds",
      "Authors": "Lenc, Keller, Varlet, Nozaradan",
      "Year": 2018,
      "DOI/URL": "https://doi.org/10.1073/pnas.1801421115",
      "Study Description": "This study used EEG and frequency tagging to examine how low vs. high tones influence neural tracking of rhythmic beats and meter.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthetic Rhythms",
      "Stimulus Description": "Pure tones (130 Hz or 1236.8 Hz) forming isochronous, unsyncopated, or syncopated rhythms, looped for 60 seconds",
      "Composer": "N/A",
      "Passage Name": "Unsyncopated Rhythm",
      "Passage Length": "60 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Neural entrainment",
        "frequency tagging",
        "beat and meter tracking"
      ],
      "normalizedFeatures": [
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "EEG-monitored passive listening with deviant duration detection; tapping task after EEG",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Mean age 28.4, 10 female",
      "Musical Training": "Moderate Training (5-10 years)",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64",
      "channelCountValue": 64,
      "Findings": "Beat frequency enhancement with low tones; no meter effect.",
      "Sampling Rate": "2048 Hz",
      "Recording Environment": "Sound-attenuated lab",
      "Data Format": "Digitized EEG",
      "Preprocessing": [
        "Filtered",
        "downsampled",
        "artifact rejection",
        "common average referencing"
      ],
      "License": "CC BY-NC-ND",
      "EEG Analysis Techniques": [
        "Frequency tagging",
        "z-score standardization",
        "cochlear model comparison"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "t-tests",
        "permutation tests"
      ],
      "Event Markers": [
        "Rhythm onset",
        "tone deviants"
      ],
      "Publication": "PNAS 115(32):8221–8226",
      "year": 2018
    },
    {
      "id": "study-106",
      "Study Name": "Neural Tracking of the Musical Beat is Enhanced by Low-Frequency Sounds",
      "Authors": "Lenc, Keller, Varlet, Nozaradan",
      "Year": 2018,
      "DOI/URL": "https://doi.org/10.1073/pnas.1801421115",
      "Study Description": "This study used EEG and frequency tagging to examine how low vs. high tones influence neural tracking of rhythmic beats and meter.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthetic Rhythms",
      "Stimulus Description": "Pure tones (130 Hz or 1236.8 Hz) forming isochronous, unsyncopated, or syncopated rhythms, looped for 60 seconds",
      "Composer": "N/A",
      "Passage Name": "Syncopated Rhythm",
      "Passage Length": "60 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Neural entrainment",
        "frequency tagging",
        "beat and meter tracking"
      ],
      "normalizedFeatures": [
        "tempo",
        "synchronization",
        "spectral"
      ],
      "Task Description": "EEG-monitored passive listening with deviant duration detection; tapping task after EEG",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Mean age 28.4, 10 female",
      "Musical Training": "Moderate Training (5-10 years)",
      "EEG System Used": "Biosemi ActiveTwo",
      "Channel Count": "64",
      "channelCountValue": 64,
      "Findings": "Low tones boosted EEG responses at both beat and meter frequencies for syncopated rhythms.",
      "Sampling Rate": "2048 Hz",
      "Recording Environment": "Sound-attenuated lab",
      "Data Format": "Digitized EEG",
      "Preprocessing": [
        "Filtered",
        "downsampled",
        "artifact rejection",
        "common average referencing"
      ],
      "License": "CC BY-NC-ND",
      "EEG Analysis Techniques": [
        "Frequency tagging",
        "z-score standardization",
        "cochlear model comparison"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "t-tests",
        "permutation tests"
      ],
      "Event Markers": [
        "Rhythm onset",
        "tone deviants"
      ],
      "Publication": "PNAS 115(32):8221–8226",
      "year": 2018
    },
    {
      "id": "study-107",
      "Study Name": "NMED-RP Naturalistic Music EEG Dataset – Rhythm Pilot",
      "Authors": "Appaji, Jay, Kaneshiro, Blair",
      "Year": 2018,
      "Dataset": "https://purl.stanford.edu/rz763kn3821",
      "Study Description": "Pilot study examined EEG and behavioral responses to brief rhythmic musical excerpts.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpts of natural music and rhythmic patterns; multiple listens",
      "Composer": "Various",
      "Passage Length": "30 seconds",
      "passageLengthSeconds": 30,
      "Musical Features Analyzed": [
        "Rhythmic complexity",
        "Beat perception",
        "Enjoyment",
        "Temporal attention"
      ],
      "normalizedFeatures": [
        "attention",
        "tempo",
        "localization"
      ],
      "Task Description": "Participants listened to 16 different musical excerpts multiple times and rated enjoyment, rhythmic complexity, and ease of finding the beat after each trial",
      "Number of Participants": "5",
      "participantsValue": 5,
      "Demographics": "Adults with normal hearing; individual sessions, mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system + reference",
      "channelCountValue": 128,
      "Findings": "ERP averaging performed; dataset supports future exploration of rhythmic complexity and enjoyment.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Controlled EEG lab setting",
      "Data Format": "Digital EEG traces and behavioral ratings in .mat format",
      "Preprocessing": [
        "Not explicitly detailed; assumed filtering",
        "epoching",
        "downsampling based on NMED family standards"
      ],
      "License": "CC BY 3.0",
      "EEG Analysis Techniques": [
        "RCA",
        "ERP averaging",
        "Descriptive statistics"
      ],
      "Statistical Tests": [
        "Not reported (pilot dataset)"
      ],
      "Event Markers": [
        "Trial-aligned EEG blocks with per-trial behavioral ratings"
      ],
      "Publication": "Stanford Digital Repository (2018): https://purl.stanford.edu/rz763kn3821",
      "year": 2018
    },
    {
      "id": "study-108",
      "Study Name": "Rhesus Monkeys (Macaca mulatta) Sense Isochrony in Rhythm, but Not the Beat: Additional Support for the Gradual Audiomotor Evolution Hypothesis",
      "Authors": "Henkjan Honing, Fleur Bouwer, Gámez José, Hugo Merchant",
      "Year": 2018,
      "DOI/URL": "https://doi.org/10.3389/fnins.2018.00475",
      "Study Description": "This study used EEG to examine whether rhesus monkeys can detect isochrony and perceive beats in rhythmic sound sequences, testing their auditory timing abilities in support of the Gradual Audiomotor Evolution Hypothesis.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Accented and unaccented percussive sounds in isochronous pattern",
      "Passage Length": "~9:45 minutes per block",
      "passageLengthSeconds": 585,
      "Musical Features Analyzed": [
        "MMN",
        "P3a",
        "P1",
        "N1",
        "Isochrony sensitivity",
        "Beat perception"
      ],
      "normalizedFeatures": [
        "tempo",
        "erp"
      ],
      "Task Description": "Passive listening to rhythmic sequences with deviant detection",
      "Number of Participants": "2",
      "participantsValue": 2,
      "Demographics": "Rhesus monkeys (Macaca mulatta), adult males",
      "Musical Training": "Not Applicable",
      "EEG System Used": "Grass EEG electrodes (Fz, Cz, Pz, F3, F4) with Tucker-Davis Technologies hardware",
      "Channel Count": "5 channels, 10–20 system",
      "channelCountValue": 5,
      "Findings": "Monkeys exhibited larger mismatch negativity (MMN) in the isochronous condition, indicating sensitivity to isochrony. However, no interaction between metrical position and isochrony was observed—supporting the conclusion that while monkeys detect temporal regularity, they do not perceive beat structure.",
      "Sampling Rate": "498.25 Hz",
      "Recording Environment": "Acoustically treated cage with loudspeakers 1.1m away",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "5–75 Hz bandpass filter",
        "no baseline correction"
      ],
      "License": "© 2018 Honing et al., Frontiers Media S.A.",
      "EEG Analysis Techniques": [
        "ERP averaging (MMN",
        "P3a",
        "P1",
        "N1)"
      ],
      "Statistical Tests": [
        "ANOVA (with factors Isochrony",
        "Position",
        "and Type)"
      ],
      "Event Markers": [
        "Time-locked to deviant onset in rhythmic streams"
      ],
      "Publication": "Frontiers in Neuroscience 12:475",
      "year": 2018
    },
    {
      "id": "study-109",
      "Study Name": "Rhesus Monkeys (Macaca mulatta) Sense Isochrony in Rhythm, but Not the Beat: Additional Support for the Gradual Audiomotor Evolution Hypothesis",
      "Authors": "Henkjan Honing, Fleur Bouwer, Gámez José, Hugo Merchant",
      "Year": 2018,
      "DOI/URL": "https://doi.org/10.3389/fnins.2018.00475",
      "Study Description": "This study used EEG to examine whether rhesus monkeys can detect isochrony and perceive beats in rhythmic sound sequences, testing their auditory timing abilities in support of the Gradual Audiomotor Evolution Hypothesis.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Synthesized Music/Tone",
      "Stimulus Description": "Accented and unaccented percussive sounds in jittered pattern",
      "Passage Length": "~9:45 minutes per block",
      "passageLengthSeconds": 585,
      "Musical Features Analyzed": [
        "MMN",
        "P3a",
        "P1",
        "N1",
        "Isochrony sensitivity",
        "Beat perception"
      ],
      "normalizedFeatures": [
        "tempo",
        "erp"
      ],
      "Task Description": "Passive listening to rhythmic sequences with deviant detection",
      "Number of Participants": "2",
      "participantsValue": 2,
      "Demographics": "Rhesus monkeys (Macaca mulatta), adult males",
      "Musical Training": "Not Applicable",
      "EEG System Used": "Grass EEG electrodes (Fz, Cz, Pz, F3, F4) with Tucker-Davis Technologies hardware",
      "Channel Count": "5 channels, 10–20 system",
      "channelCountValue": 5,
      "Findings": "Monkeys still showed MMN responses to deviant stimuli but of lower amplitude compared to the isochronous condition. The absence of interaction with metrical structure confirms lack of beat perception, reinforcing the Gradual Audiomotor Evolution hypothesis that beat-based timing evolved gradually and is largely absent in monkeys.",
      "Sampling Rate": "498.25 Hz",
      "Recording Environment": "Acoustically treated cage with loudspeakers 1.1m away",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "5–75 Hz bandpass filter",
        "no baseline correction"
      ],
      "License": "© 2018 Honing et al., Frontiers Media S.A.",
      "EEG Analysis Techniques": [
        "ERP averaging (MMN",
        "P3a",
        "P1",
        "N1)"
      ],
      "Statistical Tests": [
        "ANOVA (with factors Isochrony",
        "Position",
        "and Type)"
      ],
      "Event Markers": [
        "Time-locked to deviant onset in rhythmic streams"
      ],
      "Publication": "Frontiers in Neuroscience 12:475",
      "year": 2018
    },
    {
      "id": "study-110",
      "Study Name": "NMED-H 2.0: Naturalistic Music EEG Dataset—Hindi",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Dmochowski, Jacek P., Norcia, Anthony M., and Berger, Jonathan",
      "Year": 2019,
      "DOI/URL": "https://purl.stanford.edu/sd922db3535",
      "Dataset": "https://purl.stanford.edu/sd922db3535",
      "Study Description": "This study examined how intact and manipulated temporal structure of Hindi pop music affects inter-subject EEG correlation.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Four Various Hindi pop songs – intact temporal structure",
      "Composer": "Various",
      "Passage Length": "~4.5 minutes",
      "passageLengthSeconds": 270,
      "Musical Features Analyzed": [
        "Temporal structure",
        "Beat tracking",
        "Inter-subject correlation"
      ],
      "normalizedFeatures": [
        "tempo",
        "coherence",
        "synchronization",
        "localization"
      ],
      "Task Description": "Each participant heard one of four songs under four temporal conditions (intact, shuffled, reversed, scrambled); listened twice per stimulus and rated Pleasantness, Musicality, Order, and Interest",
      "Number of Participants": "12",
      "participantsValue": 12,
      "Demographics": "Adults with normal hearing; 50% female, right-handed",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system + reference",
      "channelCountValue": 128,
      "Findings": "ISC and variance explained were highest for intact condition; behavioral ratings aligned with EEG responses.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (clean)",
      "Recording Environment": "Sound-treated EEG lab",
      "Data Format": "Digital EEG (raw, clean, RCA), behavioral ratings",
      "Preprocessing": [
        "Filtering",
        "downsampling",
        "artifact rejection",
        "re-referencing",
        "spatial filtering (RCA)"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "RCA",
        "Inter-subject correlation",
        "Time-domain analysis"
      ],
      "Statistical Tests": [
        "ISC",
        "Variance explained",
        "Behavioral-EEG correlation"
      ],
      "Event Markers": [
        "Trial-aligned per condition and stimulus"
      ],
      "Publication": "ISMIR 2019 Late-Breaking/Demo",
      "year": 2019
    },
    {
      "id": "study-111",
      "Study Name": "NMED-H 2.0: Naturalistic Music EEG Dataset—Hindi",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Dmochowski, Jacek P., Norcia, Anthony M., and Berger, Jonathan",
      "Year": 2019,
      "DOI/URL": "https://purl.stanford.edu/sd922db3535",
      "Dataset": "https://purl.stanford.edu/sd922db3535",
      "Study Description": "This study examined how intact and manipulated temporal structure of Hindi pop music affects inter-subject EEG correlation.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Four Various Hindi pop songs  – measure-shuffled",
      "Composer": "Various",
      "Passage Length": "~4.5 minutes",
      "passageLengthSeconds": 270,
      "Musical Features Analyzed": [
        "Temporal disruption",
        "Beat segmentation",
        "Neural response variability"
      ],
      "normalizedFeatures": [
        "tempo",
        "localization"
      ],
      "Task Description": "Each participant heard one of four songs under four temporal conditions (intact, shuffled, reversed, scrambled); listened twice per stimulus and rated Pleasantness, Musicality, Order, and Interest",
      "Number of Participants": "12",
      "participantsValue": 12,
      "Demographics": "Adults with normal hearing; 50% female, right-handed",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system + reference",
      "channelCountValue": 128,
      "Findings": "ISC and behavioral ratings decreased in measure-shuffled condition, reflecting disruption in temporal structure.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (clean)",
      "Recording Environment": "Sound-treated EEG lab",
      "Data Format": "Digital EEG (raw, clean, RCA), behavioral ratings",
      "Preprocessing": [
        "Filtering",
        "downsampling",
        "artifact rejection",
        "re-referencing",
        "spatial filtering (RCA)"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "RCA",
        "Inter-subject correlation",
        "Time-domain analysis"
      ],
      "Statistical Tests": [
        "ISC",
        "Variance explained",
        "Behavioral-EEG correlation"
      ],
      "Event Markers": [
        "Trial-aligned per condition and stimulus"
      ],
      "Publication": "ISMIR 2019 Late-Breaking/Demo",
      "year": 2019
    },
    {
      "id": "study-112",
      "Study Name": "NMED-H 2.0: Naturalistic Music EEG Dataset—Hindi",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Dmochowski, Jacek P., Norcia, Anthony M., and Berger, Jonathan",
      "Year": 2019,
      "DOI/URL": "https://purl.stanford.edu/sd922db3535",
      "Dataset": "https://purl.stanford.edu/sd922db3535",
      "Study Description": "This study examined how intact and manipulated temporal structure of Hindi pop music affects inter-subject EEG correlation.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Four Various Hindi pop songs – time-reversed",
      "Composer": "Various",
      "Passage Length": "~4.5 minutes",
      "passageLengthSeconds": 270,
      "Musical Features Analyzed": [
        "Temporal reversal",
        "Backward structure",
        "Neural dissimilarity"
      ],
      "normalizedFeatures": [
        "localization"
      ],
      "Task Description": "Each participant heard one of four songs under four temporal conditions (intact, shuffled, reversed, scrambled); listened twice per stimulus and rated Pleasantness, Musicality, Order, and Interest",
      "Number of Participants": "12",
      "participantsValue": 12,
      "Demographics": "Adults with normal hearing; 50% female, right-handed",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system + reference",
      "channelCountValue": 128,
      "Findings": "ISC and behavioral alignment lowest in reversed condition; backward temporal structure led to neural dissimilarity.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (clean)",
      "Recording Environment": "Sound-treated EEG lab",
      "Data Format": "Digital EEG (raw, clean, RCA), behavioral ratings",
      "Preprocessing": [
        "Filtering",
        "downsampling",
        "artifact rejection",
        "re-referencing",
        "spatial filtering (RCA)"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "RCA",
        "Inter-subject correlation",
        "Time-domain analysis"
      ],
      "Statistical Tests": [
        "ISC",
        "Variance explained",
        "Behavioral-EEG correlation"
      ],
      "Event Markers": [
        "Trial-aligned per condition and stimulus"
      ],
      "Publication": "ISMIR 2019 Late-Breaking/Demo",
      "year": 2019
    },
    {
      "id": "study-113",
      "Study Name": "NMED-H 2.0: Naturalistic Music EEG Dataset—Hindi",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Dmochowski, Jacek P., Norcia, Anthony M., and Berger, Jonathan",
      "Year": 2019,
      "DOI/URL": "https://purl.stanford.edu/sd922db3535",
      "Dataset": "https://purl.stanford.edu/sd922db3535",
      "Study Description": "This study examined how intact and manipulated temporal structure of Hindi pop music affects inter-subject EEG correlation.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Four Various Hindi pop songs – phase-scrambled",
      "Composer": "Various",
      "Passage Length": "~4.5 minutes",
      "passageLengthSeconds": 270,
      "Musical Features Analyzed": [
        "Phase incoherence",
        "Rhythm loss",
        "Cortical noise sensitivity"
      ],
      "normalizedFeatures": [
        "tempo"
      ],
      "Task Description": "Each participant heard one of four songs under four temporal conditions (intact, shuffled, reversed, scrambled); listened twice per stimulus and rated Pleasantness, Musicality, Order, and Interest",
      "Number of Participants": "12",
      "participantsValue": 12,
      "Demographics": "Adults with normal hearing; 50% female, right-handed",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system + reference",
      "channelCountValue": 128,
      "Findings": "Phase-scrambled condition yielded lowest ISC; loss of rhythm coherence disrupted EEG alignment.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (clean)",
      "Recording Environment": "Sound-treated EEG lab",
      "Data Format": "Digital EEG (raw, clean, RCA), behavioral ratings",
      "Preprocessing": [
        "Filtering",
        "downsampling",
        "artifact rejection",
        "re-referencing",
        "spatial filtering (RCA)"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "RCA",
        "Inter-subject correlation",
        "Time-domain analysis"
      ],
      "Statistical Tests": [
        "ISC",
        "Variance explained",
        "Behavioral-EEG correlation"
      ],
      "Event Markers": [
        "Trial-aligned per condition and stimulus"
      ],
      "Publication": "ISMIR 2019 Late-Breaking/Demo",
      "year": 2019
    },
    {
      "id": "study-114",
      "Study Name": "MAD-EEG: an EEG dataset for decoding auditory attention to a target instrument in polyphonic music",
      "Authors": "Giorgia Cantisani, Gabriel Trégoat, Slim Essid, Gaël Richard",
      "Year": 2019,
      "DOI/URL": "https://hal.telecom-paris.fr/hal-02291882v1",
      "Dataset": "https://zenodo.org/records/4537751#.YS5MOI4zYuU",
      "Study Description": "This is an open-access EEG dataset designed to decode auditory attention to a target instrument within polyphonic music, using realistic stimuli and single-trial recordings from subjects instructed to selectively attend to specific instruments in duets and trios.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Pop music duets (Voice, Guitar, Bass, Drums)",
      "Composer": "Multiple commercial pop",
      "Passage Length": "6 seconds (looped 4x)",
      "passageLengthSeconds": 6,
      "Musical Features Analyzed": [
        "Attention",
        "Spectrogram correlation",
        "Instrument tracking",
        "Spatial rendering"
      ],
      "normalizedFeatures": [
        "attention",
        "spectral"
      ],
      "Task Description": "Attend to one target instrument in 2-instrument pop mixes",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "Adults, 7 male 1 female, ages 23–54, mostly right-handed",
      "Musical Training": "Minimal Training (<5 years)",
      "EEG System Used": "B-Alert X24 (Advanced Brain Monitoring)",
      "Channel Count": "20 channels, 10–20 system",
      "channelCountValue": 20,
      "Findings": "Decoding succeeded in pop duets using simple linear model; performance linked to spatial cues and instrument familiarity. Significant correlation with attended pop instrument (r > unattended); EEG decoding accuracy >78% in duets.",
      "Sampling Rate": "256 Hz",
      "Recording Environment": "Audio-visual lab, stereo speaker setup at ±45°",
      "Data Format": "Digital EEG traces + synchronized stimuli",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "50 Hz notch",
        "time-aligned audio triggers"
      ],
      "License": "© Speech, Music and Mind Workshop 2019",
      "EEG Analysis Techniques": [
        "Stimulus reconstruction via linear regression",
        "Spectrogram alignment"
      ],
      "Statistical Tests": [
        "Pearson correlation",
        "Wilcoxon signed-rank",
        "Randomization test"
      ],
      "Event Markers": [
        "Stimulus onset via serial port timestamp"
      ],
      "Publication": "Speech, Music and Mind Workshop at Interspeech 2019",
      "year": 2019
    },
    {
      "id": "study-115",
      "Study Name": "MAD-EEG: an EEG dataset for decoding auditory attention to a target instrument in polyphonic music",
      "Authors": "Giorgia Cantisani, Gabriel Trégoat, Slim Essid, Gaël Richard",
      "Year": 2019,
      "DOI/URL": "https://hal.telecom-paris.fr/hal-02291882v1",
      "Dataset": "https://zenodo.org/records/4537751#.YS5MOI4zYuU",
      "Study Description": "This is an open-access EEG dataset designed to decode auditory attention to a target instrument within polyphonic music, using realistic stimuli and single-trial recordings from subjects instructed to selectively attend to specific instruments in duets and trios.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Pop music trios (Voice, Guitar, Bass, Drums)",
      "Composer": "Multiple commercial pop",
      "Passage Length": "6 seconds (looped 4x)",
      "passageLengthSeconds": 6,
      "Musical Features Analyzed": [
        "Attention",
        "Spectrogram correlation",
        "Instrument tracking",
        "Spatial rendering"
      ],
      "normalizedFeatures": [
        "attention",
        "spectral"
      ],
      "Task Description": "Attend to one target instrument in 3-instrument pop mixes",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "Adults, 7 male 1 female, ages 23–54, mostly right-handed",
      "Musical Training": "Minimal Training (<5 years)",
      "EEG System Used": "B-Alert X24 (Advanced Brain Monitoring)",
      "Channel Count": "20 channels, 10–20 system",
      "channelCountValue": 20,
      "Findings": "Instrument identification from EEG more difficult with 3 overlapping sources; attention decoding remains above chance",
      "Sampling Rate": "256 Hz",
      "Recording Environment": "Audio-visual lab, stereo speaker setup at ±45°",
      "Data Format": "Digital EEG traces + synchronized stimuli",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "50 Hz notch",
        "time-aligned audio triggers"
      ],
      "License": "© Speech, Music and Mind Workshop 2019",
      "EEG Analysis Techniques": [
        "Stimulus reconstruction via linear regression",
        "Spectrogram alignment"
      ],
      "Statistical Tests": [
        "Pearson correlation",
        "Wilcoxon signed-rank",
        "Randomization test"
      ],
      "Event Markers": [
        "Stimulus onset via serial port timestamp"
      ],
      "Publication": "Speech, Music and Mind Workshop at Interspeech 2019",
      "year": 2019
    },
    {
      "id": "study-116",
      "Study Name": "MAD-EEG: an EEG dataset for decoding auditory attention to a target instrument in polyphonic music",
      "Authors": "Giorgia Cantisani, Gabriel Trégoat, Slim Essid, Gaël Richard",
      "Year": 2019,
      "DOI/URL": "https://hal.telecom-paris.fr/hal-02291882v1",
      "Dataset": "https://zenodo.org/records/4537751#.YS5MOI4zYuU",
      "Study Description": "This is an open-access EEG dataset designed to decode auditory attention to a target instrument within polyphonic music, using realistic stimuli and single-trial recordings from subjects instructed to selectively attend to specific instruments in duets and trios.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Classical music duets (Flute, Oboe, French Horn, Bassoon, Cello)",
      "Composer": "Multiple classical excerpts",
      "Passage Length": "6 seconds (looped 4x)",
      "passageLengthSeconds": 6,
      "Musical Features Analyzed": [
        "Attention",
        "Spectrogram correlation",
        "Instrument tracking",
        "Spatial rendering"
      ],
      "normalizedFeatures": [
        "attention",
        "spectral"
      ],
      "Task Description": "Attend to one target instrument in 2-instrument classical mixes",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "Adults, 7 male 1 female, ages 23–54, mostly right-handed",
      "Musical Training": "Minimal Training (<5 years)",
      "EEG System Used": "B-Alert X24 (Advanced Brain Monitoring)",
      "Channel Count": "20 channels, 10–20 system",
      "channelCountValue": 20,
      "Findings": "EEG decoding correlated with musical line complexity and instrument prominence in classical duets",
      "Sampling Rate": "256 Hz",
      "Recording Environment": "Audio-visual lab, stereo speaker setup at ±45°",
      "Data Format": "Digital EEG traces + synchronized stimuli",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "50 Hz notch",
        "time-aligned audio triggers"
      ],
      "License": "© Speech, Music and Mind Workshop 2019",
      "EEG Analysis Techniques": [
        "Stimulus reconstruction via linear regression",
        "Spectrogram alignment"
      ],
      "Statistical Tests": [
        "Pearson correlation",
        "Wilcoxon signed-rank",
        "Randomization test"
      ],
      "Event Markers": [
        "Stimulus onset via serial port timestamp"
      ],
      "Publication": "Speech, Music and Mind Workshop at Interspeech 2019",
      "year": 2019
    },
    {
      "id": "study-117",
      "Study Name": "MAD-EEG: an EEG dataset for decoding auditory attention to a target instrument in polyphonic music",
      "Authors": "Giorgia Cantisani, Gabriel Trégoat, Slim Essid, Gaël Richard",
      "Year": 2019,
      "DOI/URL": "https://hal.telecom-paris.fr/hal-02291882v1",
      "Dataset": "https://zenodo.org/records/4537751#.YS5MOI4zYuU",
      "Study Description": "This is an open-access EEG dataset designed to decode auditory attention to a target instrument within polyphonic music, using realistic stimuli and single-trial recordings from subjects instructed to selectively attend to specific instruments in duets and trios.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Classical music trios (Flute, Oboe, French Horn, Bassoon, Cello)",
      "Composer": "Multiple classical excerpts",
      "Passage Length": "6 seconds (looped 4x)",
      "passageLengthSeconds": 6,
      "Musical Features Analyzed": [
        "Attention",
        "Spectrogram correlation",
        "Instrument tracking",
        "Spatial rendering"
      ],
      "normalizedFeatures": [
        "attention",
        "spectral"
      ],
      "Task Description": "Attend to one target instrument in 3-instrument classical mixes",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "Adults, 7 male 1 female, ages 23–54, mostly right-handed",
      "Musical Training": "Minimal Training (<5 years)",
      "EEG System Used": "B-Alert X24 (Advanced Brain Monitoring)",
      "Channel Count": "20 channels, 10–20 system",
      "channelCountValue": 20,
      "Findings": "Findings support feasibility of attention tracking in realistic classical trios, but accuracy drops with complexity",
      "Sampling Rate": "256 Hz",
      "Recording Environment": "Audio-visual lab, stereo speaker setup at ±45°",
      "Data Format": "Digital EEG traces + synchronized stimuli",
      "Preprocessing": [
        "Visual inspection",
        "ICA",
        "50 Hz notch",
        "time-aligned audio triggers"
      ],
      "License": "© Speech, Music and Mind Workshop 2019",
      "EEG Analysis Techniques": [
        "Stimulus reconstruction via linear regression",
        "Spectrogram alignment"
      ],
      "Statistical Tests": [
        "Pearson correlation",
        "Wilcoxon signed-rank",
        "Randomization test"
      ],
      "Event Markers": [
        "Stimulus onset via serial port timestamp"
      ],
      "Publication": "Speech, Music and Mind Workshop at Interspeech 2019",
      "year": 2019
    },
    {
      "id": "study-118",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "This study measured inter-subject correlation (ISC) of EEG responses to repeated listening of classical instrumental music, investigating effects of repetition, familiarity, and musical training on neural synchrony.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "La Gazza Ladra: Overture",
      "Composer": "Gioachino Rossini",
      "Passage Name": "La Gazza Ladra: Overture",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG-monitored passive listening; repeated exposure; attention/distract manipulation",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed: 10 participants with >1 year musical training",
      "EEG System Used": "BrainAmp DC (Exp1), BioSemi ActiveTwo (Exp2/3)",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC decreased most sharply with repetition for familiar music (Rossini excerpt).",
      "Sampling Rate": "1000 Hz (Exp1), 512 Hz (Exp2/3)",
      "Recording Environment": "Sound-treated laboratory room",
      "Data Format": "Digital EEG traces (filtered and downsampled)",
      "Preprocessing": [
        "Robust PCA",
        "Filtering",
        "EOG regression",
        "Artifact rejection"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis (ISC)",
        "Spectral Flux Computation"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA",
        "t-tests",
        "Permutation tests"
      ],
      "Event Markers": [
        "Onset and Offset Triggers with StimTracker"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-119",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "A Faust Symphony, 3. Mephistopheles",
      "Composer": "Franz Liszt",
      "Passage Name": "A Faust Symphony, Mephistopheles",
      "Passage Length": "1 minute 33 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG-monitored passive listening; repeated exposure; attention/distract manipulation",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC (Exp1), BioSemi ActiveTwo (Exp2/3)",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC decreased across repetitions for familiar Liszt excerpt.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Robust PCA",
        "Filtering",
        "Artifact rejection"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis (ISC)"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA",
        "t-tests",
        "Permutation"
      ],
      "Event Markers": [
        "Onset and Offset triggers"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-120",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "String Quartet No. 3 in D Major, Op. 44 No. 1, I. Molto allegro vivace",
      "Composer": "Felix Mendelssohn",
      "Passage Name": "String Quartet No. 3 in D Major, Op. 44 No. 1, I.",
      "Passage Length": "1 minute 46 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG-monitored passive listening; repeated exposure; attention/distract manipulation",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC decreased over repeats for familiar Mendelssohn excerpt.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Robust PCA",
        "Artifact Rejection",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "ISC via CCA"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA",
        "t-tests",
        "Permutation"
      ],
      "Event Markers": [
        "Time-lock to music onset"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-121",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Five Orchestral Pieces, Op. 16 I. Vorgefühle",
      "Composer": "Arnold Schoenberg",
      "Passage Name": "Five Orchestral Pieces Op. 16 I. Vorgefühle",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG-monitored passive listening; repeated exposure; attention/distract manipulation",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC sustained across repeats for unfamiliar Schoenberg excerpt.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Robust PCA",
        "Artifact Removal"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Time-locked triggers"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-122",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Contrapunctus IX from The Art of Fugue, BWV 1080",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Contrapunctus IX from The Art of Fugue",
      "Passage Length": "1 minute 5 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC remained high across repetitions for Bach fugue.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Robust PCA",
        "Artifact rejection"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Time-locked to music onset"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-123",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Adagio from Concerto for Oboe and Strings in D Minor",
      "Composer": "Alessandro Marcello",
      "Passage Name": "Adagio from Oboe Concerto in D Minor",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC decreased moderately over repetitions for familiar Marcello excerpt.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Robust PCA",
        "Filtering",
        "Artifact correction"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Event markers music onset"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-124",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Toccata in D Minor, BWV 565",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Toccata in D Minor BWV 565",
      "Passage Length": "59 seconds",
      "passageLengthSeconds": 59,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC strong initially but declined slightly across repeats for Bach Toccata.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact correction",
        "Robust PCA"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Stimulus onset aligned"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-125",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Sinfonia from Cantata No. 29",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Sinfonia from Cantata No. 29",
      "Passage Length": "1 minute",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC sustained across repeats for Sinfonia excerpt.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Filtering",
        "Artifact rejection",
        "ICA"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Time markers per repetition"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-126",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Violin Partita No. 2 in D Minor, BWV 1004: Chaconne",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Chaconne from Partita No. 2",
      "Passage Length": "1 minute 15 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC steady across repeats, high synchrony for Bach Chaconne.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Filtering",
        "EOG regression",
        "Artifact removal"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Stimulus onset triggers"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-127",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpt from \"Ich habe genug,\" BWV 82",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Ich habe genug BWV 82 excerpt",
      "Passage Length": "1 minute 1 second",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC started high and slightly declined across repeats.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact rejection",
        "Robust PCA"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Time-locked to music onset"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-128",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Minuet from Anna Magdalena Notebook",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Minuet from Anna Magdalena Notebook",
      "Passage Length": "55 seconds",
      "passageLengthSeconds": 55,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC moderately decreased over repetitions for Minuet.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact removal",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Stimulus onset per repeat"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-129",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpt from Brandenburg Concerto No. 3, BWV 1048",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Brandenburg Concerto No. 3 excerpt",
      "Passage Length": "1 minute 3 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC sustained at relatively high levels across repeats.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Robust PCA",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Music excerpt onset markers"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-130",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpt from Cello Suite No. 1 in G Major, BWV 1007",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Cello Suite No. 1 G Major excerpt",
      "Passage Length": "1 minute 7 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC gradually decreased with familiarity over repetitions.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact correction",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Beat-locked to stimulus onset"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-131",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Fugue from Well-Tempered Clavier Book 1 in C Major",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Fugue in C Major WTC Book 1",
      "Passage Length": "1 minute 8 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "High ISC initially; declined slightly by last repetition.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact rejection",
        "Robust PCA"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Stimulus-aligned trigger markers"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-132",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpt from Brandenburg Concerto No. 5, BWV 1050",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Brandenburg Concerto No. 5 excerpt",
      "Passage Length": "1 minute 7 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC initially high and stable; slight decline by final repeat.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact rejection",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Music excerpt onset aligned"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-133",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpt from Well-Tempered Clavier Book 2 in C Minor, BWV 871",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Fugue in C Minor WTC Book 2",
      "Passage Length": "1 minute 1 second",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC consistently high across all presentations.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact removal",
        "PCA"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Stimulus markers per excerpt"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-134",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Piano Sonata No. 11, K. 331 (1st Movement excerpt)",
      "Composer": "Wolfgang Amadeus Mozart",
      "Passage Name": "Piano Sonata No. 11 K. 331 excerpt",
      "Passage Length": "1 minute 2 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC moderate, slightly decreasing with repeats.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact cleaning",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Event-locked to music start"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-135",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpt from Mass in B Minor, BWV 232 (Gloria)",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Mass in B Minor Gloria excerpt",
      "Passage Length": "1 minute 1 second",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC stable and strong even across multiple repetitions.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Robust PCA",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Onset-aligned"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-136",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Excerpt from Harpsichord Concerto No. 5, BWV 1056",
      "Composer": "Johann Sebastian Bach",
      "Passage Name": "Harpsichord Concerto No. 5 excerpt",
      "Passage Length": "1 minute 5 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC declined slightly but remained robust by last repeat.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact rejection",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Stimulus-aligned"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-137",
      "Study Name": "Music synchronizes brainwaves across listeners with strong effects of repetition, familiarity, and training",
      "Authors": "Jens Madsen, Elizabeth Hellmuth Margulis, Rhimmon Simchy-Gross, Lucas C. Parra",
      "Year": 2019,
      "DOI/URL": "https://doi.org/10.1038/s41598-019-40254-w",
      "Study Description": "Same as above",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Piano Sonata in F Major, K. 280 (1st Movement excerpt)",
      "Composer": "Wolfgang Amadeus Mozart",
      "Passage Name": "Piano Sonata in F Major K. 280 excerpt",
      "Passage Length": "1 minute 0 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Familiarity",
        "ISC slope",
        "Repetition effects"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization"
      ],
      "Task Description": "Same as above",
      "Number of Participants": "20",
      "participantsValue": 20,
      "Demographics": "University students, ages 18–34, mixed gender",
      "Musical Training": "Mixed",
      "EEG System Used": "BrainAmp DC / BioSemi ActiveTwo",
      "Channel Count": "60/64 channels",
      "channelCountValue": 60,
      "Findings": "ISC dropped more quickly across repeats than other excerpts.",
      "Sampling Rate": "1000/512 Hz",
      "Recording Environment": "Sound-treated lab room",
      "Data Format": "Digital EEG",
      "Preprocessing": [
        "Artifact removal",
        "Filtering"
      ],
      "License": "Creative Commons Attribution 4.0",
      "EEG Analysis Techniques": [
        "Correlated Component Analysis"
      ],
      "Statistical Tests": [
        "Repeated-Measures ANOVA"
      ],
      "Event Markers": [
        "Music onset markers"
      ],
      "Publication": "Scientific Reports 9:3576",
      "year": 2019
    },
    {
      "id": "study-138",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 1 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-139",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 2 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-140",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 3 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-141",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 4 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-142",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 5 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-143",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 6 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-144",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 7 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-145",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 8 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-146",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 9 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-147",
      "Study Name": "Cortical Tracking of Acoustic Envelope during Naturalistic Music Listening",
      "Authors": "Di Liberto, Pelofi, Shamma, de Cheveigné",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1250/ast.41.361",
      "Study Description": "This study investigated how musical expertise affects cortical tracking of the acoustic envelope during continuous music listening, using EEG to measure envelope tracking fidelity across musicians and non-musicians.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Monophonic Excerpt 10 (MIDI violin/flute rendered with piano timbre)",
      "Composer": "J.S. Bach",
      "Passage Length": "~2 minutes",
      "passageLengthSeconds": 120,
      "Musical Features Analyzed": [
        "Acoustic envelope tracking (P1",
        "N1",
        "P2 TRF components)"
      ],
      "normalizedFeatures": [
        "envelope"
      ],
      "Task Description": "EEG-monitored passive listening; participants fixated visually during auditory stimulus",
      "Number of Participants": "14",
      "participantsValue": 14,
      "Demographics": "Healthy adults (ages 24–37), 7 musicians and 7 non-musicians",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "BioSemi ActiveTwo",
      "Channel Count": "64 channels",
      "channelCountValue": 64,
      "Findings": "Musicians exhibited significantly stronger cortical tracking (higher TRF amplitudes and reconstruction accuracies) compared to non-musicians; no effect of familiarity ratings between groups",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Sound-attenuated room",
      "Data Format": "Digital EEG signals",
      "Preprocessing": [
        "0.5–45 Hz Butterworth filter; downsampled to 128 Hz; bad channels interpolated; average mastoid referencing; artifact rejection applied"
      ],
      "License": "© 2020 Acoustical Society of Japan",
      "EEG Analysis Techniques": [
        "Multivariate Temporal Response Function (mTRF); Forward and Backward Models; Multiway Canonical Correlation Analysis (MCCA)"
      ],
      "Statistical Tests": [
        "Paired t-tests; permutation tests; effect size analysis; correlation analysis"
      ],
      "Event Markers": [
        "Trial timing (piece presented 3×)"
      ],
      "Publication": "Acoustical Science and Technology 41(1):361–364",
      "year": 2020
    },
    {
      "id": "study-148",
      "Study Name": "Natural music evokes correlated EEG responses reflecting temporal structure and beat",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Norcia, Anthony M., Dmochowski, Jacek P., Berger, Jonathan",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1016/j.neuroimage.2020.116559",
      "Dataset": "https://purl.stanford.edu/sd922db3535",
      "Study Description": "This study tested EEG synchronization and coherence using intact and manipulated Hindi pop songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Hindi pop song with intact and manipulated structure (measure-shuffled, reversed, scrambled)",
      "Composer": "Salim–Sulaiman",
      "Passage Name": "Ainvayi Ainvayi (Band Baaja Baaraat)",
      "Passage Length": "~4.5 minutes",
      "passageLengthSeconds": 270,
      "Musical Features Analyzed": [
        "Temporal structure",
        "Beat",
        "ISC",
        "SRC",
        "CPSD phase",
        "Coherence"
      ],
      "normalizedFeatures": [
        "tempo",
        "coherence",
        "synchronization",
        "localization"
      ],
      "Task Description": "Attentive passive listening; participants rated pleasantness, musicality, order, and interest after each condition",
      "Number of Participants": "48",
      "participantsValue": 48,
      "Demographics": "Right-handed adults, ages 18–34, normal hearing, mixed gender, 32 had formal musical training, mean = 7.57 years",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "All songs showed higher ISC and coherence in intact condition; manipulation reduced neural alignment.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Shielded booth with trigger-synced audio",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv)",
      "Preprocessing": [
        "Bandpass filtering (0.3–50 Hz)",
        "downsampling (125 Hz)",
        "ICA",
        "baseline correction",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "SRC",
        "RCA",
        "CPSD phase",
        "Coherence"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Linear mixed models",
        "Permutation testing",
        "Circular ANOVA"
      ],
      "Event Markers": [
        "Square-wave pulses via audio output to EEG amplifier"
      ],
      "Publication": "NeuroImage 214 (2020): 116559",
      "year": 2020
    },
    {
      "id": "study-149",
      "Study Name": "Natural music evokes correlated EEG responses reflecting temporal structure and beat",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Norcia, Anthony M., Dmochowski, Jacek P., Berger, Jonathan",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1016/j.neuroimage.2020.116559",
      "Dataset": "https://purl.stanford.edu/sd922db3535",
      "Study Description": "This study tested EEG synchronization and coherence using intact and manipulated Hindi pop songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Hindi pop song with intact and manipulated structure (measure-shuffled, reversed, scrambled)",
      "Composer": "Pritam Chakraborty",
      "Passage Name": "Daaru Desi (Cocktail)",
      "Passage Length": "~4.5 minutes",
      "passageLengthSeconds": 270,
      "Musical Features Analyzed": [
        "Temporal structure",
        "Beat",
        "ISC",
        "SRC",
        "CPSD phase",
        "Coherence"
      ],
      "normalizedFeatures": [
        "tempo",
        "coherence",
        "synchronization",
        "localization"
      ],
      "Task Description": "Attentive passive listening; participants rated pleasantness, musicality, order, and interest after each condition",
      "Number of Participants": "48",
      "participantsValue": 48,
      "Demographics": "Right-handed adults, ages 18–34, normal hearing, mixed gender",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "CPSD and SRC measures confirmed stronger alignment in intact versions compared to scrambled forms.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Shielded booth with trigger-synced audio",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv)",
      "Preprocessing": [
        "Bandpass filtering (0.3–50 Hz)",
        "downsampling (125 Hz)",
        "ICA",
        "baseline correction",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "SRC",
        "RCA",
        "CPSD phase",
        "Coherence"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Linear mixed models",
        "Permutation testing",
        "Circular ANOVA"
      ],
      "Event Markers": [
        "Square-wave pulses via audio output to EEG amplifier"
      ],
      "Publication": "NeuroImage 214 (2020): 116559",
      "year": 2020
    },
    {
      "id": "study-150",
      "Study Name": "Natural music evokes correlated EEG responses reflecting temporal structure and beat",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Norcia, Anthony M., Dmochowski, Jacek P., Berger, Jonathan",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1016/j.neuroimage.2020.116559",
      "Dataset": "https://purl.stanford.edu/sd922db3535",
      "Study Description": "This study tested EEG synchronization and coherence using intact and manipulated Hindi pop songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Hindi pop song with intact and manipulated structure (measure-shuffled, reversed, scrambled)",
      "Composer": "Salim–Sulaiman",
      "Passage Name": "Haule Haule (Rab Ne Bana Di Jodi)",
      "Passage Length": "~4.5 minutes",
      "passageLengthSeconds": 270,
      "Musical Features Analyzed": [
        "Temporal structure",
        "Beat",
        "ISC",
        "SRC",
        "CPSD phase",
        "Coherence"
      ],
      "normalizedFeatures": [
        "tempo",
        "coherence",
        "synchronization",
        "localization"
      ],
      "Task Description": "Attentive passive listening; participants rated pleasantness, musicality, order, and interest after each condition",
      "Number of Participants": "48",
      "participantsValue": 48,
      "Demographics": "Right-handed adults, ages 18–34, normal hearing, mixed gender",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Attentive listening and formal training improved EEG correlation; reversed structure disrupted beat tracking.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Shielded booth with trigger-synced audio",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv)",
      "Preprocessing": [
        "Bandpass filtering (0.3–50 Hz)",
        "downsampling (125 Hz)",
        "ICA",
        "baseline correction",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "SRC",
        "RCA",
        "CPSD phase",
        "Coherence"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Linear mixed models",
        "Permutation testing",
        "Circular ANOVA"
      ],
      "Event Markers": [
        "Square-wave pulses via audio output to EEG amplifier"
      ],
      "Publication": "NeuroImage 214 (2020): 116559",
      "year": 2020
    },
    {
      "id": "study-151",
      "Study Name": "Natural music evokes correlated EEG responses reflecting temporal structure and beat",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Norcia, Anthony M., Dmochowski, Jacek P., Berger, Jonathan",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1016/j.neuroimage.2020.116559",
      "Dataset": "https://purl.stanford.edu/sd922db3535",
      "Study Description": "This study tested EEG synchronization and coherence using intact and manipulated Hindi pop songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Hindi pop song with intact and manipulated structure (measure-shuffled, reversed, scrambled)",
      "Composer": "Pritam Chakraborty",
      "Passage Name": "Malang (Dhoom 3)",
      "Passage Length": "~4.5 minutes",
      "passageLengthSeconds": 270,
      "Musical Features Analyzed": [
        "Temporal structure",
        "Beat",
        "ISC",
        "SRC",
        "CPSD phase",
        "Coherence"
      ],
      "normalizedFeatures": [
        "tempo",
        "coherence",
        "synchronization",
        "localization"
      ],
      "Task Description": "Attentive passive listening; participants rated pleasantness, musicality, order, and interest after each condition",
      "Number of Participants": "48",
      "participantsValue": 48,
      "Demographics": "Right-handed adults, ages 18–34, normal hearing, mixed gender",
      "Musical Training": "Mixed Groups, Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Malang stimulus confirmed drop in neural coherence during temporal disruption; circular ANOVA supported timing sensitivity.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Shielded booth with trigger-synced audio",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv)",
      "Preprocessing": [
        "Bandpass filtering (0.3–50 Hz)",
        "downsampling (125 Hz)",
        "ICA",
        "baseline correction",
        "average referencing"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "SRC",
        "RCA",
        "CPSD phase",
        "Coherence"
      ],
      "Statistical Tests": [
        "Repeated-measures ANOVA",
        "Linear mixed models",
        "Permutation testing",
        "Circular ANOVA"
      ],
      "Event Markers": [
        "Square-wave pulses via audio output to EEG amplifier"
      ],
      "Publication": "NeuroImage 214 (2020): 116559",
      "year": 2020
    },
    {
      "id": "study-152",
      "Study Name": "Neural and physiological data from participants listening to affective music",
      "Authors": "Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1038/s41597-020-0507-6",
      "Dataset": "https://doi.org/10.18112/openneuro.ds002721.v1.0.1",
      "Study Description": "Participants passively listened to 12-second film music clips labeled for emotional content. After each excerpt, they rated their emotional response along eight dimensions using Likert scales.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Short film music clips labeled for emotion",
      "Composer": "Multiple film composers",
      "Passage Length": "12 seconds",
      "passageLengthSeconds": 12,
      "Musical Features Analyzed": [
        "Pleasantness",
        "Energy",
        "Tension",
        "Fear",
        "Happiness",
        "Sadness",
        "Anger",
        "Tenderness"
      ],
      "normalizedFeatures": [
        "emotion",
        "valence",
        "arousal"
      ],
      "Task Description": "Passive listening with self-report on 8 emotion axes using Likert scale",
      "Number of Participants": "31",
      "participantsValue": 31,
      "Demographics": "Ages 18–66, 13 male 18 female, healthy adults",
      "Musical Training": "Not Reported",
      "EEG System Used": "BrainProducts BrainAmp",
      "Channel Count": "19 channels, 10/20 system",
      "channelCountValue": 19,
      "Findings": "EEG analysis revealed frontal asymmetry correlates associated with distinct self-reported emotions (e.g., sadness, tension, happiness). Trial-averaged data aligned with expected patterns of affective brain responses.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Quiet lab with fixation cross",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "ICA",
        "visual inspection",
        "removal of noisy trials ±100μV"
      ],
      "License": "© 2020 The Authors (CC BY 4.0)",
      "EEG Analysis Techniques": [
        "Frontal asymmetry",
        "Emotion-tagged EEG analysis"
      ],
      "Statistical Tests": [
        "t-tests",
        "within-subject comparisons"
      ],
      "Event Markers": [
        "Time-locked to music onset"
      ],
      "Publication": "Nature Scientific Data 7:177 (2020)",
      "year": 2020
    },
    {
      "id": "study-153",
      "Study Name": "Neural and physiological data from participants listening to affective music",
      "Authors": "Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1038/s41597-020-0507-6",
      "Dataset": "https://doi.org/10.18112/openneuro.ds002722.v1.0.1",
      "Study Description": "Participants passively listened to 12-second film music clips labeled for emotional content. After each excerpt, they rated their emotional response along eight dimensions using Likert scales.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Music generated to induce affective trajectories (e.g., calm → excited)",
      "Composer": "Generated via Max/MSP",
      "Passage Length": "21 seconds",
      "passageLengthSeconds": 21,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Affective trajectories"
      ],
      "normalizedFeatures": [
        "emotion",
        "valence",
        "arousal"
      ],
      "Task Description": "Participants reported affective states during listening via FEELTRACE",
      "Number of Participants": "19",
      "participantsValue": 19,
      "Demographics": "Ages 19–30, 9 male 10 female",
      "Musical Training": "Not Reported",
      "EEG System Used": "BrainProducts BrainAmp",
      "Channel Count": "19 channels, 10/20 system",
      "channelCountValue": 19,
      "Findings": "Real-time affect reporting revealed predictable EEG and physiological patterns (e.g., GSR, ECG) aligned with target emotion trajectories. Frontal asymmetry tracked intended affective changes during listening.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Lab with onscreen affect-tracking (FEELTRACE)",
      "Data Format": "Digital EEG, ECG, GSR traces",
      "Preprocessing": [
        "Automated rejection",
        "threshold ±100μV",
        "visual spot-checking"
      ],
      "License": "© 2020 The Authors (CC BY 4.0)",
      "EEG Analysis Techniques": [
        "Frontal asymmetry",
        "Affective state tracking",
        "GSR-based feedback"
      ],
      "Statistical Tests": [
        "Multivariate analysis",
        "regressions",
        "t-tests"
      ],
      "Event Markers": [
        "Trial onset",
        "music onset",
        "FEELTRACE input"
      ],
      "Publication": "Nature Scientific Data 7:177 (2020)",
      "year": 2020
    },
    {
      "id": "study-154",
      "Study Name": "Neural and physiological data from participants listening to affective music",
      "Authors": "Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1038/s41597-020-0507-6",
      "Dataset": "https://doi.org/10.18112/openneuro.ds002724.v1.0.1",
      "Study Description": "Participants passively listened to 12-second film music clips labeled for emotional content. After each excerpt, they rated their emotional response along eight dimensions using Likert scales.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Music clips targeting affective transitions (valence/arousal shifts)",
      "Composer": "Generated via Max/MSP",
      "Passage Length": "40 seconds",
      "passageLengthSeconds": 40,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal trajectories over time"
      ],
      "normalizedFeatures": [
        "valence",
        "arousal"
      ],
      "Task Description": "Participants reported felt emotion in real-time with FEELTRACE",
      "Number of Participants": "10",
      "participantsValue": 10,
      "Demographics": "Ages 19–30, 3 male 7 female",
      "Musical Training": "Not Reported",
      "EEG System Used": "BrainProducts BrainAmp",
      "Channel Count": "19 channels, 10/20 system",
      "channelCountValue": 19,
      "Findings": "Temporal EEG dynamics and real-time FEELTRACE data demonstrated encoding of shifting valence/arousal states. Feedback loops enabled modeling of personalized affect trajectories over time.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Controlled lab with feedback display",
      "Data Format": "Digital EEG, ECG, GSR traces",
      "Preprocessing": [
        "Automated rejection",
        "threshold ±100μV",
        "visual spot-checking"
      ],
      "License": "© 2020 The Authors (CC BY 4.0)",
      "EEG Analysis Techniques": [
        "Temporal affect encoding",
        "EEG trajectories",
        "Feedback adaptation"
      ],
      "Statistical Tests": [
        "Repeated measures",
        "within-participant modeling"
      ],
      "Event Markers": [
        "Trial onset",
        "FEELTRACE reporting windows"
      ],
      "Publication": "Nature Scientific Data 7:177 (2020)",
      "year": 2020
    },
    {
      "id": "study-155",
      "Study Name": "Neural and physiological data from participants listening to affective music",
      "Authors": "Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1038/s41597-020-0507-6",
      "Dataset": "https://doi.org/10.18112/openneuro.ds002723.v1.1.0",
      "Study Description": "Participants passively listened to 12-second film music clips labeled for emotional content. After each excerpt, they rated their emotional response along eight dimensions using Likert scales.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "BCMI-generated affective music (online, personalized)",
      "Composer": "Generated via Max/MSP",
      "Passage Length": "60 seconds",
      "passageLengthSeconds": 60,
      "Musical Features Analyzed": [
        "Real-time affect control",
        "Music feedback loop"
      ],
      "normalizedFeatures": [
        "emotion"
      ],
      "Task Description": "BCMI attempts to change affective state in real time",
      "Number of Participants": "8",
      "participantsValue": 8,
      "Demographics": "Ages 19–30, 2 male 6 female",
      "Musical Training": "Not Reported",
      "EEG System Used": "BrainProducts BrainAmp",
      "Channel Count": "19 channels, 10/20 system",
      "channelCountValue": 19,
      "Findings": "EEG and physiological responses showed that online-generated music could dynamically influence user affect. Neural markers tracked effective affect modulation attempts, confirming feasibility of closed-loop BCMI.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Real-time feedback with auditory/music and visual cues",
      "Data Format": "Digital EEG, ECG, GSR traces",
      "Preprocessing": [
        "Post-hoc thresholding ±100μV",
        "visual inspection"
      ],
      "License": "© 2020 The Authors (CC BY 4.0)",
      "EEG Analysis Techniques": [
        "Affective trajectory tracking",
        "online decoding",
        "EEG-GSR correlation"
      ],
      "Statistical Tests": [
        "Mixed effects modeling",
        "feedback analysis"
      ],
      "Event Markers": [
        "Music onset",
        "system action marker"
      ],
      "Publication": "Nature Scientific Data 7:177 (2020)",
      "year": 2020
    },
    {
      "id": "study-156",
      "Study Name": "Neural and physiological data from participants listening to affective music",
      "Authors": "Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1038/s41597-020-0507-6",
      "Dataset": "https://doi.org/10.18112/openneuro.ds002720.v1.0.1",
      "Study Description": "Participants passively listened to 12-second film music clips labeled for emotional content. After each excerpt, they rated their emotional response along eight dimensions using Likert scales.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Max/MSP Tempo-adjustable music generated for motor-imagery control",
      "Composer": "Generated via Max/MSP",
      "Passage Length": "12 seconds (per trial)",
      "passageLengthSeconds": 12,
      "Musical Features Analyzed": [
        "Tempo",
        "Motor-imagery modulation",
        "Feedback"
      ],
      "normalizedFeatures": [
        "tempo",
        "imagery"
      ],
      "Task Description": "Users control tempo through upper limb motor imagery",
      "Number of Participants": "18",
      "participantsValue": 18,
      "Demographics": "Ages 19–28, 14 male 4 female",
      "Musical Training": "Not Reported",
      "EEG System Used": "BrainProducts BrainAmp",
      "Channel Count": "19 channels, 10/20 system",
      "channelCountValue": 19,
      "Findings": "Event-related desynchronization/synchronization (ERD/ERS) in EEG correlated with successful tempo modulation. Changes in tempo entrained sensorimotor EEG patterns, especially in motor-related electrodes.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Lab with onscreen tempo/ball feedback",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Visual rejection",
        "artifact detection on F3",
        "T3",
        "C3",
        "Cz",
        "P3"
      ],
      "License": "© 2020 The Authors (CC BY 4.0)",
      "EEG Analysis Techniques": [
        "Event-related desynchronization/synchronization (ERD/ERS)"
      ],
      "Statistical Tests": [
        "t-tests",
        "time-frequency analysis"
      ],
      "Event Markers": [
        "Trial start",
        "Feedback onset"
      ],
      "Publication": "Nature Scientific Data 7:177 (2020)",
      "year": 2020
    },
    {
      "id": "study-157",
      "Study Name": "Neural and physiological data from participants listening to affective music",
      "Authors": "Ian Daly, Nicoletta Nicolaou, Duncan Williams, Faustina Hwang, Alexis Kirke, Eduardo Miranda, Slawomir J. Nasuto",
      "Year": 2020,
      "DOI/URL": "https://doi.org/10.1038/s41597-020-0507-6",
      "Dataset": "https://doi.org/10.18112/openneuro.ds002725.v1.0.1",
      "Study Description": "Participants passively listened to 12-second film music clips labeled for emotional content. After each excerpt, they rated their emotional response along eight dimensions using Likert scales.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Computer-Generated Music",
      "Stimulus Description": "Same musical stimuli as BCMI study recorded during simultaneous EEG-fMRI",
      "Composer": "Generated via Max/MSP",
      "Passage Length": "40 seconds",
      "passageLengthSeconds": 40,
      "Musical Features Analyzed": [
        "Affective state",
        "EEG-fMRI cross-modal coherence"
      ],
      "normalizedFeatures": [
        "emotion",
        "coherence"
      ],
      "Task Description": "Participants listened while undergoing simultaneous EEG-fMRI recording",
      "Number of Participants": "1",
      "participantsValue": 1,
      "Demographics": "Right-handed male, age ~26",
      "Musical Training": "Not Reported",
      "EEG System Used": "EEG: BrainAmp MR Plus (MR-compatible) + fMRI (Siemens)",
      "Channel Count": "19 channel, 10/20 system + fMRI voxels",
      "channelCountValue": 19,
      "Findings": "Preliminary analysis confirmed alignment between EEG source components and fMRI signals during music listening. Artifact-corrected EEG and BOLD data suggested cross-modal coherence during emotion tracking.",
      "Sampling Rate": "5000 Hz",
      "Recording Environment": "fMRI scanner with MR-compatible EEG cap",
      "Data Format": "Digital EEG (EEGlab format), fMRI (NIfTI)",
      "Preprocessing": [
        "Gradient and pulse artifact correction",
        "ICA",
        "slice-time corrected fMRI"
      ],
      "License": "© 2020 The Authors (CC BY 4.0)",
      "EEG Analysis Techniques": [
        "EEG-fMRI correlation",
        "ICA source separation",
        "Artifact modeling"
      ],
      "Statistical Tests": [
        "Qualitative pilot analysis",
        "artifact noise assessment"
      ],
      "Event Markers": [
        "fMRI scan onset",
        "EEG sync pulses",
        "music onset"
      ],
      "Publication": "Nature Scientific Data 7:177 (2020)",
      "year": 2020
    },
    {
      "id": "study-158",
      "Study Name": "NMED-M Naturalistic Music EEG Dataset - Minimalism | Inter-Subject Correlation during New Music Listening: A Study of Electrophysiological and Behavioral Responses to Steve Reich’s Piano Phase",
      "Authors": "Dauer, Tysen, Nguyen, Duc T., Gang, Nick, Dmochowski, Jacek P., Berger, Jonathan, Kaneshiro, Blair",
      "Year": 2021,
      "DOI/URL": "https://doi.org/10.1101/2021.04.27.441708",
      "Dataset": "https://purl.stanford.edu/kt396gb0630",
      "Study Description": "Study examined EEG and behavioral synchronization to minimalist music using different versions of Piano Phase.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Original 1987 recording of *Piano Phase* by Nurit Tilles and Edmund Neimann",
      "Composer": "Steve Reich",
      "Passage Name": "Piano Phase (Original)",
      "Passage Length": "6 minutes",
      "passageLengthSeconds": 360,
      "Musical Features Analyzed": [
        "Minimalist structure",
        "Temporal phasing",
        "Inter-subject correlation"
      ],
      "normalizedFeatures": [
        "coherence",
        "synchronization",
        "localization"
      ],
      "Task Description": "Participants passively listened to full piece and later provided behavioral ratings and continuous engagement tracking",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "Adults with normal hearing, mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Original version showed strong ISC and CB alignment, confirming neural coherence in minimalist structure.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (cleaned)",
      "Recording Environment": "Acoustically treated EEG lab",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv), Continuous behavior (.mat)",
      "Preprocessing": [
        "High-pass filter",
        "downsampling",
        "ICA",
        "interpolation",
        "re-referencing",
        "trial averaging"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "RCA",
        "EEG–CB alignment",
        "Engagement analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures correlation",
        "ISC comparisons"
      ],
      "Event Markers": [
        "Stimulus-aligned EEG and CB markers"
      ],
      "Publication": "bioRxiv 2021.04.27.441708",
      "year": 2021
    },
    {
      "id": "study-159",
      "Study Name": "NMED-M Naturalistic Music EEG Dataset - Minimalism | Inter-Subject Correlation during New Music Listening: A Study of Electrophysiological and Behavioral Responses to Steve Reich’s Piano Phase",
      "Authors": "Dauer, Tysen, Nguyen, Duc T., Gang, Nick, Dmochowski, Jacek P., Berger, Jonathan, Kaneshiro, Blair",
      "Year": 2021,
      "DOI/URL": "https://doi.org/10.1101/2021.04.27.441708",
      "Dataset": "https://purl.stanford.edu/kt396gb0630",
      "Study Description": "Study examined EEG and behavioral synchronization to minimalist music using different versions of Piano Phase.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Matt Winn’s *Phased & Konfused Mix* of *Piano Phase*",
      "Composer": "Steve Reich, Matt Winn",
      "Passage Name": "Piano Phase (Remix)",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Remix structure",
        "Beat-based layering",
        "Listener engagement"
      ],
      "normalizedFeatures": [
        "tempo"
      ],
      "Task Description": "Participants passively listened to full piece and later provided behavioral ratings and continuous engagement tracking",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "Adults with normal hearing, mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Remix version induced distinct engagement patterns; beat layering increased neural synchrony.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (cleaned)",
      "Recording Environment": "Acoustically treated EEG lab",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv), Continuous behavior (.mat)",
      "Preprocessing": [
        "High-pass filter",
        "downsampling",
        "ICA",
        "interpolation",
        "re-referencing",
        "trial averaging"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "RCA",
        "EEG–CB alignment",
        "Engagement analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures correlation",
        "ISC comparisons"
      ],
      "Event Markers": [
        "Stimulus-aligned EEG and CB markers"
      ],
      "Publication": "bioRxiv 2021.04.27.441708",
      "year": 2021
    },
    {
      "id": "study-160",
      "Study Name": "NMED-M Naturalistic Music EEG Dataset - Minimalism | Inter-Subject Correlation during New Music Listening: A Study of Electrophysiological and Behavioral Responses to Steve Reich’s Piano Phase",
      "Authors": "Dauer, Tysen, Nguyen, Duc T., Gang, Nick, Dmochowski, Jacek P., Berger, Jonathan, Kaneshiro, Blair",
      "Year": 2021,
      "DOI/URL": "https://doi.org/10.1101/2021.04.27.441708",
      "Dataset": "https://purl.stanford.edu/kt396gb0630",
      "Study Description": "Study examined EEG and behavioral synchronization to minimalist music using different versions of Piano Phase.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Abrupt change version of *Piano Phase* with sudden transitions between sections",
      "Composer": "Steve Reich",
      "Passage Name": "Piano Phase (Abrupt Change)",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Discontinuous structure",
        "Repetition violation",
        "Engagement shifts"
      ],
      "normalizedFeatures": [
        "expectancy"
      ],
      "Task Description": "Participants passively listened to full piece and later provided behavioral ratings and continuous engagement tracking",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "Adults with normal hearing, mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Abrupt change version reduced ISC; sudden transitions disrupted temporal predictability.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (cleaned)",
      "Recording Environment": "Acoustically treated EEG lab",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv), Continuous behavior (.mat)",
      "Preprocessing": [
        "High-pass filter",
        "downsampling",
        "ICA",
        "interpolation",
        "re-referencing",
        "trial averaging"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "RCA",
        "EEG–CB alignment",
        "Engagement analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures correlation",
        "ISC comparisons"
      ],
      "Event Markers": [
        "Stimulus-aligned EEG and CB markers"
      ],
      "Publication": "bioRxiv 2021.04.27.441708",
      "year": 2021
    },
    {
      "id": "study-161",
      "Study Name": "NMED-M Naturalistic Music EEG Dataset - Minimalism | Inter-Subject Correlation during New Music Listening: A Study of Electrophysiological and Behavioral Responses to Steve Reich’s Piano Phase",
      "Authors": "Dauer, Tysen, Nguyen, Duc T., Gang, Nick, Dmochowski, Jacek P., Berger, Jonathan, Kaneshiro, Blair",
      "Year": 2021,
      "DOI/URL": "https://doi.org/10.1101/2021.04.27.441708",
      "Dataset": "https://purl.stanford.edu/kt396gb0630",
      "Study Description": "Study examined EEG and behavioral synchronization to minimalist music using different versions of Piano Phase.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Segment shuffle version of *Piano Phase* with reordered musical sections",
      "Composer": "Steve Reich",
      "Passage Name": "Piano Phase (Segment Shuffle)",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Structural unpredictability",
        "Minimalist motif variation",
        "Neural entrainment"
      ],
      "normalizedFeatures": [
        "synchronization"
      ],
      "Task Description": "Participants passively listened to full piece and later provided behavioral ratings and continuous engagement tracking",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "Adults with normal hearing, mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Segment shuffle version showed lower ISC; unpredictable structure weakened entrainment.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (cleaned)",
      "Recording Environment": "Acoustically treated EEG lab",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv), Continuous behavior (.mat)",
      "Preprocessing": [
        "High-pass filter",
        "downsampling",
        "ICA",
        "interpolation",
        "re-referencing",
        "trial averaging"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "RCA",
        "EEG–CB alignment",
        "Engagement analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures correlation",
        "ISC comparisons"
      ],
      "Event Markers": [
        "Stimulus-aligned EEG and CB markers"
      ],
      "Publication": "bioRxiv 2021.04.27.441708",
      "year": 2021
    },
    {
      "id": "study-162",
      "Study Name": "NMED-M Naturalistic Music EEG Dataset - Minimalism | Inter-Subject Correlation during New Music Listening: A Study of Electrophysiological and Behavioral Responses to Steve Reich’s Piano Phase",
      "Authors": "Dauer, Tysen, Nguyen, Duc T., Gang, Nick, Dmochowski, Jacek P., Berger, Jonathan, Kaneshiro, Blair",
      "Year": 2021,
      "DOI/URL": "https://doi.org/10.1101/2021.04.27.441708",
      "Dataset": "https://purl.stanford.edu/kt396gb0630",
      "Study Description": "Study examined EEG and behavioral synchronization to minimalist music using different versions of Piano Phase.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Tremolo version of *Piano Phase* with pitch held constant and only rhythm preserved",
      "Composer": "Steve Reich",
      "Passage Name": "Piano Phase (Tremolo)",
      "Passage Length": "5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Rhythmic regularity",
        "Pitch monotony",
        "Listener arousal"
      ],
      "normalizedFeatures": [
        "arousal",
        "melody"
      ],
      "Task Description": "Participants passively listened to full piece and later provided behavioral ratings and continuous engagement tracking",
      "Number of Participants": "30",
      "participantsValue": 30,
      "Demographics": "Adults with normal hearing, mixed gender",
      "Musical Training": "Not Reported",
      "EEG System Used": "Electrical Geodesics GES300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Tremolo version decreased ISC; pitch monotony altered listener arousal and engagement.",
      "Sampling Rate": "1000 Hz (raw), 125 Hz (cleaned)",
      "Recording Environment": "Acoustically treated EEG lab",
      "Data Format": "Digital EEG (.mat), Behavioral ratings (.csv), Continuous behavior (.mat)",
      "Preprocessing": [
        "High-pass filter",
        "downsampling",
        "ICA",
        "interpolation",
        "re-referencing",
        "trial averaging"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "ISC",
        "RCA",
        "EEG–CB alignment",
        "Engagement analysis"
      ],
      "Statistical Tests": [
        "Repeated-measures correlation",
        "ISC comparisons"
      ],
      "Event Markers": [
        "Stimulus-aligned EEG and CB markers"
      ],
      "Publication": "bioRxiv 2021.04.27.441708",
      "year": 2021
    },
    {
      "id": "study-163",
      "Study Name": "The Effect of Music Listening on EEG Functional Connectivity of Brain: A Short-Duration and Long-Duration Study",
      "Authors": "Danyal Mahmood, Humaira Nisar, Vooi Voon Yap, Chi-Yi Tsai",
      "Year": 2022,
      "DOI/URL": "https://doi.org/10.3390/math10030349",
      "Study Description": "Short-duration study comparing EEG-FC after listening to favorite vs. relaxing music",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Favorite music chosen by participant",
      "Composer": "Varied (participant-selected)",
      "Passage Length": "~5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Functional connectivity",
        "EEG frequency bands (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "coherence",
        "spectral"
      ],
      "Task Description": "Passive listening while EEG recorded; comparisons made to pre-music baseline",
      "Number of Participants": "Not Reported",
      "participantsValue": -1,
      "Demographics": "Not Reported",
      "Musical Training": "Not Reported",
      "EEG System Used": "Emotiv EPOC",
      "Channel Count": "14 channels, 10-20 system",
      "channelCountValue": 14,
      "Findings": "Alpha and beta band connectivity increased more after favorite music than relaxing music; theta band increased more after relaxing music",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass filtering (0.5–60 Hz)",
        "ICA for ocular artifact",
        "MATLAB-based frequency decomposition"
      ],
      "License": "Creative Commons Attribution (CC BY)",
      "EEG Analysis Techniques": [
        "Inter-Site Phase Clustering (ISPC)",
        "Graph theory",
        "Connectivity matrices"
      ],
      "Statistical Tests": [
        "ANOVA",
        "t-test"
      ],
      "Event Markers": [
        "Time-locked to start of stimulus"
      ],
      "Publication": "Mathematics 10(3):349",
      "year": 2022
    },
    {
      "id": "study-164",
      "Study Name": "The Effect of Music Listening on EEG Functional Connectivity of Brain: A Short-Duration and Long-Duration Study",
      "Authors": "Danyal Mahmood, Humaira Nisar, Vooi Voon Yap, Chi-Yi Tsai",
      "Year": 2022,
      "DOI/URL": "https://doi.org/10.3390/math10030349",
      "Study Description": "Short-duration study comparing EEG-FC after listening to favorite vs. relaxing music",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Relaxing music with alpha binaural beats",
      "Passage Length": "~5 minutes",
      "passageLengthSeconds": 300,
      "Musical Features Analyzed": [
        "Functional connectivity",
        "EEG frequency bands (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "coherence",
        "spectral"
      ],
      "Task Description": "Passive listening while EEG recorded; comparisons made to pre-music baseline",
      "Number of Participants": "Not Reported",
      "participantsValue": -1,
      "Demographics": "Not Reported",
      "Musical Training": "Not Reported",
      "EEG System Used": "Emotiv EPOC",
      "Channel Count": "14 channels, 10-20 system",
      "channelCountValue": 14,
      "Findings": "Relaxing music increased theta and average connectivity more than favorite music; showed stronger relaxation-related FC changes",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass filtering (0.5–60 Hz)",
        "ICA for ocular artifact",
        "MATLAB-based frequency decomposition"
      ],
      "License": "Creative Commons Attribution (CC BY)",
      "EEG Analysis Techniques": [
        "Inter-Site Phase Clustering (ISPC)",
        "Graph theory",
        "Connectivity matrices"
      ],
      "Statistical Tests": [
        "ANOVA",
        "t-test"
      ],
      "Event Markers": [
        "Time-locked to start of stimulus"
      ],
      "Publication": "Mathematics 10(3):349",
      "year": 2022
    },
    {
      "id": "study-165",
      "Study Name": "The Effect of Music Listening on EEG Functional Connectivity of Brain: A Short-Duration and Long-Duration Study",
      "Authors": "Danyal Mahmood, Humaira Nisar, Vooi Voon Yap, Chi-Yi Tsai",
      "Year": 2022,
      "DOI/URL": "https://doi.org/10.3390/math10030349",
      "Study Description": "Long-duration study assessing daily music listening impact on EEG-FC",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Relaxing music with alpha binaural beats over 2 weeks",
      "Passage Length": "30 minutes/day for 14 days",
      "passageLengthSeconds": 1800,
      "Musical Features Analyzed": [
        "Functional connectivity",
        "EEG frequency bands (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "coherence",
        "spectral"
      ],
      "Task Description": "Music Listening Group listened daily; EEG at baseline, week 1, and week 2",
      "Number of Participants": "Not Reported",
      "participantsValue": -1,
      "Demographics": "Not Reported",
      "Musical Training": "Not Reported",
      "EEG System Used": "Emotiv EPOC",
      "Channel Count": "14 channels, 10-20 system",
      "channelCountValue": 14,
      "Findings": "Increased delta, alpha, and beta connectivity over time in the Music Listening Group; Control group showed no significant changes",
      "Sampling Rate": "128 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Bandpass filtering (0.5–60 Hz)",
        "ICA for ocular artifact",
        "MATLAB-based frequency decomposition"
      ],
      "License": "Creative Commons Attribution (CC BY)",
      "EEG Analysis Techniques": [
        "Inter-Site Phase Clustering (ISPC)",
        "Graph theory",
        "Connectivity matrices"
      ],
      "Statistical Tests": [
        "ANOVA",
        "t-test"
      ],
      "Event Markers": [
        "Time-locked to weekly measurement"
      ],
      "Publication": "Mathematics 10(3):349",
      "year": 2022
    },
    {
      "id": "study-166",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "1 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Gen Hoshino",
      "Passage Name": "Koi",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-167",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "2 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Ed Sheeran",
      "Passage Name": "Shape of You",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-168",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "3 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Daoko X Kenshi Yonezu",
      "Passage Name": "Uchiage Hanabi",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-169",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "4 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Keyakizaka46",
      "Passage Name": "Fukyōwaon",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-170",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "5 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Keyakizaka46",
      "Passage Name": "Futari Saison",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-171",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "6 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Twice",
      "Passage Name": "TT",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-172",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "7 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Nogizaka 46",
      "Passage Name": "Influencer",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-173",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "8 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Pikotaro",
      "Passage Name": "PPAP (Pen-Pineapple-Apple-Pen)",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-174",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "9 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Radwimps",
      "Passage Name": "Zenzenzense",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-175",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "10 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Keyakizaka46",
      "Passage Name": "Silent Majority",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-176",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "11 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Austin Mahone",
      "Passage Name": "Dirty Work",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-177",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "12 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Nogizaka 46",
      "Passage Name": "Nigemizu",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-178",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "13 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Gen Hoshino",
      "Passage Name": "Family Song",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-179",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "14 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Kenshi Yonezu",
      "Passage Name": "Peace Sign",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-180",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "15 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "AKB48",
      "Passage Name": "Negaigoto no Mochigusare",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-181",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "16 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Nogizaka 46",
      "Passage Name": "Itsuka Dekiru kara Kyō Dekiru",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-182",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "17 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Keyakizaka46",
      "Passage Name": "Kaze ni Fukarete mo",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-183",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "18 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Perfume",
      "Passage Name": "Tokyo Girl",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-184",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "19 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Suchmos",
      "Passage Name": "Stay Tune",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-185",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "20 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Sekai no Owari",
      "Passage Name": "Rain",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-186",
      "Study Name": "Inter-subject correlations of EEG reflect subjective arousal and acoustic features of music",
      "Authors": "Fuyu Ueno, Sotaro Shimada",
      "Year": 2023,
      "DOI/URL": "https://doi.org/10.3389/fnhum.2023.1225377",
      "Study Description": "This study used concurrent EEG-fMRI and EEG-only recordings to reconstruct music perception from brain activity using source-localized EEG and neural decoding models. Examines whether EEG ISC reflects population music preferences using Billboard chart songs.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "21 on Billboard Japan Hot 100. 62-second edited clip including chorus and surrounding sections, normalized with fade-in and fade-out",
      "Composer": "Justin Bieber",
      "Passage Name": "What Do You Mean?",
      "Passage Length": "62 seconds",
      "passageLengthSeconds": 62,
      "Musical Features Analyzed": [
        "Tonality",
        "Tempo",
        "EEG ISC",
        "Principal Component Analysis"
      ],
      "normalizedFeatures": [
        "tempo",
        "harmony",
        "coherence",
        "synchronization"
      ],
      "Task Description": "EEG during passive music listening followed by self-report of preference, enjoyment, frequency of listening, and arousal",
      "Number of Participants": "17",
      "participantsValue": 17,
      "Demographics": "Healthy Japanese adults, mean age 21.4 ± 0.69, 7 female",
      "Musical Training": "Mixed Groups",
      "EEG System Used": "g.tec (BCI Research System), 30-channel setup",
      "Channel Count": "30 channels, 10-20 system",
      "channelCountValue": 30,
      "Findings": "Higher-ranked music had significantly higher ISC values, suggesting greater listener engagement. Principal component analysis also revealed distinct clusters differing in arousal, tonality, and tempo—linking ISC to both emotional response and musical features.",
      "Sampling Rate": "512 Hz",
      "Recording Environment": "Lab with headphones, visual fixation cross",
      "Data Format": "Not Reported",
      "Preprocessing": [
        "Bandpass 1–60 Hz",
        "50Hz notch filter",
        "ICA",
        "CorrCA"
      ],
      "License": "CC BY",
      "EEG Analysis Techniques": [
        "ISC",
        "CorrCA",
        "PCA",
        "clustering",
        "GLM"
      ],
      "Statistical Tests": [
        "t-test",
        "ANOVA",
        "Tukey’s HSD"
      ],
      "Event Markers": [
        "Not Reported"
      ],
      "Publication": "Frontiers in Human Neuroscience Volume 17 (2023)",
      "year": 2023
    },
    {
      "id": "study-187",
      "Study Name": "NMED-E Naturalistic Music EEG Dataset - Elgar | Inter-subject correlation of electroencephalographic and behavioural responses reflects time-varying engagement with natural music",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Norcia, Anthony M., Dmochowski, Jacek P., Berger, Jonathan",
      "Year": 2024,
      "DOI/URL": "https://doi.org/10.1111/ejn.16324",
      "Dataset": "https://purl.stanford.edu/pp371jh5722",
      "Study Description": "This study investigated how neural and behavioral responses align over time during listening to naturalistic classical music.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Original 1965 performance of Elgar’s Cello Concerto in E Minor",
      "Composer": "Edward Elgar",
      "Passage Name": "Cello Concerto in E Minor, Op. 85 (Movement I)",
      "Passage Length": "8 minutes",
      "passageLengthSeconds": 480,
      "Musical Features Analyzed": [
        "Temporal structure",
        "Engagement",
        "Inter-subject correlation",
        "Amplitude envelope"
      ],
      "normalizedFeatures": [
        "coherence",
        "power",
        "synchronization",
        "envelope",
        "localization"
      ],
      "Task Description": "EEG recorded during passive listening to full piece; followed by behavioral ratings and continuous behavioral tracking in separate session",
      "Number of Participants": "24",
      "participantsValue": 24,
      "Demographics": "Right-handed adult musicians, ages ~18–35, normal hearing, fluent English, ≥5 years formal classical training, no cello experience",
      "Musical Training": "Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES 300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Significant ISC and EEG–CB correlation observed; time-resolved ISC revealed peaks in listener engagement.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Acoustically and electrically shielded EEG booth",
      "Data Format": "Digital EEG (raw and cleaned), Behavioral ratings and continuous engagement",
      "Preprocessing": [
        "High-pass (.3 Hz)",
        "notch (59–61 Hz)",
        "low-pass (50 Hz)",
        "downsampling (125 Hz)",
        "ICA for artifact removal",
        "average referencing",
        "baseline-corrected",
        "interpolation"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "RCA",
        "ISC",
        "Time-resolved ISC",
        "EEG-CB cross-correlation"
      ],
      "Statistical Tests": [
        "Wilcoxon signed-rank tests",
        "Permutation testing",
        "FDR correction"
      ],
      "Event Markers": [
        "Time-locked audio triggers via audio channel pulses"
      ],
      "Publication": "European Journal of Neuroscience 59(12):3162–3183",
      "year": 2024
    },
    {
      "id": "study-188",
      "Study Name": "NMED-E Naturalistic Music EEG Dataset - Elgar | Inter-subject correlation of electroencephalographic and behavioural responses reflects time-varying engagement with natural music",
      "Authors": "Kaneshiro, Blair, Nguyen, Duc T., Norcia, Anthony M., Dmochowski, Jacek P., Berger, Jonathan",
      "Year": 2024,
      "DOI/URL": "https://doi.org/10.1111/ejn.16324",
      "Dataset": "https://purl.stanford.edu/pp371jh5722",
      "Study Description": "This study investigated how neural and behavioral responses align over time during listening to naturalistic classical music.",
      "Paradigm Type": [
        "Naturalistic"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Phase-scrambled version of Elgar’s concerto with amplitude envelope preserved",
      "Composer": "Edward Elgar",
      "Passage Name": "Cello Concerto in E Minor, Op. 85 (Phase-Scrambled)",
      "Passage Length": "8 minutes",
      "passageLengthSeconds": 480,
      "Musical Features Analyzed": [
        "Amplitude envelope",
        "Temporal disorganization",
        "Inter-subject correlation"
      ],
      "normalizedFeatures": [
        "coherence",
        "power",
        "synchronization",
        "envelope",
        "localization"
      ],
      "Task Description": "EEG recorded during passive listening to control stimulus; followed by behavioral ratings and continuous behavioral tracking in separate session",
      "Number of Participants": "24",
      "participantsValue": 24,
      "Demographics": "Right-handed adult musicians, ages ~18–35, normal hearing, fluent English, ≥5 years formal classical training, no cello experience",
      "Musical Training": "Moderate Training (5-10 years)",
      "EEG System Used": "Electrical Geodesics GES 300 (EGI)",
      "Channel Count": "128 channels, 10–10 system",
      "channelCountValue": 128,
      "Findings": "Phase-scrambled version had lower ISC; preserved amplitude envelope supported partial alignment.",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Acoustically and electrically shielded EEG booth",
      "Data Format": "Digital EEG (raw and cleaned), Behavioral ratings and continuous engagement",
      "Preprocessing": [
        "High-pass (.3 Hz)",
        "notch (59–61 Hz)",
        "low-pass (50 Hz)",
        "downsampling (125 Hz)",
        "ICA for artifact removal",
        "average referencing",
        "baseline-corrected",
        "interpolation"
      ],
      "License": "CC BY 4.0",
      "EEG Analysis Techniques": [
        "RCA",
        "ISC",
        "Time-resolved ISC",
        "EEG-CB cross-correlation"
      ],
      "Statistical Tests": [
        "Wilcoxon signed-rank tests",
        "Permutation testing",
        "FDR correction"
      ],
      "Event Markers": [
        "Time-locked audio triggers via audio channel pulses"
      ],
      "Publication": "European Journal of Neuroscience 59(12):3162–3183",
      "year": 2024
    },
    {
      "id": "study-189",
      "Study Name": "Neural encoding of musical expectations in a nonhuman primate",
      "Authors": "Roberta Bianco, Giacomo Novembre, Francesco Contini, Eleonora De Pisapia, Piergiorgio Trevisan, Mathew Diamond, Leonardo Chelazzi",
      "Year": 2024,
      "DOI/URL": "https://doi.org/10.1016/j.cub.2023.12.019",
      "Dataset": "https://doi.org/10.48557/U5SHX6",
      "Study Description": "This EEG study assessed how two rhesus monkeys encode pitch- and timing-based surprise while listening to original and temporally shuffled Bach melodies from www.jsbach.net.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "10 real piano melodies composed by J.S. Bach",
      "Composer": "Johann Sebastian Bach",
      "Passage Length": "~158.07 seconds",
      "passageLengthSeconds": 158.07,
      "Musical Features Analyzed": [
        "Pitch-based surprise",
        "Timing-based surprise",
        "Entropy",
        "ERP",
        "mTRF"
      ],
      "normalizedFeatures": [
        "melody",
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening to music while watching silent videos",
      "Number of Participants": "2",
      "participantsValue": 2,
      "Demographics": "Male rhesus monkeys, 11 years old, 10 and 9.8 kg",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Biosemi Active-2 system with custom monkey EEG caps",
      "Channel Count": "26 channels",
      "channelCountValue": 26,
      "Findings": "ERP amplitudes were larger for unexpected (high surprise) notes in real vs. shuffled music, indicating sensitivity to structure. Timing-based expectations, but not pitch-based, enhanced prediction accuracy in monkeys.",
      "Sampling Rate": "1024 Hz",
      "Recording Environment": "Primate chair in controlled lab, passive viewing of silent visuals",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "ERP segmentation",
        "Bandpass filtering",
        "Quantization to 16th notes",
        "Amplitude normalization"
      ],
      "License": "CC BY license",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "mTRF modeling",
        "Pitch and timing surprise modeling"
      ],
      "Statistical Tests": [
        "Cluster-based permutation test",
        "Linear mixed-effects model"
      ],
      "Event Markers": [
        "Time-locked to note onsets and stimulus surprise values"
      ],
      "Publication": "Current Biology 34(2):444–450",
      "year": 2024
    },
    {
      "id": "study-190",
      "Study Name": "Neural encoding of musical expectations in a nonhuman primate",
      "Authors": "Roberta Bianco, Giacomo Novembre, Francesco Contini, Eleonora De Pisapia, Piergiorgio Trevisan, Mathew Diamond, Leonardo Chelazzi",
      "Year": 2024,
      "DOI/URL": "https://doi.org/10.1016/j.cub.2023.12.019",
      "Dataset": "https://doi.org/10.48557/U5SHX6",
      "Study Description": "This EEG study assessed how two rhesus monkeys encode pitch- and timing-based surprise while listening to original and temporally shuffled Bach melodies from www.jsbach.net.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "4 shuffled melodies derived from real Bach melodies with pitch/timing disruption",
      "Composer": "Johann Sebastian Bach (source material)",
      "Passage Length": "~158.07 seconds",
      "passageLengthSeconds": 158.07,
      "Musical Features Analyzed": [
        "Pitch-based surprise",
        "Timing-based entropy",
        "ERP",
        "mTRF"
      ],
      "normalizedFeatures": [
        "melody",
        "erp",
        "expectancy"
      ],
      "Task Description": "Passive listening to randomized pitch/timing melodies while watching silent videos",
      "Number of Participants": "2",
      "participantsValue": 2,
      "Demographics": "Male rhesus monkeys, 11 years old, 10 and 9.8 kg",
      "Musical Training": "No Formal Training",
      "EEG System Used": "Biosemi Active-2 system with custom monkey EEG caps",
      "Channel Count": "26 channels",
      "channelCountValue": 26,
      "Findings": "No significant ERP or mTRF prediction effects emerged. Monkeys failed to track surprise in these scrambled sequences, indicating expectations are not driven purely by acoustics.",
      "Sampling Rate": "1024 Hz",
      "Recording Environment": "Primate chair in controlled lab, passive viewing of silent visuals",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "ERP segmentation",
        "Bandpass filtering",
        "Quantization to 16th notes",
        "Amplitude normalization"
      ],
      "License": "CC BY license",
      "EEG Analysis Techniques": [
        "ERP averaging",
        "mTRF modeling",
        "Pitch and timing surprise modeling"
      ],
      "Statistical Tests": [
        "Cluster-based permutation test",
        "Linear mixed-effects model"
      ],
      "Event Markers": [
        "Time-locked to note onsets and stimulus surprise values"
      ],
      "Publication": "Current Biology 34(2):444–450",
      "year": 2024
    },
    {
      "id": "study-191",
      "Study Name": "Music tempo modulates emotional states as revealed through EEG insights",
      "Authors": "Zengyao Yang, Qiruo Su, Jieren Xie, Hechong Su, Tianrun Huang, Chengcheng Han, Sicong Zhang, Kai Zhang, Guanghua Xu",
      "Year": 2025,
      "DOI/URL": "https://doi.org/10.1038/s41598-025-92679-1",
      "Study Description": "This study explored how different tempi modulate valence, arousal, and EEG features",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Musical Excerpt",
      "Stimulus Description": "Classical piano, 56/106/156 bpm",
      "Composer": "Wolfgang Amadeus Mozart",
      "Passage Name": "Mozart: Sonata for 2 Pianos in D major, K.448/375a",
      "Passage Length": "~13 minutes",
      "passageLengthSeconds": 780,
      "Musical Features Analyzed": [
        "Valence",
        "Arousal",
        "Theta",
        "Alpha",
        "Beta",
        "Gamma",
        "Delta",
        "Power Spectrum",
        "Dispersion Entropy",
        "Covariance",
        "PLV"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "gamma",
        "valence",
        "arousal",
        "power",
        "spectral"
      ],
      "Task Description": "Passive listening with post-trial valence and arousal ratings",
      "Number of Participants": "26",
      "participantsValue": 26,
      "Demographics": "Right-handed native Chinese adults, mixed gender, ages 17–38",
      "Musical Training": "No Formal Training",
      "EEG System Used": "NeuSen.W32, Neuracle",
      "Channel Count": "59 channels, 10–10 system",
      "channelCountValue": 59,
      "Findings": "Faster tempo increased valence and frontal Beta/Gamma power; slow tempo enhanced frontal Alpha/Theta and parieto-occipital PLV",
      "Sampling Rate": "1000 Hz",
      "Recording Environment": "Sound-treated room",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Artifact rejection",
        "CAR",
        "notch filtering",
        "bandpass filtering (0.1–90 Hz)",
        "ICA"
      ],
      "License": "© 2025 Springer Nature",
      "EEG Analysis Techniques": [
        "Power spectrum",
        "Dispersion entropy",
        "Covariance",
        "PLV"
      ],
      "Statistical Tests": [
        "ANOVA",
        "Tukey HSD"
      ],
      "Event Markers": [
        "Onset of musical stimulus"
      ],
      "Publication": "Scientific Reports 15:8276",
      "year": 2025
    },
    {
      "id": "study-192",
      "Study Name": "The power of music: impact on EEG signals",
      "Authors": "Basma Bahgat El Sayed, Mye Ali Basheer, Marwa Safwat Shalaby, Hala Rashad El Habashy, Saly Hasan Elkholy",
      "Year": 2025,
      "DOI/URL": "https://doi.org/10.1007/s00426-024-02060-6",
      "Study Description": "This study investigated how Egyptian folk and classical music influence cortical activation patterns and interhemispheric EEG dynamics in young adults, revealing that folk music increases frontal slow-wave power while classical music evokes EEG patterns associated with relaxed cognitive states.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Egyptian folk music with vocals and traditional instrumentation",
      "Composer": "Traditional Egyptian folk",
      "Passage Length": "3 minutes",
      "passageLengthSeconds": 180,
      "Musical Features Analyzed": [
        "Power Ratio Index (PRI)",
        "Interhemispheric PRI difference",
        "QEEG bands (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "power"
      ],
      "Task Description": "Passive listening with eyes closed during 10-min track (1 min silence → 3 min folk → 3 min silence → 3 min classic)",
      "Number of Participants": "76",
      "participantsValue": 76,
      "Demographics": "High school students (ages 15–26, mean 16.73 ± 2.37), 72% female, majority right-handed, Only 2.6% played an instrument; most had no training",
      "Musical Training": "No Formal Training",
      "EEG System Used": "EBNeuro Galileo NT (Mizar version 3.61)",
      "Channel Count": "NA, 10–20 system",
      "channelCountValue": null,
      "Findings": "Folk music caused regional slowing and reduced cognitive processing, especially in frontal cortex",
      "Sampling Rate": "Not reported",
      "Recording Environment": "Quiet room using headphones",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Visual inspection",
        "artifact rejection",
        "PRI computed from band ratios",
        "Jamovi analysis"
      ],
      "License": "© The Author(s) 2025 under CC BY 4.0",
      "EEG Analysis Techniques": [
        "QEEG band ratio analysis (delta+theta / alpha+beta)",
        "Interhemispheric difference"
      ],
      "Statistical Tests": [
        "ANOVA",
        "Friedman test",
        "Mann–Whitney U",
        "Durbin–Conover pairwise",
        "Levene’s test"
      ],
      "Event Markers": [
        "Manual segmentation by audio epoch timing"
      ],
      "Publication": "Psychological Research (2025) 89:42",
      "year": 2025
    },
    {
      "id": "study-193",
      "Study Name": "The power of music: impact on EEG signals",
      "Authors": "Basma Bahgat El Sayed, Mye Ali Basheer, Marwa Safwat Shalaby, Hala Rashad El Habashy, Saly Hasan Elkholy",
      "Year": 2025,
      "DOI/URL": "https://doi.org/10.1007/s00426-024-02060-6",
      "Study Description": "This study investigated how Egyptian folk and classical music influence cortical activation patterns and interhemispheric EEG dynamics in young adults, revealing that folk music increases frontal slow-wave power while classical music evokes EEG patterns associated with relaxed cognitive states.",
      "Paradigm Type": [
        "Controlled"
      ],
      "Stimulus Type": "Complete Musical Piece",
      "Stimulus Description": "Egyptian instrumental classical music (no lyrics)",
      "Composer": "Egyptian classical",
      "Passage Length": "3 minutes",
      "passageLengthSeconds": 180,
      "Musical Features Analyzed": [
        "Power Ratio Index (PRI)",
        "Interhemispheric PRI difference",
        "QEEG bands (delta",
        "theta",
        "alpha",
        "beta)"
      ],
      "normalizedFeatures": [
        "delta",
        "theta",
        "alpha",
        "beta",
        "power"
      ],
      "Task Description": "Passive listening with eyes closed during 10-min track (1 min silence → 3 min folk → 3 min silence → 3 min classic)",
      "Number of Participants": "76",
      "participantsValue": 76,
      "Demographics": "High school students (ages 15–26, mean 16.73 ± 2.37), 72% female, majority right-handed, Only 2.6% played an instrument; most had no training",
      "Musical Training": "No Formal Training",
      "EEG System Used": "EBNeuro Galileo NT (Mizar version 3.61)",
      "Channel Count": "NA, 10–20 system",
      "channelCountValue": null,
      "Findings": "Classic instrumental music induced EEG patterns associated with relaxation, especially in left temporal regions",
      "Sampling Rate": "Not reported",
      "Recording Environment": "Quiet room using headphones",
      "Data Format": "Digital EEG traces",
      "Preprocessing": [
        "Visual inspection",
        "artifact rejection",
        "PRI computed from band ratios",
        "Jamovi analysis"
      ],
      "License": "© The Author(s) 2025 under CC BY 4.0",
      "EEG Analysis Techniques": [
        "QEEG band ratio analysis (delta+theta / alpha+beta)",
        "Interhemispheric difference"
      ],
      "Statistical Tests": [
        "ANOVA",
        "Friedman test",
        "Mann–Whitney U",
        "Durbin–Conover pairwise",
        "Levene’s test"
      ],
      "Event Markers": [
        "Manual segmentation by audio epoch timing"
      ],
      "Publication": "Psychological Research (2025) 89:42",
      "year": 2025
    }
  ]
}